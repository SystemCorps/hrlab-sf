{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Trainig Templete\n",
    "## Based on Tensorlfow - CNN MNIST example\n",
    "https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/examples/tutorials/layers/cnn_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 library 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 640\n",
    "height = 480\n",
    "channel = 3\n",
    "total_pix = width * height * channel\n",
    "num_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(d_dir, split_rate):\n",
    "    \n",
    "    num_class = len(d_dir.keys())\n",
    "\n",
    "    out = {}\n",
    "    \n",
    "\n",
    "    for i in range(num_class):\n",
    "        x_train, x_test = train_test_split(d_dir[i], test_size=split_rate)\n",
    "        \n",
    "        if i == 0:\n",
    "            train_temp = x_train\n",
    "            test_temp = x_test\n",
    "        else:\n",
    "            train_temp.extend(x_train)\n",
    "            test_temp.extend(x_test)\n",
    "    \n",
    "    random.shuffle(train_temp)\n",
    "    out['train'] = train_temp\n",
    "    random.shuffle(test_temp)\n",
    "    out['test'] = test_temp\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n"
     ]
    }
   ],
   "source": [
    "image_dir1 = '/dataset/'\n",
    "class_name1 = ['untorn', 'torn']\n",
    "\n",
    "data_dir1 = {}\n",
    "for i in range(len(class_name1)):\n",
    "    temp_dir = image_dir1 + class_name1[i]\n",
    "    img_files = [[os.path.join(temp_dir, f), np.int16(i)] for f in os.listdir(temp_dir) if f.endswith('.png')]\n",
    "    data_dir1[i] = img_files\n",
    "print(len(data_dir1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data_dir = data_process(data_dir1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = np.zeros((len(tr_data_dir['train']), total_pix), dtype=np.float32)\n",
    "tr_label = np.zeros((len(tr_data_dir['train']), 1), dtype=np.int32)\n",
    "\n",
    "for i in range(len(tr_data_dir['train'])):\n",
    "    img = cv2.imread(tr_data_dir['train'][i][0])\n",
    "    tr_data[i][:] = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC).flatten()\n",
    "    tr_label[i] = tr_data_dir['train'][i][1]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "ev_data = np.zeros((len(tr_data_dir['test']), total_pix), dtype=np.float32)\n",
    "ev_label = np.zeros((len(tr_data_dir['test']), 1), dtype=np.int32)\n",
    "\n",
    "for i in range(len(tr_data_dir['test'])):\n",
    "    img = cv2.imread(tr_data_dir['test'][i][0])\n",
    "    ev_data[i][:] = cv2.resize(img, (width, height), interpolation=cv2.INTER_CUBIC).flatten()\n",
    "    ev_label[i] = tr_data_dir['test'][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_label = np.zeros((len(tr_data_dir['train']), 2), dtype=np.float32)\n",
    "\n",
    "for i in range(len(tr_data_dir['train'])):\n",
    "    if tr_data_dir['train'][i][1] == 0:\n",
    "        tr_label[i][0] = 1\n",
    "        tr_label[i][1] = 0\n",
    "    \n",
    "    else:\n",
    "        tr_label[i][0] = 0\n",
    "        tr_label[i][1] = 1\n",
    "    \n",
    "    \n",
    "\n",
    "ev_label = np.zeros((len(tr_data_dir['test']), 2), dtype=np.float32)\n",
    "\n",
    "for i in range(len(tr_data_dir['test'])):\n",
    "    if tr_data_dir['test'][i][1] == 0:\n",
    "        ev_label[i][0] = 1\n",
    "        ev_label[i][1] = 0\n",
    "        \n",
    "    else:\n",
    "        ev_label[i][0] = 0\n",
    "        ev_label[i][1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    \n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, n):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=n)\n",
    "\n",
    "def max_pool_2x2(x, n):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding = 'SAME', name = n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", shape = [None, total_pix], name='X')\n",
    "\n",
    "# float32 - for sigmoid cross entropy\n",
    "# int64 - for sparse softmax cross entropy\n",
    "Y = tf.placeholder(tf.int64, shape = [None, 1], name='Y')\n",
    "#Y = tf.placeholder(\"float\", shape = [None, 2], name='Y')\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model function for CNN.\"\"\"\n",
    "\n",
    "input_layer = tf.reshape(X, [-1, 480, 640, 3])\n",
    "\n",
    "\n",
    "conv1 = tf.layers.conv2d(\n",
    "\n",
    "    inputs=input_layer,\n",
    "    filters=48,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu,\n",
    "    name = 'conv1')\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, name='pool1')\n",
    "\n",
    "\n",
    "\n",
    "conv2 = tf.layers.conv2d(\n",
    "    inputs=pool1,\n",
    "    filters=96,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu,\n",
    "    name = 'conv2')\n",
    "\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, name='pool2')\n",
    "\n",
    "\n",
    "\n",
    "conv3 = tf.layers.conv2d(\n",
    "    inputs=pool2,\n",
    "    filters=96,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu,\n",
    "    name = 'conv3')\n",
    "\n",
    "pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2, name='pool3')\n",
    "\n",
    "\n",
    "\n",
    "conv4 = tf.layers.conv2d(\n",
    "    inputs=pool3,\n",
    "    filters=96,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu,\n",
    "    name = 'conv4')\n",
    "\n",
    "pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2, name='pool4')\n",
    "\n",
    "\n",
    "\n",
    "conv5 = tf.layers.conv2d(\n",
    "    inputs=pool4,\n",
    "    filters=96,\n",
    "    kernel_size=[5, 5],\n",
    "    padding=\"same\",\n",
    "    activation=tf.nn.relu,\n",
    "    name = 'conv5')\n",
    "\n",
    "pool5 = tf.layers.max_pooling2d(inputs=conv5, pool_size=[2, 2], strides=2, name='pool5')\n",
    "pool5_flat = tf.reshape(pool5, [-1, 15 * 20 * 96])\n",
    "\n",
    "dense = tf.layers.dense(inputs=pool5_flat, units=1024, activation=tf.nn.relu, name='fc1')\n",
    "\n",
    "dropout = tf.layers.dropout(inputs=dense, rate=(1-keep_prob), name='fc_drop')\n",
    "\n",
    "\n",
    "logits = tf.layers.dense(inputs=dropout, units=2, name='logits')\n",
    "y_conv = tf.nn.softmax(logits, name='softmax_tensor')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img = tf.reshape(X, [-1, height, width, channel])\n",
    "\n",
    "wc1 = weight_variable([5, 5, 3, 48], 'W1')\n",
    "bc1 = bias_variable([48], 'b1')\n",
    "hc1 = tf.nn.relu(conv2d(x_img, wc1, 'conv1') + bc1)\n",
    "hp1 = max_pool_2x2(hc1, 'max1')\n",
    "\n",
    "wc2 = weight_variable([5, 5, 48, 96], 'W2')\n",
    "bc2 = bias_variable([96], 'b2')\n",
    "hc2 = tf.nn.relu(conv2d(hp1, wc2, 'conv2') + bc2)\n",
    "hp2 = max_pool_2x2(hc2, 'max2')\n",
    "\n",
    "wc3 = weight_variable([5, 5, 96, 96], 'W3')\n",
    "bc3 = bias_variable([96], 'b3')\n",
    "hc3 = tf.nn.relu(conv2d(hp2, wc3, 'conv3') + bc3)\n",
    "hp3 = max_pool_2x2(hc3, 'max3')\n",
    "\n",
    "wc4 = weight_variable([5, 5, 96, 96], 'W4')\n",
    "bc4 = bias_variable([96], 'b4')\n",
    "hc4 = tf.nn.relu(conv2d(hp3, wc4, 'conv4') + bc4)\n",
    "hp4 = max_pool_2x2(hc4, 'max4')\n",
    "\n",
    "wc5 = weight_variable([5, 5, 96, 96], 'W5')\n",
    "bc5 = bias_variable([96], 'b5')\n",
    "hc5 = tf.nn.relu(conv2d(hp4, wc5, 'conv5') + bc5)\n",
    "hp5 = max_pool_2x2(hc5, 'max5')\n",
    "\n",
    "wf1 = weight_variable([15*20*96, 1024], 'Wf1')\n",
    "bf1 = bias_variable([1024], 'bf1')\n",
    "hf_flat = tf.reshape(hp5, [-1, 15*20*96])\n",
    "hf1 = tf.nn.relu(tf.matmul(hf_flat, wf1) + bf1)\n",
    "hf_drop = tf.nn.dropout(hf1, keep_prob)\n",
    "\n",
    "wf2 = weight_variable([1024, num_class], 'Wf2')\n",
    "bf2 = bias_variable([num_class], 'bf2')\n",
    "\n",
    "logits = tf.matmul(hf_drop, wf2) + bf2\n",
    "y_conv = tf.nn.softmax(logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05-11-05-55\n"
     ]
    }
   ],
   "source": [
    "today = datetime.today().strftime(\"%m-%d-%H-%M\")\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816 163\n"
     ]
    }
   ],
   "source": [
    "learning_r = 1.0\n",
    "batch_s = 16\n",
    "epochs = 5\n",
    "inner_steps = int((tr_data.shape[0])/batch_s)\n",
    "total_steps = int((tr_data.shape[0]/batch_s*epochs))\n",
    "print(total_steps, inner_steps)\n",
    "disp_step = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_out = tf.argmax(input=logits, axis=1, name='argmax')\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=Y, logits=logits)\n",
    "#loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate = learning_r).minimize(loss=loss,\n",
    "                                                                                    global_step=tf.train.get_global_step())\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.sparse_softmax_cross_entropy(labels = Y, logits = logits)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate = learning_r).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_accuracy = np.zeros((len(ev_label), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10, training accuracy 0.5, loss 0.695179\n",
      "step 20, training accuracy 0.4375, loss 0.695474\n",
      "step 30, training accuracy 0.5, loss 0.701901\n",
      "step 40, training accuracy 0.625, loss 0.683379\n",
      "step 50, training accuracy 0.5625, loss 0.685385\n",
      "step 60, training accuracy 0.5, loss 0.693409\n",
      "step 70, training accuracy 0.625, loss 0.669257\n",
      "step 80, training accuracy 0.5625, loss 0.685469\n",
      "step 90, training accuracy 0.5625, loss 0.685408\n",
      "step 100, training accuracy 0.5, loss 0.697416\n",
      "step 110, training accuracy 0.625, loss 0.669756\n",
      "step 120, training accuracy 0.4375, loss 0.699180\n",
      "step 130, training accuracy 0.5625, loss 0.685326\n",
      "step 140, training accuracy 0.625, loss 0.663885\n",
      "step 150, training accuracy 0.5625, loss 0.692347\n",
      "step 160, training accuracy 0.5625, loss 0.687207\n",
      "step 170, training accuracy 0.625, loss 0.672104\n",
      "step 180, training accuracy 0.6875, loss 0.629885\n",
      "step 190, training accuracy 0.5, loss 0.731438\n",
      "step 200, training accuracy 0.5625, loss 0.685706\n",
      "step 210, training accuracy 0.625, loss 0.662327\n",
      "step 220, training accuracy 0.625, loss 0.662727\n",
      "step 230, training accuracy 0.5625, loss 0.685332\n",
      "step 240, training accuracy 0.5625, loss 0.685464\n",
      "step 250, training accuracy 0.6875, loss 0.649783\n",
      "step 260, training accuracy 0.5625, loss 0.691027\n",
      "step 270, training accuracy 0.5, loss 0.693163\n",
      "step 280, training accuracy 0.75, loss 0.619533\n",
      "step 290, training accuracy 0.5, loss 0.697021\n",
      "step 300, training accuracy 0.5625, loss 0.685533\n",
      "step 310, training accuracy 0.4375, loss 0.701022\n",
      "step 320, training accuracy 0.5, loss 0.693186\n",
      "step 330, training accuracy 0.5625, loss 0.685499\n",
      "step 340, training accuracy 0.625, loss 0.664661\n",
      "step 350, training accuracy 0.8125, loss 0.534276\n",
      "step 360, training accuracy 0.5625, loss 0.685794\n",
      "step 370, training accuracy 0.4375, loss 0.706564\n",
      "step 380, training accuracy 0.6875, loss 0.656894\n",
      "step 390, training accuracy 0.75, loss 0.583642\n",
      "step 400, training accuracy 0.75, loss 0.602926\n",
      "step 410, training accuracy 0.5625, loss 0.694305\n",
      "step 420, training accuracy 0.875, loss 0.432954\n",
      "step 430, training accuracy 0.75, loss 0.590252\n",
      "step 440, training accuracy 0.5, loss 0.695129\n",
      "step 450, training accuracy 0.5625, loss 0.687680\n",
      "step 460, training accuracy 0.5625, loss 0.689031\n",
      "step 470, training accuracy 0.5625, loss 0.685694\n",
      "step 480, training accuracy 0.5, loss 0.694612\n",
      "step 490, training accuracy 0.5625, loss 0.688822\n",
      "step 500, training accuracy 0.6875, loss 0.634627\n",
      "step 510, training accuracy 0.5625, loss 0.687869\n",
      "step 520, training accuracy 0.4375, loss 0.693673\n",
      "step 530, training accuracy 0.625, loss 0.667228\n",
      "step 540, training accuracy 0.5, loss 0.695543\n",
      "step 550, training accuracy 0.5625, loss 0.688067\n",
      "step 560, training accuracy 0.6875, loss 0.630056\n",
      "step 570, training accuracy 0.5625, loss 0.685354\n",
      "step 580, training accuracy 0.5625, loss 0.692377\n",
      "step 590, training accuracy 0.5, loss 0.694222\n",
      "step 600, training accuracy 0.4375, loss 0.693154\n",
      "step 610, training accuracy 0.75, loss 0.593994\n",
      "step 620, training accuracy 0.625, loss 0.663868\n",
      "step 630, training accuracy 0.5, loss 0.697536\n",
      "step 640, training accuracy 0.5625, loss 0.687086\n",
      "step 650, training accuracy 0.5, loss 0.693657\n",
      "step 660, training accuracy 0.6875, loss 0.651904\n",
      "step 670, training accuracy 0.4375, loss 0.703739\n",
      "step 680, training accuracy 0.5, loss 0.703301\n",
      "step 690, training accuracy 0.625, loss 0.677014\n",
      "step 700, training accuracy 0.5625, loss 0.686354\n",
      "step 710, training accuracy 0.5, loss 0.705469\n",
      "step 720, training accuracy 0.8125, loss 0.528134\n",
      "step 730, training accuracy 0.625, loss 0.664310\n",
      "step 740, training accuracy 0.5, loss 0.695725\n",
      "step 750, training accuracy 0.5, loss 0.693189\n",
      "step 760, training accuracy 0.75, loss 0.599794\n",
      "step 770, training accuracy 0.6875, loss 0.627049\n",
      "step 780, training accuracy 0.5625, loss 0.685492\n",
      "step 790, training accuracy 0.5625, loss 0.685370\n",
      "step 800, training accuracy 0.625, loss 0.666746\n",
      "step 810, training accuracy 0.625, loss 0.669147\n"
     ]
    }
   ],
   "source": [
    "t_count = 1\n",
    "\n",
    "for j in range(epochs):\n",
    "\n",
    "    for i in range(inner_steps):\n",
    "\n",
    "        tr_batch = {}\n",
    "\n",
    "        if i < inner_steps-1:\n",
    "            tr_batch['data'] = tr_data[i*batch_s:(i+1)*batch_s][:]\n",
    "            tr_batch['label'] = tr_label[i*batch_s:(i+1)*batch_s][:]\n",
    "\n",
    "        else:\n",
    "            tr_batch['data'] = tr_data[i*batch_s:][:]\n",
    "            tr_batch['label'] = tr_label[i*batch_s:][:]\n",
    "            \n",
    "        sess.run(train_step, feed_dict={X: tr_batch['data'],\n",
    "                                        Y: tr_batch['label'],\n",
    "                                        keep_prob: 0.5})\n",
    "\n",
    "\n",
    "        if t_count % disp_step == 0:\n",
    "            \n",
    "            loss_tr = sess.run(loss, feed_dict = {X: tr_batch['data'],\n",
    "                                                  Y: tr_batch['label'],\n",
    "                                                  keep_prob: 1.0})\n",
    "            \n",
    "            acc_tr = sess.run(accuracy, feed_dict = {X: tr_batch['data'],\n",
    "                                                     Y: tr_batch['label'],\n",
    "                                                     keep_prob: 1.0})\n",
    "            \n",
    "            print('step %d, training accuracy %g, loss %f' % (t_count, acc_tr, loss_tr))\n",
    "            #print(acc_tr, loss_tr)\n",
    "\n",
    "        t_count = t_count+1\n",
    "\n",
    "\n",
    "for k in range(len(ev_label)):\n",
    "    ev_accuracy[k] = sess.run(accuracy, feed_dict={X: ev_data[k:k+1],\n",
    "                                                   Y: ev_label[k:k+1],\n",
    "                                                   keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.saver()\n",
    "save_path = '/model/CNN2600-Vis'\n",
    "saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(ev_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.where(ev_label == 0)\n",
    "test2 = np.where(ev_label == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test1[0]), len(test2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(356/(356+297))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr_batch['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
