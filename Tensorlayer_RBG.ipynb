{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from datetime import datetime\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "import tensorlayer as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "width = 640\n",
    "height = 480\n",
    "channel = 3\n",
    "total_pix = width * height * channel\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "x = tf.placeholder(\"float\", [None, total_pix])\n",
    "y = tf.placeholder(np.int64, [None, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_x = tf.reshape(x, [-1, 480, 640, 3])\n",
    "\n",
    "input_layer = tl.layers.InputLayer(input_x, name='input')\n",
    "\n",
    "conv1 = tl.layers.Conv2d(input_layer,\n",
    "                         48,\n",
    "                         [5, 5],\n",
    "                         [1, 1],\n",
    "                         act=tf.nn.relu,\n",
    "                         padding='SAME',\n",
    "                         name='conv1')\n",
    "pool1 = tl.layers.MaxPool2d(conv1, [2, 2], [2, 2], padding='SAME', name='pool1')\n",
    "\n",
    "\n",
    "conv2 = tl.layers.Conv2d(pool1,\n",
    "                         96,\n",
    "                         [5, 5],\n",
    "                         [1, 1],\n",
    "                         act=tf.nn.relu,\n",
    "                         padding='SAME',\n",
    "                         name='conv2')\n",
    "pool2 = tl.layers.MaxPool2d(conv2, [2, 2], [2, 2], padding='SAME', name='pool2')\n",
    "\n",
    "\n",
    "conv3 = tl.layers.Conv2d(pool2,\n",
    "                         96,\n",
    "                         [5, 5],\n",
    "                         [1, 1],\n",
    "                         act=tf.nn.relu,\n",
    "                         padding='SAME',\n",
    "                         name='conv3')\n",
    "pool3 = tl.layers.MaxPool2d(conv3, [2, 2], [2, 2], padding='SAME', name='pool3')\n",
    "\n",
    "\n",
    "conv4 = tl.layers.Conv2d(pool3,\n",
    "                         96,\n",
    "                         [5, 5],\n",
    "                         [1, 1],\n",
    "                         act=tf.nn.relu,\n",
    "                         padding='SAME',\n",
    "                         name='conv4')\n",
    "pool4 = tl.layers.MaxPool2d(conv4, [2, 2], [2, 2], padding='SAME', name='pool4')\n",
    "\n",
    "\n",
    "conv5 = tl.layers.Conv2d(pool4,\n",
    "                         96,\n",
    "                         [5, 5],\n",
    "                         [1, 1],\n",
    "                         act=tf.nn.relu,\n",
    "                         padding='SAME',\n",
    "                         name='conv5')\n",
    "pool5 = tl.layers.MaxPool2d(conv5, [2, 2], [2, 2], padding='SAME', name='pool5')\n",
    "\n",
    "\n",
    "pool5_flat = tl.layers.FlattenLayer(pool5, name='flatten')\n",
    "dense1 = tl.layers.DenseLayer(pool5_flat, 1024, act=tf.nn.relu, name='dense1')\n",
    "drop1 = tl.layers.DropoutLayer(dense1, keep=0.5, name='drop1')\n",
    "output_layer = tl.layers.DenseLayer(drop1, 2, act=tf.nn.relu, name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = output_layer.outputs\n",
    "\n",
    "y_prob = tf.nn.softmax(logits, name=\"probability\")\n",
    "y_op = tf.argmax(input = logits, axis=1)\n",
    "#loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=y)\n",
    "loss = tl.cost.cross_entropy(output=logits, target=y, name='loss')\n",
    "\n",
    "correct_prediction = tf.equal(y_op, y)\n",
    "acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#acc2 = tf.metrics.accuracy(labels=y, predictions=y_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = output_layer.all_params\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss=loss,\n",
    "                                                                           global_step=tf.train.get_global_step(),\n",
    "                                                                          var_list=train_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(d_dir, split_rate):\n",
    "    \n",
    "    num_class = len(d_dir.keys())\n",
    "\n",
    "    out = {}\n",
    "    \n",
    "\n",
    "    for i in range(num_class):\n",
    "        x_train, x_test = train_test_split(d_dir[i], test_size=split_rate)\n",
    "        \n",
    "        if i == 0:\n",
    "            train_temp = x_train\n",
    "            test_temp = x_test\n",
    "        else:\n",
    "            train_temp.extend(x_train)\n",
    "            test_temp.extend(x_test)\n",
    "    \n",
    "    random.shuffle(train_temp)\n",
    "    out['train'] = train_temp\n",
    "    random.shuffle(test_temp)\n",
    "    out['test'] = test_temp\n",
    "    \n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir1 = '/dataset/'\n",
    "class_name1 = ['untorn', 'torn']\n",
    "\n",
    "data_dir1 = {}\n",
    "for i in range(len(class_name1)):\n",
    "    temp_dir = image_dir1 + class_name1[i]\n",
    "    img_files = [[os.path.join(temp_dir, f), np.int16(i)] for f in os.listdir(temp_dir) if f.endswith('.png')]\n",
    "    data_dir1[i] = img_files\n",
    "print(len(data_dir1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_data_dir = data_process(data_dir1, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = np.zeros((len(tr_data_dir['train']), total_pix), dtype=np.float32)\n",
    "tr_label = np.zeros((len(tr_data_dir['train']), 1), dtype=np.int32)\n",
    "\n",
    "for i in range(len(tr_data_dir['train'])):\n",
    "    img = cv2.imread(tr_data_dir['train'][i][0])\n",
    "    tr_data[i][:] = img.flatten()\n",
    "    tr_label[i] = tr_data_dir['train'][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_data = np.zeros((len(tr_data_dir['test']), total_pix), dtype=np.float32)\n",
    "ev_label = np.zeros((len(tr_data_dir['test']), 1), dtype=np.int32)\n",
    "\n",
    "for i in range(len(tr_data_dir['test'])):\n",
    "    img = cv2.imread(tr_data_dir['test'][i][0])\n",
    "    ev_data[i][:] = img.flatten()\n",
    "    ev_label[i] = tr_data_dir['test'][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "today = datetime.today().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_s = 16\n",
    "epochs = 45\n",
    "tr_steps = int(tr_data.shape[0]/batch_s)\n",
    "print(tr_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.layers.initialize_global_variables(sess)\n",
    "output_layer.print_params()\n",
    "output_layer.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross entropy with logits\n",
    "count = 1\n",
    "for i in range(epochs):\n",
    "    for j in range(tr_steps):\n",
    "        \n",
    "        if j == (tr_steps-1):\n",
    "            temp = tr_label[j*batch_s:]\n",
    "            temp2 = temp.reshape(len(temp),)\n",
    "            feed_dict = {x: tr_data[j*batch_s:], y: temp2}\n",
    "        \n",
    "        else:\n",
    "            temp = tr_label[j*batch_s:(j+1)*batch_s]\n",
    "            temp2 = temp.reshape(batch_s,)\n",
    "            feed_dict = {x: tr_data[j*batch_s:(j+1)*batch_s], y: temp2}\n",
    "            \n",
    "        \n",
    "        \n",
    "        feed_dict.update(output_layer.all_drop)\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "        \n",
    "        if count % 10 == 0:\n",
    "            dp_dict = tl.utils.dict_to_one(output_layer.all_drop)\n",
    "            feed_dict.update(dp_dict)\n",
    "            error, accuracy = sess.run([loss, acc], feed_dict=feed_dict)\n",
    "            print(\"Steps: %d, loss: %f, accuracy: %f\" % (count, error, accuracy))\n",
    "        \n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "for i in range(epochs):\n",
    "    for j in range(tr_steps):\n",
    "        \n",
    "        if j == (tr_steps-1):\n",
    "            feed_dict = {x: tr_data[j*batch_s:], y: tr_label[j*batch_s:]}\n",
    "        \n",
    "        else:\n",
    "            feed_dict = {x: tr_data[j*batch_s:(j+1)*batch_s], y: tr_label[j*batch_s:(j+1)*batch_s]}\n",
    "            \n",
    "        \n",
    "        \n",
    "        feed_dict.update(output_layer.all_drop)\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "        \n",
    "        if count % 10 == 0:\n",
    "            dp_dict = tl.utils.dict_to_one(output_layer.all_drop)\n",
    "            feed_dict.update(dp_dict)\n",
    "            error, accuracy = sess.run([loss, acc], feed_dict=feed_dict)\n",
    "            print(\"Steps: %d, loss: %f, accuracy: %f\" % (count, error, accuracy))\n",
    "        \n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.zeros((len(ev_label), 1))\n",
    "ac = np.zeros((len(ev_label), 1))\n",
    "\n",
    "for k in range(len(ev_label)):\n",
    "    dp_dict = tl.utils.dict_to_one(output_layer.all_drop)\n",
    "    feed_dict = {x: ev_data[k:k+1], y: ev_label[k:k+1]}\n",
    "    feed_dict.update(dp_dict)\n",
    "    t1, t2 = sess.run([loss, acc2], feed_dict=feed_dict)\n",
    "    err[k] = t1\n",
    "    ac[k] = t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.mean(ac))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
