{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import inception_v3\n",
    "import vgg\n",
    "from datagenerator import ImageDataGenerator\n",
    "from tensorflow.contrib.data import Iterator\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['valid3', 'train3', 'train4', 'valid4', 'valid1', 'valid0', 'train2', 'train1', 'valid2', 'train0'])\n"
     ]
    }
   ],
   "source": [
    "# Use same number for training and validation\n",
    "# Ex) 0th folding -> 'train0' for training, 'valid0' for validation\n",
    "data_txt = open('/hrlab-sf/icros/data_split/5fold_0802.txt', 'r')\n",
    "data_json = data_txt.read()\n",
    "tr_data_dir = json.loads(data_json)\n",
    "print(tr_data_dir.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the textfiles for the trainings and validation set\n",
    "num = 0\n",
    "date = \"20180810\"\n",
    "train_file = './train%d_HR_ICROS.txt'%num\n",
    "val_file1 = './valid%d_HR_ICROS.txt'%num\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 30\n",
    "display_step = 20\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"/hdd3/dhj_container/ICROS_vgg/vgg0811%d/\"% num\n",
    "checkpoint_path = \"/hdd3/dhj_container/ICROS_vgg/vggch0811%d\"% num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /hrlab-sf/icros/VGG/datagenerator.py:66: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:From /hrlab-sf/icros/VGG/datagenerator.py:71: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "114\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # data load\n",
    "    tr_data = ImageDataGenerator(train_file,\n",
    "                                 mode='training',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=False)\n",
    "    val_data1 = ImageDataGenerator(val_file1,\n",
    "                                  mode='inference',\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_classes=num_classes,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                           tr_data.data.output_shapes)\n",
    "\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "\n",
    "    # Ops for initializing the two different iterators\n",
    "    training_init_op = iterator.make_initializer(tr_data.data)\n",
    "    validation_init_op1 = iterator.make_initializer(val_data1.data)\n",
    "\n",
    "    # TF placeholder for graph input and output\n",
    "    x = tf.placeholder(tf.float32, [batch_size, 224, 224, 3])\n",
    "    y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "\n",
    "\n",
    "\n",
    "    net, net_points = vgg.vgg_16(x,\n",
    "                                num_classes=num_classes,\n",
    "                                dropout_keep_prob=0.5\n",
    "                               )\n",
    "        \n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "                \n",
    "    # Train op\n",
    "    with tf.name_scope(\"train\"):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            \n",
    "    # Add the loss to summary\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Add the accuracy to the summary\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all summaries together\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Initialize the FileWriter\n",
    "    writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "    # Initialize an saver for store model checkpoints\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    train_batches_per_epoch = int(np.floor(tr_data.data_size / batch_size))\n",
    "    val_batches_per_epoch1 = int(np.floor(val_data1.data_size / batch_size)) \n",
    "    print(train_batches_per_epoch)\n",
    "    print(val_batches_per_epoch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 07:52:24.617092 Start training...\n",
      "2018-08-11 07:52:24.624662 Open tensorboard --logdir=/hdd3/dhj_container/ICROS_vgg/vgg08110/\n",
      "2018-08-11 07:52:24.626385 Epoch number: 1\n",
      "2018-08-11 07:52:29.000325 0 step\n",
      "2018-08-11 07:52:38.869721 20 step\n",
      "2018-08-11 07:52:51.565032 40 step\n",
      "2018-08-11 07:53:03.623183 60 step\n",
      "2018-08-11 07:53:15.420753 80 step\n",
      "2018-08-11 07:53:27.479306 100 step\n",
      "2018-08-11 07:53:35.079703 Start validation\n",
      "2018-08-11 07:53:53.477166 Validation Accuracy = 0.7467\n",
      "2018-08-11 07:53:53.477297 Epoch number: 2\n",
      "2018-08-11 07:53:54.036449 0 step\n",
      "2018-08-11 07:54:00.494060 20 step\n",
      "2018-08-11 07:54:07.010925 40 step\n",
      "2018-08-11 07:54:13.559232 60 step\n",
      "2018-08-11 07:54:20.116935 80 step\n",
      "2018-08-11 07:54:26.686288 100 step\n",
      "2018-08-11 07:54:30.900354 Start validation\n",
      "2018-08-11 07:54:36.708788 Validation Accuracy = 0.7712\n",
      "2018-08-11 07:54:36.709363 Epoch number: 3\n",
      "2018-08-11 07:54:37.190961 0 step\n",
      "2018-08-11 07:54:43.723565 20 step\n",
      "2018-08-11 07:54:50.311445 40 step\n",
      "2018-08-11 07:54:56.966781 60 step\n",
      "2018-08-11 07:55:03.632475 80 step\n",
      "2018-08-11 07:55:10.294713 100 step\n",
      "2018-08-11 07:55:14.498454 Start validation\n",
      "2018-08-11 07:55:20.383706 Validation Accuracy = 0.8571\n",
      "2018-08-11 07:55:20.383900 Epoch number: 4\n",
      "2018-08-11 07:55:20.885824 0 step\n",
      "2018-08-11 07:55:27.588747 20 step\n",
      "2018-08-11 07:55:34.286994 40 step\n",
      "2018-08-11 07:55:41.011866 60 step\n",
      "2018-08-11 07:55:47.607827 80 step\n",
      "2018-08-11 07:55:54.271751 100 step\n",
      "2018-08-11 07:55:58.616022 Start validation\n",
      "2018-08-11 07:56:04.522670 Validation Accuracy = 0.9129\n",
      "2018-08-11 07:56:04.522883 Epoch number: 5\n",
      "2018-08-11 07:56:04.996361 0 step\n",
      "2018-08-11 07:56:11.688956 20 step\n",
      "2018-08-11 07:56:18.354419 40 step\n",
      "2018-08-11 07:56:25.052819 60 step\n",
      "2018-08-11 07:56:31.655642 80 step\n",
      "2018-08-11 07:56:38.315108 100 step\n",
      "2018-08-11 07:56:42.632967 Start validation\n",
      "2018-08-11 07:56:48.720380 Validation Accuracy = 0.9174\n",
      "2018-08-11 07:56:48.720535 Epoch number: 6\n",
      "2018-08-11 07:56:49.279359 0 step\n",
      "2018-08-11 07:56:55.874382 20 step\n",
      "2018-08-11 07:57:02.567513 40 step\n",
      "2018-08-11 07:57:09.325296 60 step\n",
      "2018-08-11 07:57:15.970136 80 step\n",
      "2018-08-11 07:57:22.604665 100 step\n",
      "2018-08-11 07:57:26.843031 Start validation\n",
      "2018-08-11 07:57:32.915461 Validation Accuracy = 0.9230\n",
      "2018-08-11 07:57:32.915595 Epoch number: 7\n",
      "2018-08-11 07:57:33.473305 0 step\n",
      "2018-08-11 07:57:40.175072 20 step\n",
      "2018-08-11 07:57:46.823456 40 step\n",
      "2018-08-11 07:57:53.420779 60 step\n",
      "2018-08-11 07:58:00.099882 80 step\n",
      "2018-08-11 07:58:06.811830 100 step\n",
      "2018-08-11 07:58:11.048526 Start validation\n",
      "2018-08-11 07:58:17.106026 Validation Accuracy = 0.9107\n",
      "2018-08-11 07:58:17.106196 Epoch number: 8\n",
      "2018-08-11 07:58:17.645520 0 step\n",
      "2018-08-11 07:58:24.453397 20 step\n",
      "2018-08-11 07:58:31.111626 40 step\n",
      "2018-08-11 07:58:37.721873 60 step\n",
      "2018-08-11 07:58:44.332363 80 step\n",
      "2018-08-11 07:58:50.969678 100 step\n",
      "2018-08-11 07:58:55.151870 Start validation\n",
      "2018-08-11 07:59:01.157686 Validation Accuracy = 0.9397\n",
      "2018-08-11 07:59:01.157836 Epoch number: 9\n",
      "2018-08-11 07:59:01.706497 0 step\n",
      "2018-08-11 07:59:08.368249 20 step\n",
      "2018-08-11 07:59:14.988483 40 step\n",
      "2018-08-11 07:59:21.618813 60 step\n",
      "2018-08-11 07:59:28.325074 80 step\n",
      "2018-08-11 07:59:34.998603 100 step\n",
      "2018-08-11 07:59:39.267408 Start validation\n",
      "2018-08-11 07:59:45.279567 Validation Accuracy = 0.9464\n",
      "2018-08-11 07:59:45.279685 Epoch number: 10\n",
      "2018-08-11 07:59:45.842858 0 step\n",
      "2018-08-11 07:59:52.653110 20 step\n",
      "2018-08-11 07:59:59.295001 40 step\n",
      "2018-08-11 08:00:05.920688 60 step\n",
      "2018-08-11 08:00:12.593860 80 step\n",
      "2018-08-11 08:00:19.171929 100 step\n",
      "2018-08-11 08:00:23.352998 Start validation\n",
      "2018-08-11 08:00:29.433836 Validation Accuracy = 0.9710\n",
      "2018-08-11 08:00:29.433989 Epoch number: 11\n",
      "2018-08-11 08:00:30.000893 0 step\n",
      "2018-08-11 08:00:36.719105 20 step\n",
      "2018-08-11 08:00:43.391641 40 step\n",
      "2018-08-11 08:00:50.102633 60 step\n",
      "2018-08-11 08:00:56.716355 80 step\n",
      "2018-08-11 08:01:03.318138 100 step\n",
      "2018-08-11 08:01:07.589453 Start validation\n",
      "2018-08-11 08:01:13.611339 Validation Accuracy = 0.9598\n",
      "2018-08-11 08:01:13.611478 Epoch number: 12\n",
      "2018-08-11 08:01:14.171955 0 step\n",
      "2018-08-11 08:01:20.935588 20 step\n",
      "2018-08-11 08:01:27.526417 40 step\n",
      "2018-08-11 08:01:34.119401 60 step\n",
      "2018-08-11 08:01:40.776321 80 step\n",
      "2018-08-11 08:01:47.393816 100 step\n",
      "2018-08-11 08:01:51.596850 Start validation\n",
      "2018-08-11 08:01:57.663332 Validation Accuracy = 0.9576\n",
      "2018-08-11 08:01:57.663474 Epoch number: 13\n",
      "2018-08-11 08:01:58.210142 0 step\n",
      "2018-08-11 08:02:04.928272 20 step\n",
      "2018-08-11 08:02:11.620680 40 step\n",
      "2018-08-11 08:02:18.254931 60 step\n",
      "2018-08-11 08:02:24.879014 80 step\n",
      "2018-08-11 08:02:31.504497 100 step\n",
      "2018-08-11 08:02:35.755628 Start validation\n",
      "2018-08-11 08:02:41.844941 Validation Accuracy = 0.9565\n",
      "2018-08-11 08:02:41.845070 Epoch number: 14\n",
      "2018-08-11 08:02:42.416943 0 step\n",
      "2018-08-11 08:02:49.068299 20 step\n",
      "2018-08-11 08:02:55.739760 40 step\n",
      "2018-08-11 08:03:02.324335 60 step\n",
      "2018-08-11 08:03:08.974112 80 step\n",
      "2018-08-11 08:03:15.618959 100 step\n",
      "2018-08-11 08:03:19.815641 Start validation\n",
      "2018-08-11 08:03:25.928208 Validation Accuracy = 0.9654\n",
      "2018-08-11 08:03:25.928346 Epoch number: 15\n",
      "2018-08-11 08:03:26.497188 0 step\n",
      "2018-08-11 08:03:33.226657 20 step\n",
      "2018-08-11 08:03:39.854272 40 step\n",
      "2018-08-11 08:03:46.505128 60 step\n",
      "2018-08-11 08:03:53.177099 80 step\n",
      "2018-08-11 08:03:59.814977 100 step\n",
      "2018-08-11 08:04:04.017648 Start validation\n",
      "2018-08-11 08:04:10.137723 Validation Accuracy = 0.9743\n",
      "2018-08-11 08:04:10.137854 Epoch number: 16\n",
      "2018-08-11 08:04:10.689214 0 step\n",
      "2018-08-11 08:04:17.302794 20 step\n",
      "2018-08-11 08:04:23.972024 40 step\n",
      "2018-08-11 08:04:30.570386 60 step\n",
      "2018-08-11 08:04:37.229719 80 step\n",
      "2018-08-11 08:04:43.853621 100 step\n",
      "2018-08-11 08:04:48.037209 Start validation\n",
      "2018-08-11 08:04:54.090059 Validation Accuracy = 0.9743\n",
      "2018-08-11 08:04:54.090206 Epoch number: 17\n",
      "2018-08-11 08:04:54.630921 0 step\n",
      "2018-08-11 08:05:01.382475 20 step\n",
      "2018-08-11 08:05:08.024036 40 step\n",
      "2018-08-11 08:05:14.606590 60 step\n",
      "2018-08-11 08:05:21.306078 80 step\n",
      "2018-08-11 08:05:27.880346 100 step\n",
      "2018-08-11 08:05:32.117977 Start validation\n",
      "2018-08-11 08:05:38.219248 Validation Accuracy = 0.9665\n",
      "2018-08-11 08:05:38.219381 Epoch number: 18\n",
      "2018-08-11 08:05:38.761053 0 step\n",
      "2018-08-11 08:05:45.450334 20 step\n",
      "2018-08-11 08:05:52.143018 40 step\n",
      "2018-08-11 08:05:58.780309 60 step\n",
      "2018-08-11 08:06:05.455287 80 step\n",
      "2018-08-11 08:06:12.031462 100 step\n",
      "2018-08-11 08:06:16.268855 Start validation\n",
      "2018-08-11 08:06:22.410357 Validation Accuracy = 0.9688\n",
      "2018-08-11 08:06:22.410497 Epoch number: 19\n",
      "2018-08-11 08:06:23.002978 0 step\n",
      "2018-08-11 08:06:29.655630 20 step\n",
      "2018-08-11 08:06:36.319854 40 step\n",
      "2018-08-11 08:06:42.983600 60 step\n",
      "2018-08-11 08:06:49.603436 80 step\n",
      "2018-08-11 08:06:56.305966 100 step\n",
      "2018-08-11 08:07:00.588401 Start validation\n",
      "2018-08-11 08:07:06.692844 Validation Accuracy = 0.9531\n",
      "2018-08-11 08:07:06.692966 Epoch number: 20\n",
      "2018-08-11 08:07:07.264008 0 step\n",
      "2018-08-11 08:07:13.921885 20 step\n",
      "2018-08-11 08:07:20.610943 40 step\n",
      "2018-08-11 08:07:27.289138 60 step\n",
      "2018-08-11 08:07:33.937667 80 step\n",
      "2018-08-11 08:07:40.534667 100 step\n",
      "2018-08-11 08:07:44.830488 Start validation\n",
      "2018-08-11 08:07:50.974682 Validation Accuracy = 0.9754\n",
      "2018-08-11 08:07:50.974827 Epoch number: 21\n",
      "2018-08-11 08:07:51.471000 0 step\n",
      "2018-08-11 08:07:58.164236 20 step\n",
      "2018-08-11 08:08:04.758831 40 step\n",
      "2018-08-11 08:08:11.377862 60 step\n",
      "2018-08-11 08:08:18.017045 80 step\n",
      "2018-08-11 08:08:24.654441 100 step\n",
      "2018-08-11 08:08:28.871245 Start validation\n",
      "2018-08-11 08:08:34.922520 Validation Accuracy = 0.9676\n",
      "2018-08-11 08:08:34.922656 Epoch number: 22\n",
      "2018-08-11 08:08:35.402459 0 step\n",
      "2018-08-11 08:08:42.107181 20 step\n",
      "2018-08-11 08:08:48.671834 40 step\n",
      "2018-08-11 08:08:55.222898 60 step\n",
      "2018-08-11 08:09:01.819218 80 step\n",
      "2018-08-11 08:09:08.393625 100 step\n",
      "2018-08-11 08:09:12.581719 Start validation\n",
      "2018-08-11 08:09:18.601504 Validation Accuracy = 0.9844\n",
      "2018-08-11 08:09:18.601603 Epoch number: 23\n",
      "2018-08-11 08:09:19.095873 0 step\n",
      "2018-08-11 08:09:25.787626 20 step\n",
      "2018-08-11 08:09:32.425132 40 step\n",
      "2018-08-11 08:09:39.108922 60 step\n",
      "2018-08-11 08:09:45.672834 80 step\n",
      "2018-08-11 08:09:52.273968 100 step\n",
      "2018-08-11 08:09:56.562387 Start validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 08:10:02.579596 Validation Accuracy = 0.9844\n",
      "2018-08-11 08:10:02.579789 Epoch number: 24\n",
      "2018-08-11 08:10:03.065239 0 step\n",
      "2018-08-11 08:10:09.680468 20 step\n",
      "2018-08-11 08:10:16.271877 40 step\n",
      "2018-08-11 08:10:22.980107 60 step\n",
      "2018-08-11 08:10:29.672926 80 step\n",
      "2018-08-11 08:10:36.253441 100 step\n",
      "2018-08-11 08:10:40.496105 Start validation\n",
      "2018-08-11 08:10:46.487713 Validation Accuracy = 0.9866\n",
      "2018-08-11 08:10:46.487818 Epoch number: 25\n",
      "2018-08-11 08:10:46.991632 0 step\n",
      "2018-08-11 08:10:53.566389 20 step\n",
      "2018-08-11 08:11:00.156340 40 step\n",
      "2018-08-11 08:11:06.852939 60 step\n",
      "2018-08-11 08:11:13.447923 80 step\n",
      "2018-08-11 08:11:20.073119 100 step\n",
      "2018-08-11 08:11:24.320888 Start validation\n",
      "2018-08-11 08:11:30.409641 Validation Accuracy = 0.9888\n",
      "2018-08-11 08:11:30.409765 Epoch number: 26\n",
      "2018-08-11 08:11:30.893437 0 step\n",
      "2018-08-11 08:11:37.479466 20 step\n",
      "2018-08-11 08:11:44.024237 40 step\n",
      "2018-08-11 08:11:50.653225 60 step\n",
      "2018-08-11 08:11:57.258265 80 step\n",
      "2018-08-11 08:12:03.958801 100 step\n",
      "2018-08-11 08:12:08.155345 Start validation\n",
      "2018-08-11 08:12:14.303576 Validation Accuracy = 0.9844\n",
      "2018-08-11 08:12:14.303675 Epoch number: 27\n",
      "2018-08-11 08:12:14.793710 0 step\n",
      "2018-08-11 08:12:21.354946 20 step\n",
      "2018-08-11 08:12:27.968187 40 step\n",
      "2018-08-11 08:12:34.552197 60 step\n",
      "2018-08-11 08:12:41.247756 80 step\n",
      "2018-08-11 08:12:47.924083 100 step\n",
      "2018-08-11 08:12:52.189500 Start validation\n",
      "2018-08-11 08:12:58.351515 Validation Accuracy = 0.9866\n",
      "2018-08-11 08:12:58.351623 Epoch number: 28\n",
      "2018-08-11 08:12:58.852791 0 step\n",
      "2018-08-11 08:13:05.485853 20 step\n",
      "2018-08-11 08:13:12.131130 40 step\n",
      "2018-08-11 08:13:18.730457 60 step\n",
      "2018-08-11 08:13:25.364063 80 step\n",
      "2018-08-11 08:13:32.051296 100 step\n",
      "2018-08-11 08:13:36.322239 Start validation\n",
      "2018-08-11 08:13:42.413860 Validation Accuracy = 0.9844\n",
      "2018-08-11 08:13:42.414050 Epoch number: 29\n",
      "2018-08-11 08:13:42.924422 0 step\n",
      "2018-08-11 08:13:49.566400 20 step\n",
      "2018-08-11 08:13:56.127862 40 step\n",
      "2018-08-11 08:14:02.810887 60 step\n",
      "2018-08-11 08:14:09.507835 80 step\n",
      "2018-08-11 08:14:16.161703 100 step\n",
      "2018-08-11 08:14:20.403507 Start validation\n",
      "2018-08-11 08:14:26.461216 Validation Accuracy = 0.9877\n",
      "2018-08-11 08:14:26.461402 Epoch number: 30\n",
      "2018-08-11 08:14:26.964386 0 step\n",
      "2018-08-11 08:14:33.610549 20 step\n",
      "2018-08-11 08:14:40.251804 40 step\n",
      "2018-08-11 08:14:46.828995 60 step\n",
      "2018-08-11 08:14:53.436061 80 step\n",
      "2018-08-11 08:15:00.025306 100 step\n",
      "2018-08-11 08:15:04.285332 Start validation\n",
      "121\n",
      "1993\n",
      "121\n",
      "1717\n",
      "3\n",
      "2067\n",
      "1322\n",
      "1322\n",
      "1913\n",
      "30\n",
      "1054\n",
      "1036\n",
      "2018-08-11 08:15:10.285053 Validation Accuracy = 0.9855\n",
      "2018-08-11 08:15:10.285109 Saving checkpoint of model...\n",
      "2018-08-11 08:15:21.112937 Model checkpoint saved at /hdd3/dhj_container/ICROS_vgg/vggch08110/model_epoch30.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "vaild_result = np.array([])\n",
    "vaild_result.resize((training_epochs,1))\n",
    "\n",
    "\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "   \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open tensorboard --logdir={}\".format(datetime.now(),\n",
    "                                                      filewriter_path))\n",
    "    \n",
    "    img_batch = np.zeros((batch_size,224,224,3), dtype ='uint8')\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "       \n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "       \n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        for step in range(train_batches_per_epoch):\n",
    "\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)       \n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: img_batch, y: label_batch})\n",
    "\n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step % display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict={x: img_batch,\n",
    "                                                        y: label_batch})\n",
    "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "                print(\"{} {} step\".format(datetime.now(), step))\n",
    "\n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        sess.run(validation_init_op1)\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        wrong_cnt = 0\n",
    "        pre = np.array([])\n",
    "        pre.resize((val_batches_per_epoch1,batch_size))\n",
    "        \n",
    "        for a in range(val_batches_per_epoch1):\n",
    "\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "            acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            \n",
    "            if epoch == (training_epochs-1):\n",
    "                pre = pre.astype('uint32')\n",
    "                for i in range(batch_size):\n",
    "                    if pre[test_count][i] == False:\n",
    "                        order = step*batch_size + i\n",
    "                        set_num = num+1\n",
    "                        temp = tr_data_dir['train%d'%set_num][order]\n",
    "                        name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                        print(name)\n",
    "            \n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "            \n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
    "                                                       test_acc))\n",
    "        vaild_result[epoch] = test_acc\n",
    "        \n",
    "        if epoch == training_epochs-1 :\n",
    "            print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "            # save checkpoint of the model\n",
    "            checkpoint_name = os.path.join(checkpoint_path,\n",
    "                                           'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "            save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "            print(\"{} Model checkpoint saved at {}\".format(datetime.now(),\n",
    "                                                           checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74665179],\n",
       "       [0.77120536],\n",
       "       [0.85714286],\n",
       "       [0.91294643],\n",
       "       [0.91741071],\n",
       "       [0.92299107],\n",
       "       [0.91071429],\n",
       "       [0.93973214],\n",
       "       [0.94642857],\n",
       "       [0.97098214],\n",
       "       [0.95982143],\n",
       "       [0.95758929],\n",
       "       [0.95647321],\n",
       "       [0.96540179],\n",
       "       [0.97433036],\n",
       "       [0.97433036],\n",
       "       [0.96651786],\n",
       "       [0.96875   ],\n",
       "       [0.953125  ],\n",
       "       [0.97544643],\n",
       "       [0.96763393],\n",
       "       [0.984375  ],\n",
       "       [0.984375  ],\n",
       "       [0.98660714],\n",
       "       [0.98883929],\n",
       "       [0.984375  ],\n",
       "       [0.98660714],\n",
       "       [0.984375  ],\n",
       "       [0.98772321],\n",
       "       [0.98549107]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaild_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맞추면 1, 틀리면 0\n",
    "test_count = 0\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "pre = np.array([])\n",
    "pre.resize((val_batches_per_epoch1,batch_size))\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    img_batch = np.zeros((batch_size,227,227,3), dtype ='uint8')\n",
    "    for epoch in range(training_epochs):\n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(validation_init_op1)\n",
    "        for step in range(val_batches_per_epoch1):\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)    \n",
    "            #print(len(label_batch))\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre = pre.astype('uint32')\n",
    "            for i in range(batch_size):\n",
    "                if pre[test_count][i] == False:\n",
    "                    order = step*batch_size + i\n",
    "                    set_num = num+1\n",
    "                    temp = tr_data_dir['train%d'%set_num][order]\n",
    "                    name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                    print(name)\n",
    "            test_count+=1\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
