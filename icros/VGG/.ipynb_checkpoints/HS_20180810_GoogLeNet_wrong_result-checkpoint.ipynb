{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import inception_v3\n",
    "from datagenerator import ImageDataGenerator\n",
    "from tensorflow.contrib.data import Iterator\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train3', 'valid4', 'train4', 'train1', 'valid3', 'valid1', 'valid0', 'valid2', 'train0', 'train2'])\n"
     ]
    }
   ],
   "source": [
    "# Use same number for training and validation\n",
    "# Ex) 0th folding -> 'train0' for training, 'valid0' for validation\n",
    "data_txt = open('/HS_code/ICROS_data/data_split/5fold_0802.txt', 'r')\n",
    "data_json = data_txt.read()\n",
    "tr_data_dir = json.loads(data_json)\n",
    "print(tr_data_dir.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the textfiles for the trainings and validation set\n",
    "num = 0\n",
    "date = \"20180810\"\n",
    "train_file = '/HS_code/0_Code/train%d_HR_ICROS.txt'%num\n",
    "val_file1 = '/HS_code/0_Code/valid%d_HR_ICROS.txt'%num\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "training_epochs = 50\n",
    "display_step = 20\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"/HS_code/2_Result_TB/tensorboard_HS_%s_GoogLeNet_hr%d_valid%d_%depoch_0_001\"%(date,num,num,training_epochs)\n",
    "checkpoint_path = \"/HS_code/1_Model_CP/checkpoints_HS_%s_GoogLeNet_hr%d_valid%d_%depoch_0_001\"%(date,num,num,training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /HS_code/0_Code/GoogLeNet/datagenerator.py:66: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:From /HS_code/0_Code/GoogLeNet/datagenerator.py:71: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "28\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # data load\n",
    "    tr_data = ImageDataGenerator(train_file,\n",
    "                                 mode='training',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=True)\n",
    "    val_data1 = ImageDataGenerator(val_file1,\n",
    "                                  mode='inference',\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_classes=num_classes,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                           tr_data.data.output_shapes)\n",
    "\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "\n",
    "    # Ops for initializing the two different iterators\n",
    "    training_init_op = iterator.make_initializer(tr_data.data)\n",
    "    validation_init_op1 = iterator.make_initializer(val_data1.data)\n",
    "\n",
    "    # TF placeholder for graph input and output\n",
    "    x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "    y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "\n",
    "\n",
    "    net, net_points = inception_v3.inception_v3(x, \n",
    "                                              num_classes=num_classes,\n",
    "                                              dropout_keep_prob=0.5\n",
    "                                              )\n",
    "        \n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "                \n",
    "    # Train op\n",
    "    with tf.name_scope(\"train\"):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            \n",
    "    # Add the loss to summary\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Add the accuracy to the summary\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all summaries together\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Initialize the FileWriter\n",
    "    writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "    # Initialize an saver for store model checkpoints\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    train_batches_per_epoch = int(np.floor(tr_data.data_size / batch_size))\n",
    "    val_batches_per_epoch1 = int(np.floor(val_data1.data_size / batch_size)) \n",
    "    print(train_batches_per_epoch)\n",
    "    print(val_batches_per_epoch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10 09:08:28.605049 Start training...\n",
      "2018-08-10 09:08:28.606058 Open tensorboard --logdir=/HS_code/2_Result_TB/tensorboard_HS_20180810_GoogLeNet_hr0_valid0_50epoch_0_001\n",
      "2018-08-10 09:08:28.606307 Epoch number: 1\n",
      "2018-08-10 09:08:34.581492 0 step\n",
      "2018-08-10 09:08:45.853959 20 step\n",
      "2018-08-10 09:08:49.741648 Start validation\n",
      "2018-08-10 09:08:53.249174 Validation Accuracy = 0.5312\n",
      "2018-08-10 09:08:53.249376 Epoch number: 2\n",
      "2018-08-10 09:08:55.682083 0 step\n",
      "2018-08-10 09:09:06.915848 20 step\n",
      "2018-08-10 09:09:10.833190 Start validation\n",
      "2018-08-10 09:09:13.905429 Validation Accuracy = 0.5312\n",
      "2018-08-10 09:09:13.905637 Epoch number: 3\n",
      "2018-08-10 09:09:16.368209 0 step\n",
      "2018-08-10 09:09:28.100426 20 step\n",
      "2018-08-10 09:09:32.192301 Start validation\n",
      "2018-08-10 09:09:35.286052 Validation Accuracy = 0.6998\n",
      "2018-08-10 09:09:35.286245 Epoch number: 4\n",
      "2018-08-10 09:09:37.870504 0 step\n",
      "2018-08-10 09:09:49.755288 20 step\n",
      "2018-08-10 09:09:54.085107 Start validation\n",
      "2018-08-10 09:09:57.276763 Validation Accuracy = 0.6752\n",
      "2018-08-10 09:09:57.276956 Epoch number: 5\n",
      "2018-08-10 09:09:59.872364 0 step\n",
      "2018-08-10 09:10:11.759732 20 step\n",
      "2018-08-10 09:10:15.977959 Start validation\n",
      "2018-08-10 09:10:19.092362 Validation Accuracy = 0.7545\n",
      "2018-08-10 09:10:19.092564 Epoch number: 6\n",
      "2018-08-10 09:10:21.619870 0 step\n",
      "2018-08-10 09:10:33.546364 20 step\n",
      "2018-08-10 09:10:37.665939 Start validation\n",
      "2018-08-10 09:10:40.823521 Validation Accuracy = 0.7533\n",
      "2018-08-10 09:10:40.823723 Epoch number: 7\n",
      "2018-08-10 09:10:43.335415 0 step\n",
      "2018-08-10 09:10:55.278421 20 step\n",
      "2018-08-10 09:10:59.388369 Start validation\n",
      "2018-08-10 09:11:02.513071 Validation Accuracy = 0.6786\n",
      "2018-08-10 09:11:02.513259 Epoch number: 8\n",
      "2018-08-10 09:11:05.041722 0 step\n",
      "2018-08-10 09:11:16.954494 20 step\n",
      "2018-08-10 09:11:21.078489 Start validation\n",
      "2018-08-10 09:11:24.269910 Validation Accuracy = 0.7154\n",
      "2018-08-10 09:11:24.270157 Epoch number: 9\n",
      "2018-08-10 09:11:26.788744 0 step\n",
      "2018-08-10 09:11:39.149035 20 step\n",
      "2018-08-10 09:11:43.364676 Start validation\n",
      "2018-08-10 09:11:46.580938 Validation Accuracy = 0.7277\n",
      "2018-08-10 09:11:46.581132 Epoch number: 10\n",
      "2018-08-10 09:11:49.071687 0 step\n",
      "2018-08-10 09:12:00.789847 20 step\n",
      "2018-08-10 09:12:04.901498 Start validation\n",
      "2018-08-10 09:12:08.150017 Validation Accuracy = 0.7254\n",
      "2018-08-10 09:12:08.150174 Epoch number: 11\n",
      "2018-08-10 09:12:10.684833 0 step\n",
      "2018-08-10 09:12:22.636912 20 step\n",
      "2018-08-10 09:12:26.549071 Start validation\n",
      "2018-08-10 09:12:29.650186 Validation Accuracy = 0.7656\n",
      "2018-08-10 09:12:29.650376 Epoch number: 12\n",
      "2018-08-10 09:12:32.278612 0 step\n",
      "2018-08-10 09:12:44.235189 20 step\n",
      "2018-08-10 09:12:48.364461 Start validation\n",
      "2018-08-10 09:12:51.501842 Validation Accuracy = 0.7969\n",
      "2018-08-10 09:12:51.502093 Epoch number: 13\n",
      "2018-08-10 09:12:54.026725 0 step\n",
      "2018-08-10 09:13:05.751587 20 step\n",
      "2018-08-10 09:13:10.003185 Start validation\n",
      "2018-08-10 09:13:13.206291 Validation Accuracy = 0.7991\n",
      "2018-08-10 09:13:13.206461 Epoch number: 14\n",
      "2018-08-10 09:13:15.658632 0 step\n",
      "2018-08-10 09:13:27.437895 20 step\n",
      "2018-08-10 09:13:31.499239 Start validation\n",
      "2018-08-10 09:13:34.611997 Validation Accuracy = 0.8158\n",
      "2018-08-10 09:13:34.612193 Epoch number: 15\n",
      "2018-08-10 09:13:37.175173 0 step\n",
      "2018-08-10 09:13:49.176105 20 step\n",
      "2018-08-10 09:13:53.307078 Start validation\n",
      "2018-08-10 09:13:56.412882 Validation Accuracy = 0.7645\n",
      "2018-08-10 09:13:56.413053 Epoch number: 16\n",
      "2018-08-10 09:13:58.911409 0 step\n",
      "2018-08-10 09:14:10.643469 20 step\n",
      "2018-08-10 09:14:14.801001 Start validation\n",
      "2018-08-10 09:14:17.991544 Validation Accuracy = 0.8460\n",
      "2018-08-10 09:14:17.991751 Epoch number: 17\n",
      "2018-08-10 09:14:20.501748 0 step\n",
      "2018-08-10 09:14:32.415076 20 step\n",
      "2018-08-10 09:14:36.721839 Start validation\n",
      "2018-08-10 09:14:39.892346 Validation Accuracy = 0.8873\n",
      "2018-08-10 09:14:39.892533 Epoch number: 18\n",
      "2018-08-10 09:14:42.394943 0 step\n",
      "2018-08-10 09:14:54.332484 20 step\n",
      "2018-08-10 09:14:58.580537 Start validation\n",
      "2018-08-10 09:15:01.816793 Validation Accuracy = 0.8605\n",
      "2018-08-10 09:15:01.816928 Epoch number: 19\n",
      "2018-08-10 09:15:04.332089 0 step\n",
      "2018-08-10 09:15:16.409631 20 step\n",
      "2018-08-10 09:15:20.575171 Start validation\n",
      "2018-08-10 09:15:23.792860 Validation Accuracy = 0.8783\n",
      "2018-08-10 09:15:23.793074 Epoch number: 20\n",
      "2018-08-10 09:15:26.397566 0 step\n",
      "2018-08-10 09:15:38.513188 20 step\n",
      "2018-08-10 09:15:42.549081 Start validation\n",
      "2018-08-10 09:15:45.614426 Validation Accuracy = 0.9174\n",
      "2018-08-10 09:15:45.614621 Epoch number: 21\n",
      "2018-08-10 09:15:48.117752 0 step\n",
      "2018-08-10 09:16:00.338714 20 step\n",
      "2018-08-10 09:16:04.687837 Start validation\n",
      "2018-08-10 09:16:07.874876 Validation Accuracy = 0.9230\n",
      "2018-08-10 09:16:07.875066 Epoch number: 22\n",
      "2018-08-10 09:16:10.445695 0 step\n",
      "2018-08-10 09:16:22.352128 20 step\n",
      "2018-08-10 09:16:26.449811 Start validation\n",
      "2018-08-10 09:16:29.608009 Validation Accuracy = 0.9263\n",
      "2018-08-10 09:16:29.608113 Epoch number: 23\n",
      "2018-08-10 09:16:32.225333 0 step\n",
      "2018-08-10 09:16:44.289459 20 step\n",
      "2018-08-10 09:16:48.532293 Start validation\n",
      "2018-08-10 09:16:51.715400 Validation Accuracy = 0.9408\n",
      "2018-08-10 09:16:51.715599 Epoch number: 24\n",
      "2018-08-10 09:16:54.307994 0 step\n",
      "2018-08-10 09:17:06.416473 20 step\n",
      "2018-08-10 09:17:10.652436 Start validation\n",
      "2018-08-10 09:17:13.849200 Validation Accuracy = 0.9665\n",
      "2018-08-10 09:17:13.849404 Epoch number: 25\n",
      "2018-08-10 09:17:16.452898 0 step\n",
      "2018-08-10 09:17:28.723847 20 step\n",
      "2018-08-10 09:17:32.827850 Start validation\n",
      "2018-08-10 09:17:35.991292 Validation Accuracy = 0.9565\n",
      "2018-08-10 09:17:35.991499 Epoch number: 26\n",
      "2018-08-10 09:17:38.654724 0 step\n",
      "2018-08-10 09:17:50.755684 20 step\n",
      "2018-08-10 09:17:54.914895 Start validation\n",
      "2018-08-10 09:17:58.084994 Validation Accuracy = 0.9643\n",
      "2018-08-10 09:17:58.085140 Epoch number: 27\n",
      "2018-08-10 09:18:00.690457 0 step\n",
      "2018-08-10 09:18:12.975842 20 step\n",
      "2018-08-10 09:18:17.188892 Start validation\n",
      "2018-08-10 09:18:20.332413 Validation Accuracy = 0.9576\n",
      "2018-08-10 09:18:20.332563 Epoch number: 28\n",
      "2018-08-10 09:18:22.980184 0 step\n",
      "2018-08-10 09:18:34.933763 20 step\n",
      "2018-08-10 09:18:39.149602 Start validation\n",
      "2018-08-10 09:18:42.311760 Validation Accuracy = 0.9743\n",
      "2018-08-10 09:18:42.311955 Epoch number: 29\n",
      "2018-08-10 09:18:45.589842 0 step\n",
      "2018-08-10 09:18:57.731870 20 step\n",
      "2018-08-10 09:19:01.905127 Start validation\n",
      "2018-08-10 09:19:05.064506 Validation Accuracy = 0.9643\n",
      "2018-08-10 09:19:05.064705 Epoch number: 30\n",
      "2018-08-10 09:19:07.703934 0 step\n",
      "2018-08-10 09:19:19.958020 20 step\n",
      "2018-08-10 09:19:24.149508 Start validation\n",
      "2018-08-10 09:19:27.234070 Validation Accuracy = 0.9743\n",
      "2018-08-10 09:19:27.234174 Epoch number: 31\n",
      "2018-08-10 09:19:29.822804 0 step\n",
      "2018-08-10 09:19:41.842591 20 step\n",
      "2018-08-10 09:19:46.106026 Start validation\n",
      "2018-08-10 09:19:49.311974 Validation Accuracy = 0.9676\n",
      "2018-08-10 09:19:49.312180 Epoch number: 32\n",
      "2018-08-10 09:19:51.962019 0 step\n",
      "2018-08-10 09:20:03.964850 20 step\n",
      "2018-08-10 09:20:08.276093 Start validation\n",
      "2018-08-10 09:20:11.426260 Validation Accuracy = 0.9699\n",
      "2018-08-10 09:20:11.426388 Epoch number: 33\n",
      "2018-08-10 09:20:14.064487 0 step\n",
      "2018-08-10 09:20:26.298766 20 step\n",
      "2018-08-10 09:20:30.572465 Start validation\n",
      "2018-08-10 09:20:33.743959 Validation Accuracy = 0.9777\n",
      "2018-08-10 09:20:33.744155 Epoch number: 34\n",
      "2018-08-10 09:20:36.362287 0 step\n",
      "2018-08-10 09:20:48.492214 20 step\n",
      "2018-08-10 09:20:52.686736 Start validation\n",
      "2018-08-10 09:20:55.832071 Validation Accuracy = 0.9576\n",
      "2018-08-10 09:20:55.832265 Epoch number: 35\n",
      "2018-08-10 09:20:58.456712 0 step\n",
      "2018-08-10 09:21:10.734368 20 step\n",
      "2018-08-10 09:21:15.034550 Start validation\n",
      "2018-08-10 09:21:18.170115 Validation Accuracy = 0.9777\n",
      "2018-08-10 09:21:18.170312 Epoch number: 36\n",
      "2018-08-10 09:21:20.778553 0 step\n",
      "2018-08-10 09:21:32.967337 20 step\n",
      "2018-08-10 09:21:37.384663 Start validation\n",
      "2018-08-10 09:21:40.695012 Validation Accuracy = 0.9408\n",
      "2018-08-10 09:21:40.695198 Epoch number: 37\n",
      "2018-08-10 09:21:43.457552 0 step\n",
      "2018-08-10 09:21:55.905922 20 step\n",
      "2018-08-10 09:22:00.274943 Start validation\n",
      "2018-08-10 09:22:03.514577 Validation Accuracy = 0.9643\n",
      "2018-08-10 09:22:03.514862 Epoch number: 38\n",
      "2018-08-10 09:22:06.304240 0 step\n",
      "2018-08-10 09:22:18.784565 20 step\n",
      "2018-08-10 09:22:23.057328 Start validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-10 09:22:26.268457 Validation Accuracy = 0.9710\n",
      "2018-08-10 09:22:26.268673 Epoch number: 39\n",
      "2018-08-10 09:22:28.817974 0 step\n",
      "2018-08-10 09:22:41.318797 20 step\n",
      "2018-08-10 09:22:45.556591 Start validation\n",
      "2018-08-10 09:22:48.814862 Validation Accuracy = 0.9654\n",
      "2018-08-10 09:22:48.815058 Epoch number: 40\n",
      "2018-08-10 09:22:51.335028 0 step\n",
      "2018-08-10 09:23:03.427077 20 step\n",
      "2018-08-10 09:23:07.617350 Start validation\n",
      "2018-08-10 09:23:10.866104 Validation Accuracy = 0.9777\n",
      "2018-08-10 09:23:10.866291 Epoch number: 41\n",
      "2018-08-10 09:23:13.360867 0 step\n",
      "2018-08-10 09:23:25.084097 20 step\n",
      "2018-08-10 09:23:29.173997 Start validation\n",
      "2018-08-10 09:23:32.320306 Validation Accuracy = 0.9676\n",
      "2018-08-10 09:23:32.320509 Epoch number: 42\n",
      "2018-08-10 09:23:34.843656 0 step\n",
      "2018-08-10 09:23:46.697072 20 step\n",
      "2018-08-10 09:23:50.753426 Start validation\n",
      "2018-08-10 09:23:54.036890 Validation Accuracy = 0.9855\n",
      "2018-08-10 09:23:54.037095 Epoch number: 43\n",
      "2018-08-10 09:23:56.537708 0 step\n",
      "2018-08-10 09:24:08.140414 20 step\n",
      "2018-08-10 09:24:12.188417 Start validation\n",
      "2018-08-10 09:24:15.372583 Validation Accuracy = 0.9732\n",
      "2018-08-10 09:24:15.372778 Epoch number: 44\n",
      "2018-08-10 09:24:17.897540 0 step\n",
      "2018-08-10 09:24:29.594698 20 step\n",
      "2018-08-10 09:24:33.680734 Start validation\n",
      "2018-08-10 09:24:36.861072 Validation Accuracy = 0.9788\n",
      "2018-08-10 09:24:36.861249 Epoch number: 45\n",
      "2018-08-10 09:24:39.354658 0 step\n",
      "2018-08-10 09:24:51.121819 20 step\n",
      "2018-08-10 09:24:55.368695 Start validation\n",
      "2018-08-10 09:24:58.575518 Validation Accuracy = 0.9766\n",
      "2018-08-10 09:24:58.575704 Epoch number: 46\n",
      "2018-08-10 09:25:01.154030 0 step\n",
      "2018-08-10 09:25:13.103096 20 step\n",
      "2018-08-10 09:25:17.164931 Start validation\n",
      "2018-08-10 09:25:20.438252 Validation Accuracy = 0.9766\n",
      "2018-08-10 09:25:20.438443 Epoch number: 47\n",
      "2018-08-10 09:25:22.983216 0 step\n",
      "2018-08-10 09:25:34.883344 20 step\n",
      "2018-08-10 09:25:38.958301 Start validation\n",
      "2018-08-10 09:25:42.027285 Validation Accuracy = 0.9844\n",
      "2018-08-10 09:25:42.027483 Epoch number: 48\n",
      "2018-08-10 09:25:44.503582 0 step\n",
      "2018-08-10 09:25:56.234906 20 step\n",
      "2018-08-10 09:26:00.313280 Start validation\n",
      "2018-08-10 09:26:03.465983 Validation Accuracy = 0.9710\n",
      "2018-08-10 09:26:03.466174 Epoch number: 49\n",
      "2018-08-10 09:26:05.979388 0 step\n",
      "2018-08-10 09:26:17.521821 20 step\n",
      "2018-08-10 09:26:21.606064 Start validation\n",
      "2018-08-10 09:26:24.781361 Validation Accuracy = 0.9643\n",
      "2018-08-10 09:26:24.781595 Epoch number: 50\n",
      "2018-08-10 09:26:27.298098 0 step\n",
      "2018-08-10 09:26:39.006628 20 step\n",
      "2018-08-10 09:26:42.985764 Start validation\n",
      "1133\n",
      "359\n",
      "1104\n",
      "1951\n",
      "1533\n",
      "2276\n",
      "2281\n",
      "1730\n",
      "803\n",
      "11\n",
      "2328\n",
      "764\n",
      "976\n",
      "2351\n",
      "1433\n",
      "615\n",
      "1669\n",
      "2091\n",
      "70\n",
      "2020\n",
      "1035\n",
      "2018-08-10 09:26:46.096981 Validation Accuracy = 0.9777\n",
      "2018-08-10 09:26:46.097093 Saving checkpoint of model...\n",
      "2018-08-10 09:26:48.502443 Model checkpoint saved at /HS_code/1_Model_CP/checkpoints_HS_20180810_GoogLeNet_hr0_valid0_50epoch_0_001/model_epoch50.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "vaild_result = np.array([])\n",
    "vaild_result.resize((training_epochs,1))\n",
    "\n",
    "\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "   \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open tensorboard --logdir={}\".format(datetime.now(),\n",
    "                                                      filewriter_path))\n",
    "    \n",
    "    img_batch = np.zeros((batch_size,227,227,3), dtype ='uint8')\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "       \n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "       \n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        for step in range(train_batches_per_epoch):\n",
    "\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)       \n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: img_batch, y: label_batch})\n",
    "\n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step % display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict={x: img_batch,\n",
    "                                                        y: label_batch})\n",
    "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "                print(\"{} {} step\".format(datetime.now(), step))\n",
    "\n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        sess.run(validation_init_op1)\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        wrong_cnt = 0\n",
    "        pre = np.array([])\n",
    "        pre.resize((val_batches_per_epoch1,batch_size))\n",
    "        \n",
    "        for a in range(val_batches_per_epoch1):\n",
    "\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "            acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            \n",
    "            if epoch == (training_epochs-1):\n",
    "                pre = pre.astype('uint32')\n",
    "                for i in range(batch_size):\n",
    "                    if pre[test_count][i] == False:\n",
    "                        order = step*batch_size + i\n",
    "                        set_num = num+1\n",
    "                        temp = tr_data_dir['train%d'%set_num][order]\n",
    "                        name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                        print(name)\n",
    "            \n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "            \n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
    "                                                       test_acc))\n",
    "        vaild_result[epoch] = test_acc\n",
    "        \n",
    "        if epoch == training_epochs-1 :\n",
    "            print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "            # save checkpoint of the model\n",
    "            checkpoint_name = os.path.join(checkpoint_path,\n",
    "                                           'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "            save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "            print(\"{} Model checkpoint saved at {}\".format(datetime.now(),\n",
    "                                                           checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53125   ],\n",
       "       [0.53125   ],\n",
       "       [0.69977679],\n",
       "       [0.67522321],\n",
       "       [0.75446429],\n",
       "       [0.75334821],\n",
       "       [0.67857143],\n",
       "       [0.71540179],\n",
       "       [0.72767857],\n",
       "       [0.72544643],\n",
       "       [0.765625  ],\n",
       "       [0.796875  ],\n",
       "       [0.79910714],\n",
       "       [0.81584821],\n",
       "       [0.76450893],\n",
       "       [0.84598214],\n",
       "       [0.88727679],\n",
       "       [0.86049107],\n",
       "       [0.87834821],\n",
       "       [0.91741071],\n",
       "       [0.92299107],\n",
       "       [0.92633929],\n",
       "       [0.94084821],\n",
       "       [0.96651786],\n",
       "       [0.95647321],\n",
       "       [0.96428571],\n",
       "       [0.95758929],\n",
       "       [0.97433036],\n",
       "       [0.96428571],\n",
       "       [0.97433036],\n",
       "       [0.96763393],\n",
       "       [0.96986607],\n",
       "       [0.97767857],\n",
       "       [0.95758929],\n",
       "       [0.97767857],\n",
       "       [0.94084821],\n",
       "       [0.96428571],\n",
       "       [0.97098214],\n",
       "       [0.96540179],\n",
       "       [0.97767857],\n",
       "       [0.96763393],\n",
       "       [0.98549107],\n",
       "       [0.97321429],\n",
       "       [0.97879464],\n",
       "       [0.9765625 ],\n",
       "       [0.9765625 ],\n",
       "       [0.984375  ],\n",
       "       [0.97098214],\n",
       "       [0.96428571],\n",
       "       [0.97767857]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaild_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맞추면 1, 틀리면 0\n",
    "test_count = 0\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "pre = np.array([])\n",
    "pre.resize((val_batches_per_epoch1,batch_size))\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    img_batch = np.zeros((batch_size,227,227,3), dtype ='uint8')\n",
    "    for epoch in range(training_epochs):\n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(validation_init_op1)\n",
    "        for step in range(val_batches_per_epoch1):\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)    \n",
    "            #print(len(label_batch))\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre = pre.astype('uint32')\n",
    "            for i in range(batch_size):\n",
    "                if pre[test_count][i] == False:\n",
    "                    order = step*batch_size + i\n",
    "                    set_num = num+1\n",
    "                    temp = tr_data_dir['train%d'%set_num][order]\n",
    "                    name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                    print(name)\n",
    "            test_count+=1\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
