{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import inception_v3\n",
    "from datagenerator import ImageDataGenerator\n",
    "from tensorflow.contrib.data import Iterator\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train0', 'valid1', 'train3', 'train4', 'valid2', 'valid3', 'valid0', 'train2', 'valid4', 'train1'])\n"
     ]
    }
   ],
   "source": [
    "# Use same number for training and validation\n",
    "# Ex) 0th folding -> 'train0' for training, 'valid0' for validation\n",
    "data_txt = open('/HS_code/ICROS_data/data_split/5fold_0802.txt', 'r')\n",
    "data_json = data_txt.read()\n",
    "tr_data_dir = json.loads(data_json)\n",
    "print(tr_data_dir.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the textfiles for the trainings and validation set\n",
    "num = 3\n",
    "date = \"20180810\"\n",
    "train_file = '/HS_code/0_Code/train%d_HR_ICROS.txt'%num\n",
    "val_file1 = '/HS_code/0_Code/valid%d_HR_ICROS.txt'%num\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "learning_rate = 0.001\n",
    "training_epochs = 50\n",
    "display_step = 20\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"/HS_code/2_Result_TB/tensorboard_HS_%s_GoogLeNet_hr%d_valid%d_%depoch_0_001\"%(date,num,num,training_epochs)\n",
    "checkpoint_path = \"/HS_code/1_Model_CP/checkpoints_HS_%s_GoogLeNet_hr%d_valid%d_%depoch_0_001\"%(date,num,num,training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /HS_code/0_Code/GoogLeNet/datagenerator.py:66: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:From /HS_code/0_Code/GoogLeNet/datagenerator.py:71: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "28\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # data load\n",
    "    tr_data = ImageDataGenerator(train_file,\n",
    "                                 mode='training',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=True)\n",
    "    val_data1 = ImageDataGenerator(val_file1,\n",
    "                                  mode='inference',\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_classes=num_classes,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                           tr_data.data.output_shapes)\n",
    "\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "\n",
    "    # Ops for initializing the two different iterators\n",
    "    training_init_op = iterator.make_initializer(tr_data.data)\n",
    "    validation_init_op1 = iterator.make_initializer(val_data1.data)\n",
    "\n",
    "    # TF placeholder for graph input and output\n",
    "    x = tf.placeholder(tf.float32, [batch_size, 227, 227, 3])\n",
    "    y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "\n",
    "\n",
    "    net, net_points = inception_v3.inception_v3(x, \n",
    "                                              num_classes=num_classes,\n",
    "                                              dropout_keep_prob=0.5\n",
    "                                              )\n",
    "        \n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "                \n",
    "    # Train op\n",
    "    with tf.name_scope(\"train\"):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            \n",
    "    # Add the loss to summary\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Add the accuracy to the summary\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all summaries together\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Initialize the FileWriter\n",
    "    writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "    # Initialize an saver for store model checkpoints\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    train_batches_per_epoch = int(np.floor(tr_data.data_size / batch_size))\n",
    "    val_batches_per_epoch1 = int(np.floor(val_data1.data_size / batch_size)) \n",
    "    print(train_batches_per_epoch)\n",
    "    print(val_batches_per_epoch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /HS_code/1_Model_CP/checkpoints_HS_20180810_GoogLeNet_hr3_valid3_50epoch_0_001/model_epoch50.ckpt\n",
      "2018-08-10 10:24:06.336735 Start validation\n",
      "573\n",
      "377\n",
      "1081\n",
      "1077\n",
      "869\n",
      "1786\n",
      "974\n",
      "1137\n",
      "1106\n",
      "855\n",
      "192\n",
      "1291\n",
      "93\n",
      "439\n",
      "538\n",
      "1386\n",
      "1995\n",
      "774\n",
      "2092\n",
      "1628\n",
      "1960\n",
      "1820\n",
      "1643\n",
      "1781\n",
      "788\n",
      "1005\n",
      "1059\n",
      "79\n",
      "2329\n",
      "1408\n",
      "2279\n",
      "912\n",
      "1388\n",
      "471\n",
      "1591\n",
      "1546\n",
      "1788\n",
      "1923\n",
      "739\n",
      "1958\n",
      "2004\n",
      "2018-08-10 10:24:11.402264 Validation Accuracy = 0.9554\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "vaild_result = np.array([])\n",
    "vaild_result.resize((training_epochs,1))\n",
    "\n",
    "\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "\n",
    "    \n",
    "    save_path = os.path.join(checkpoint_path,'model_epoch'+str(50)+'.ckpt')\n",
    "    saver.restore(sess, save_path)\n",
    "    \n",
    "    img_batch = np.zeros((batch_size,227,227,3), dtype ='uint8')\n",
    "       \n",
    "    # Validate the model on the entire validation set\n",
    "    print(\"{} Start validation\".format(datetime.now()))\n",
    "    sess.run(validation_init_op1)\n",
    "    test_acc = 0.\n",
    "    test_count = 0\n",
    "    wrong_cnt = 0\n",
    "    pre = np.array([])\n",
    "    pre.resize((val_batches_per_epoch1,batch_size))\n",
    "\n",
    "    for step in range(val_batches_per_epoch1):\n",
    "\n",
    "        img_batch, label_batch = sess.run(next_batch)\n",
    "        acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
    "                                            y: label_batch})\n",
    "        pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                            y: label_batch})\n",
    "\n",
    "        \n",
    "        pre = pre.astype('uint32')\n",
    "        for i in range(batch_size):\n",
    "            if pre[test_count][i] == False:\n",
    "                order = step*batch_size + i\n",
    "                set_num = num\n",
    "                temp = tr_data_dir['train%d'%set_num][order]\n",
    "                name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                print(name)\n",
    "\n",
    "        test_acc += acc\n",
    "        test_count += 1\n",
    "\n",
    "    test_acc /= test_count\n",
    "    print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
    "                                                   test_acc))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맞추면 1, 틀리면 0\n",
    "test_count = 0\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "pre = np.array([])\n",
    "pre.resize((val_batches_per_epoch1,batch_size))\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    img_batch = np.zeros((batch_size,227,227,3), dtype ='uint8')\n",
    "    for epoch in range(training_epochs):\n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(validation_init_op1)\n",
    "        for step in range(val_batches_per_epoch1):\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)    \n",
    "            #print(len(label_batch))\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre = pre.astype('uint32')\n",
    "            for i in range(batch_size):\n",
    "                if pre[test_count][i] == False:\n",
    "                    order = step*batch_size + i\n",
    "                    set_num = num+1\n",
    "                    temp = tr_data_dir['train%d'%set_num][order]\n",
    "                    name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                    print(name)\n",
    "            test_count+=1\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
