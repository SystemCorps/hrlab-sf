{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import inception_v3\n",
    "import vgg\n",
    "from datagenerator import ImageDataGenerator\n",
    "from tensorflow.contrib.data import Iterator\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train1', 'valid1', 'train4', 'valid0', 'valid4', 'train3', 'train2', 'train0', 'valid2', 'valid3'])\n"
     ]
    }
   ],
   "source": [
    "# Use same number for training and validation\n",
    "# Ex) 0th folding -> 'train0' for training, 'valid0' for validation\n",
    "data_txt = open('/hrlab-sf/icros/data_split/5fold_0802.txt', 'r')\n",
    "data_json = data_txt.read()\n",
    "tr_data_dir = json.loads(data_json)\n",
    "print(tr_data_dir.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the textfiles for the trainings and validation set\n",
    "num = 2\n",
    "date = \"20180810\"\n",
    "train_file = './train%d_HR_ICROS.txt'%num\n",
    "val_file1 = './valid%d_HR_ICROS.txt'%num\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 30\n",
    "display_step = 20\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"/hdd3/dhj_container/ICROS_vgg/vgg0811%d/\"% num\n",
    "checkpoint_path = \"/hdd3/dhj_container/ICROS_vgg/vggch0811%d\"% num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /hrlab-sf/icros/VGG/datagenerator.py:66: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:From /hrlab-sf/icros/VGG/datagenerator.py:71: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "114\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # data load\n",
    "    tr_data = ImageDataGenerator(train_file,\n",
    "                                 mode='training',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=False)\n",
    "    val_data1 = ImageDataGenerator(val_file1,\n",
    "                                  mode='inference',\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_classes=num_classes,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                           tr_data.data.output_shapes)\n",
    "\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "\n",
    "    # Ops for initializing the two different iterators\n",
    "    training_init_op = iterator.make_initializer(tr_data.data)\n",
    "    validation_init_op1 = iterator.make_initializer(val_data1.data)\n",
    "\n",
    "    # TF placeholder for graph input and output\n",
    "    x = tf.placeholder(tf.float32, [batch_size, 224, 224, 3])\n",
    "    y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "\n",
    "\n",
    "\n",
    "    net, net_points = vgg.vgg_16(x,\n",
    "                                num_classes=num_classes,\n",
    "                                dropout_keep_prob=0.5\n",
    "                               )\n",
    "        \n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "                \n",
    "    # Train op\n",
    "    with tf.name_scope(\"train\"):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            \n",
    "    # Add the loss to summary\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Add the accuracy to the summary\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all summaries together\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Initialize the FileWriter\n",
    "    writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "    # Initialize an saver for store model checkpoints\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    train_batches_per_epoch = int(np.floor(tr_data.data_size / batch_size))\n",
    "    val_batches_per_epoch1 = int(np.floor(val_data1.data_size / batch_size)) \n",
    "    print(train_batches_per_epoch)\n",
    "    print(val_batches_per_epoch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 07:56:41.419527 Start training...\n",
      "2018-08-11 07:56:41.424943 Open tensorboard --logdir=/hdd3/dhj_container/ICROS_vgg/vgg08112/\n",
      "2018-08-11 07:56:41.426799 Epoch number: 1\n",
      "2018-08-11 07:56:45.396777 0 step\n",
      "2018-08-11 07:56:51.960485 20 step\n",
      "2018-08-11 07:56:58.506098 40 step\n",
      "2018-08-11 07:57:05.042752 60 step\n",
      "2018-08-11 07:57:11.563454 80 step\n",
      "2018-08-11 07:57:18.035847 100 step\n",
      "2018-08-11 07:57:22.195803 Start validation\n",
      "2018-08-11 07:57:28.172384 Validation Accuracy = 0.7690\n",
      "2018-08-11 07:57:28.172502 Epoch number: 2\n",
      "2018-08-11 07:57:28.694044 0 step\n",
      "2018-08-11 07:57:35.234252 20 step\n",
      "2018-08-11 07:57:41.833557 40 step\n",
      "2018-08-11 07:57:48.443947 60 step\n",
      "2018-08-11 07:57:54.998831 80 step\n",
      "2018-08-11 07:58:01.546515 100 step\n",
      "2018-08-11 07:58:05.727307 Start validation\n",
      "2018-08-11 07:58:11.573475 Validation Accuracy = 0.8181\n",
      "2018-08-11 07:58:11.573612 Epoch number: 3\n",
      "2018-08-11 07:58:12.098420 0 step\n",
      "2018-08-11 07:58:18.836979 20 step\n",
      "2018-08-11 07:58:25.568484 40 step\n",
      "2018-08-11 07:58:32.241995 60 step\n",
      "2018-08-11 07:58:38.814750 80 step\n",
      "2018-08-11 07:58:45.486096 100 step\n",
      "2018-08-11 07:58:49.678781 Start validation\n",
      "2018-08-11 07:58:55.555349 Validation Accuracy = 0.8471\n",
      "2018-08-11 07:58:55.555476 Epoch number: 4\n",
      "2018-08-11 07:58:56.123968 0 step\n",
      "2018-08-11 07:59:02.890441 20 step\n",
      "2018-08-11 07:59:09.591093 40 step\n",
      "2018-08-11 07:59:16.196732 60 step\n",
      "2018-08-11 07:59:22.926797 80 step\n",
      "2018-08-11 07:59:29.549206 100 step\n",
      "2018-08-11 07:59:33.741677 Start validation\n",
      "2018-08-11 07:59:39.658933 Validation Accuracy = 0.8761\n",
      "2018-08-11 07:59:39.659073 Epoch number: 5\n",
      "2018-08-11 07:59:40.228467 0 step\n",
      "2018-08-11 07:59:46.895262 20 step\n",
      "2018-08-11 07:59:53.584397 40 step\n",
      "2018-08-11 08:00:00.294352 60 step\n",
      "2018-08-11 08:00:06.885483 80 step\n",
      "2018-08-11 08:00:13.413783 100 step\n",
      "2018-08-11 08:00:17.667319 Start validation\n",
      "2018-08-11 08:00:23.551855 Validation Accuracy = 0.8951\n",
      "2018-08-11 08:00:23.551996 Epoch number: 6\n",
      "2018-08-11 08:00:24.112208 0 step\n",
      "2018-08-11 08:00:30.756381 20 step\n",
      "2018-08-11 08:00:37.506504 40 step\n",
      "2018-08-11 08:00:44.087182 60 step\n",
      "2018-08-11 08:00:50.777479 80 step\n",
      "2018-08-11 08:00:57.345894 100 step\n",
      "2018-08-11 08:01:01.539349 Start validation\n",
      "2018-08-11 08:01:07.358938 Validation Accuracy = 0.9040\n",
      "2018-08-11 08:01:07.359329 Epoch number: 7\n",
      "2018-08-11 08:01:07.851152 0 step\n",
      "2018-08-11 08:01:14.533960 20 step\n",
      "2018-08-11 08:01:21.189452 40 step\n",
      "2018-08-11 08:01:27.862418 60 step\n",
      "2018-08-11 08:01:34.415007 80 step\n",
      "2018-08-11 08:01:41.047018 100 step\n",
      "2018-08-11 08:01:45.298487 Start validation\n",
      "2018-08-11 08:01:51.123807 Validation Accuracy = 0.9208\n",
      "2018-08-11 08:01:51.124016 Epoch number: 8\n",
      "2018-08-11 08:01:51.609960 0 step\n",
      "2018-08-11 08:01:58.280570 20 step\n",
      "2018-08-11 08:02:05.081824 40 step\n",
      "2018-08-11 08:02:11.722559 60 step\n",
      "2018-08-11 08:02:18.363519 80 step\n",
      "2018-08-11 08:02:24.997986 100 step\n",
      "2018-08-11 08:02:29.242873 Start validation\n",
      "2018-08-11 08:02:35.170626 Validation Accuracy = 0.9330\n",
      "2018-08-11 08:02:35.170759 Epoch number: 9\n",
      "2018-08-11 08:02:35.654072 0 step\n",
      "2018-08-11 08:02:42.254454 20 step\n",
      "2018-08-11 08:02:48.987721 40 step\n",
      "2018-08-11 08:02:55.751177 60 step\n",
      "2018-08-11 08:03:02.477121 80 step\n",
      "2018-08-11 08:03:09.099786 100 step\n",
      "2018-08-11 08:03:13.319880 Start validation\n",
      "2018-08-11 08:03:19.279581 Validation Accuracy = 0.9397\n",
      "2018-08-11 08:03:19.279687 Epoch number: 10\n",
      "2018-08-11 08:03:19.775389 0 step\n",
      "2018-08-11 08:03:26.531574 20 step\n",
      "2018-08-11 08:03:33.284610 40 step\n",
      "2018-08-11 08:03:40.101686 60 step\n",
      "2018-08-11 08:03:46.829277 80 step\n",
      "2018-08-11 08:03:53.359155 100 step\n",
      "2018-08-11 08:03:57.524843 Start validation\n",
      "2018-08-11 08:04:03.422894 Validation Accuracy = 0.9498\n",
      "2018-08-11 08:04:03.423077 Epoch number: 11\n",
      "2018-08-11 08:04:03.902805 0 step\n",
      "2018-08-11 08:04:10.528089 20 step\n",
      "2018-08-11 08:04:17.268840 40 step\n",
      "2018-08-11 08:04:23.967300 60 step\n",
      "2018-08-11 08:04:30.593758 80 step\n",
      "2018-08-11 08:04:37.237352 100 step\n",
      "2018-08-11 08:04:41.434705 Start validation\n",
      "2018-08-11 08:04:47.387122 Validation Accuracy = 0.9487\n",
      "2018-08-11 08:04:47.387238 Epoch number: 12\n",
      "2018-08-11 08:04:47.887473 0 step\n",
      "2018-08-11 08:04:54.642214 20 step\n",
      "2018-08-11 08:05:01.332120 40 step\n",
      "2018-08-11 08:05:07.909889 60 step\n",
      "2018-08-11 08:05:14.547674 80 step\n",
      "2018-08-11 08:05:21.124601 100 step\n",
      "2018-08-11 08:05:25.394082 Start validation\n",
      "2018-08-11 08:05:31.272652 Validation Accuracy = 0.9375\n",
      "2018-08-11 08:05:31.272753 Epoch number: 13\n",
      "2018-08-11 08:05:31.766460 0 step\n",
      "2018-08-11 08:05:38.449148 20 step\n",
      "2018-08-11 08:05:45.070544 40 step\n",
      "2018-08-11 08:05:51.767836 60 step\n",
      "2018-08-11 08:05:58.547622 80 step\n",
      "2018-08-11 08:06:05.163345 100 step\n",
      "2018-08-11 08:06:09.373419 Start validation\n",
      "2018-08-11 08:06:15.173224 Validation Accuracy = 0.9598\n",
      "2018-08-11 08:06:15.173397 Epoch number: 14\n",
      "2018-08-11 08:06:15.675975 0 step\n",
      "2018-08-11 08:06:22.321320 20 step\n",
      "2018-08-11 08:06:28.978111 40 step\n",
      "2018-08-11 08:06:35.740812 60 step\n",
      "2018-08-11 08:06:42.368127 80 step\n",
      "2018-08-11 08:06:48.963766 100 step\n",
      "2018-08-11 08:06:53.211142 Start validation\n",
      "2018-08-11 08:06:59.047222 Validation Accuracy = 0.9665\n",
      "2018-08-11 08:06:59.047320 Epoch number: 15\n",
      "2018-08-11 08:06:59.553938 0 step\n",
      "2018-08-11 08:07:06.250651 20 step\n",
      "2018-08-11 08:07:12.904325 40 step\n",
      "2018-08-11 08:07:19.612337 60 step\n",
      "2018-08-11 08:07:26.234202 80 step\n",
      "2018-08-11 08:07:32.776338 100 step\n",
      "2018-08-11 08:07:36.964050 Start validation\n",
      "2018-08-11 08:07:42.830642 Validation Accuracy = 0.9699\n",
      "2018-08-11 08:07:42.830849 Epoch number: 16\n",
      "2018-08-11 08:07:43.316600 0 step\n",
      "2018-08-11 08:07:49.943613 20 step\n",
      "2018-08-11 08:07:56.562580 40 step\n",
      "2018-08-11 08:08:03.159648 60 step\n",
      "2018-08-11 08:08:09.848881 80 step\n",
      "2018-08-11 08:08:16.388375 100 step\n",
      "2018-08-11 08:08:20.612307 Start validation\n",
      "2018-08-11 08:08:26.490403 Validation Accuracy = 0.9621\n",
      "2018-08-11 08:08:26.490593 Epoch number: 17\n",
      "2018-08-11 08:08:26.994402 0 step\n",
      "2018-08-11 08:08:33.743195 20 step\n",
      "2018-08-11 08:08:40.332812 40 step\n",
      "2018-08-11 08:08:46.936962 60 step\n",
      "2018-08-11 08:08:53.528678 80 step\n",
      "2018-08-11 08:09:00.160947 100 step\n",
      "2018-08-11 08:09:04.356367 Start validation\n",
      "2018-08-11 08:09:10.277251 Validation Accuracy = 0.9252\n",
      "2018-08-11 08:09:10.277371 Epoch number: 18\n",
      "2018-08-11 08:09:10.755859 0 step\n",
      "2018-08-11 08:09:17.444898 20 step\n",
      "2018-08-11 08:09:24.059299 40 step\n",
      "2018-08-11 08:09:30.735589 60 step\n",
      "2018-08-11 08:09:37.380306 80 step\n",
      "2018-08-11 08:09:44.007204 100 step\n",
      "2018-08-11 08:09:48.200272 Start validation\n",
      "2018-08-11 08:09:54.045092 Validation Accuracy = 0.9754\n",
      "2018-08-11 08:09:54.045231 Epoch number: 19\n",
      "2018-08-11 08:09:54.534409 0 step\n",
      "2018-08-11 08:10:01.137247 20 step\n",
      "2018-08-11 08:10:07.770572 40 step\n",
      "2018-08-11 08:10:14.375353 60 step\n",
      "2018-08-11 08:10:21.159316 80 step\n",
      "2018-08-11 08:10:27.686489 100 step\n",
      "2018-08-11 08:10:31.843064 Start validation\n",
      "2018-08-11 08:10:37.727032 Validation Accuracy = 0.9576\n",
      "2018-08-11 08:10:37.727176 Epoch number: 20\n",
      "2018-08-11 08:10:38.208561 0 step\n",
      "2018-08-11 08:10:44.796780 20 step\n",
      "2018-08-11 08:10:51.482452 40 step\n",
      "2018-08-11 08:10:58.130619 60 step\n",
      "2018-08-11 08:11:04.739052 80 step\n",
      "2018-08-11 08:11:11.392874 100 step\n",
      "2018-08-11 08:11:15.663110 Start validation\n",
      "2018-08-11 08:11:21.541238 Validation Accuracy = 0.9766\n",
      "2018-08-11 08:11:21.541398 Epoch number: 21\n",
      "2018-08-11 08:11:22.020633 0 step\n",
      "2018-08-11 08:11:28.641182 20 step\n",
      "2018-08-11 08:11:35.218899 40 step\n",
      "2018-08-11 08:11:41.825522 60 step\n",
      "2018-08-11 08:11:48.558361 80 step\n",
      "2018-08-11 08:11:55.208187 100 step\n",
      "2018-08-11 08:11:59.493851 Start validation\n",
      "2018-08-11 08:12:05.327415 Validation Accuracy = 0.9810\n",
      "2018-08-11 08:12:05.327549 Epoch number: 22\n",
      "2018-08-11 08:12:05.811856 0 step\n",
      "2018-08-11 08:12:12.419456 20 step\n",
      "2018-08-11 08:12:19.088116 40 step\n",
      "2018-08-11 08:12:25.738631 60 step\n",
      "2018-08-11 08:12:32.423706 80 step\n",
      "2018-08-11 08:12:39.041109 100 step\n",
      "2018-08-11 08:12:43.236536 Start validation\n",
      "2018-08-11 08:12:49.071071 Validation Accuracy = 0.9643\n",
      "2018-08-11 08:12:49.071271 Epoch number: 23\n",
      "2018-08-11 08:12:49.568407 0 step\n",
      "2018-08-11 08:12:56.230982 20 step\n",
      "2018-08-11 08:13:02.829940 40 step\n",
      "2018-08-11 08:13:09.520882 60 step\n",
      "2018-08-11 08:13:16.234551 80 step\n",
      "2018-08-11 08:13:22.795450 100 step\n",
      "2018-08-11 08:13:27.007208 Start validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 08:13:32.845439 Validation Accuracy = 0.9665\n",
      "2018-08-11 08:13:32.845579 Epoch number: 24\n",
      "2018-08-11 08:13:33.330016 0 step\n",
      "2018-08-11 08:13:39.974963 20 step\n",
      "2018-08-11 08:13:46.564592 40 step\n",
      "2018-08-11 08:13:53.146451 60 step\n",
      "2018-08-11 08:13:59.722628 80 step\n",
      "2018-08-11 08:14:06.458837 100 step\n",
      "2018-08-11 08:14:10.631573 Start validation\n",
      "2018-08-11 08:14:16.495910 Validation Accuracy = 0.9766\n",
      "2018-08-11 08:14:16.496099 Epoch number: 25\n",
      "2018-08-11 08:14:17.045887 0 step\n",
      "2018-08-11 08:14:23.716820 20 step\n",
      "2018-08-11 08:14:30.287039 40 step\n",
      "2018-08-11 08:14:36.976253 60 step\n",
      "2018-08-11 08:14:43.669055 80 step\n",
      "2018-08-11 08:14:50.266540 100 step\n",
      "2018-08-11 08:14:54.508126 Start validation\n",
      "2018-08-11 08:15:00.351859 Validation Accuracy = 0.9821\n",
      "2018-08-11 08:15:00.351961 Epoch number: 26\n",
      "2018-08-11 08:15:00.852868 0 step\n",
      "2018-08-11 08:15:07.598354 20 step\n",
      "2018-08-11 08:15:14.240695 40 step\n",
      "2018-08-11 08:15:20.801402 60 step\n",
      "2018-08-11 08:15:27.521129 80 step\n",
      "2018-08-11 08:15:34.182887 100 step\n",
      "2018-08-11 08:15:38.450558 Start validation\n",
      "2018-08-11 08:15:44.333633 Validation Accuracy = 0.9877\n",
      "2018-08-11 08:15:44.333720 Epoch number: 27\n",
      "2018-08-11 08:15:44.811488 0 step\n",
      "2018-08-11 08:15:51.405066 20 step\n",
      "2018-08-11 08:15:58.045430 40 step\n",
      "2018-08-11 08:16:04.765728 60 step\n",
      "2018-08-11 08:16:11.494736 80 step\n",
      "2018-08-11 08:16:18.087165 100 step\n",
      "2018-08-11 08:16:22.280049 Start validation\n",
      "2018-08-11 08:16:28.115539 Validation Accuracy = 0.9821\n",
      "2018-08-11 08:16:28.115720 Epoch number: 28\n",
      "2018-08-11 08:16:28.598678 0 step\n",
      "2018-08-11 08:16:35.143458 20 step\n",
      "2018-08-11 08:16:41.746934 40 step\n",
      "2018-08-11 08:16:48.427218 60 step\n",
      "2018-08-11 08:16:55.135649 80 step\n",
      "2018-08-11 08:17:01.811578 100 step\n",
      "2018-08-11 08:17:06.013526 Start validation\n",
      "2018-08-11 08:17:11.897643 Validation Accuracy = 0.9721\n",
      "2018-08-11 08:17:11.897870 Epoch number: 29\n",
      "2018-08-11 08:17:12.390419 0 step\n",
      "2018-08-11 08:17:18.952054 20 step\n",
      "2018-08-11 08:17:25.512654 40 step\n",
      "2018-08-11 08:17:32.218026 60 step\n",
      "2018-08-11 08:17:38.941362 80 step\n",
      "2018-08-11 08:17:45.582074 100 step\n",
      "2018-08-11 08:17:49.794981 Start validation\n",
      "2018-08-11 08:17:55.617797 Validation Accuracy = 0.9710\n",
      "2018-08-11 08:17:55.617925 Epoch number: 30\n",
      "2018-08-11 08:17:56.099720 0 step\n",
      "2018-08-11 08:18:02.747429 20 step\n",
      "2018-08-11 08:18:09.373585 40 step\n",
      "2018-08-11 08:18:16.044277 60 step\n",
      "2018-08-11 08:18:22.655357 80 step\n",
      "2018-08-11 08:18:29.196976 100 step\n",
      "2018-08-11 08:18:33.415514 Start validation\n",
      "2288\n",
      "159\n",
      "2249\n",
      "2067\n",
      "2080\n",
      "2249\n",
      "1322\n",
      "2080\n",
      "1433\n",
      "3\n",
      "578\n",
      "1002\n",
      "1076\n",
      "2288\n",
      "2080\n",
      "361\n",
      "1784\n",
      "96\n",
      "2067\n",
      "1478\n",
      "30\n",
      "96\n",
      "103\n",
      "1054\n",
      "1433\n",
      "3\n",
      "404\n",
      "2288\n",
      "404\n",
      "2288\n",
      "2080\n",
      "1708\n",
      "2067\n",
      "1076\n",
      "815\n",
      "1913\n",
      "30\n",
      "3\n",
      "121\n",
      "1054\n",
      "103\n",
      "732\n",
      "159\n",
      "1717\n",
      "1002\n",
      "2080\n",
      "1478\n",
      "1717\n",
      "1913\n",
      "1054\n",
      "1717\n",
      "56\n",
      "1999\n",
      "1433\n",
      "815\n",
      "2288\n",
      "1708\n",
      "30\n",
      "30\n",
      "1913\n",
      "1054\n",
      "2018-08-11 08:18:39.259142 Validation Accuracy = 0.9353\n",
      "2018-08-11 08:18:39.259214 Saving checkpoint of model...\n",
      "2018-08-11 08:18:49.046633 Model checkpoint saved at /hdd3/dhj_container/ICROS_vgg/vggch08112/model_epoch30.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "vaild_result = np.array([])\n",
    "vaild_result.resize((training_epochs,1))\n",
    "\n",
    "\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "   \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open tensorboard --logdir={}\".format(datetime.now(),\n",
    "                                                      filewriter_path))\n",
    "    \n",
    "    img_batch = np.zeros((batch_size,224,224,3), dtype ='uint8')\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "       \n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "       \n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        for step in range(train_batches_per_epoch):\n",
    "\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)       \n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: img_batch, y: label_batch})\n",
    "\n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step % display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict={x: img_batch,\n",
    "                                                        y: label_batch})\n",
    "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "                print(\"{} {} step\".format(datetime.now(), step))\n",
    "\n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        sess.run(validation_init_op1)\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        wrong_cnt = 0\n",
    "        pre = np.array([])\n",
    "        pre.resize((val_batches_per_epoch1,batch_size))\n",
    "        \n",
    "        for a in range(val_batches_per_epoch1):\n",
    "\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "            acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            \n",
    "            if epoch == (training_epochs-1):\n",
    "                pre = pre.astype('uint32')\n",
    "                for i in range(batch_size):\n",
    "                    if pre[test_count][i] == False:\n",
    "                        order = step*batch_size + i\n",
    "                        set_num = num+1\n",
    "                        temp = tr_data_dir['train%d'%set_num][order]\n",
    "                        name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                        print(name)\n",
    "            \n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "            \n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
    "                                                       test_acc))\n",
    "        vaild_result[epoch] = test_acc\n",
    "        \n",
    "        if epoch == training_epochs-1 :\n",
    "            print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "            # save checkpoint of the model\n",
    "            checkpoint_name = os.path.join(checkpoint_path,\n",
    "                                           'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "            save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "            print(\"{} Model checkpoint saved at {}\".format(datetime.now(),\n",
    "                                                           checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.76897321],\n",
       "       [0.81808036],\n",
       "       [0.84709821],\n",
       "       [0.87611607],\n",
       "       [0.89508929],\n",
       "       [0.90401786],\n",
       "       [0.92075893],\n",
       "       [0.93303571],\n",
       "       [0.93973214],\n",
       "       [0.94977679],\n",
       "       [0.94866071],\n",
       "       [0.9375    ],\n",
       "       [0.95982143],\n",
       "       [0.96651786],\n",
       "       [0.96986607],\n",
       "       [0.96205357],\n",
       "       [0.92522321],\n",
       "       [0.97544643],\n",
       "       [0.95758929],\n",
       "       [0.9765625 ],\n",
       "       [0.98102679],\n",
       "       [0.96428571],\n",
       "       [0.96651786],\n",
       "       [0.9765625 ],\n",
       "       [0.98214286],\n",
       "       [0.98772321],\n",
       "       [0.98214286],\n",
       "       [0.97209821],\n",
       "       [0.97098214],\n",
       "       [0.93526786]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaild_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맞추면 1, 틀리면 0\n",
    "test_count = 0\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "pre = np.array([])\n",
    "pre.resize((val_batches_per_epoch1,batch_size))\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    img_batch = np.zeros((batch_size,227,227,3), dtype ='uint8')\n",
    "    for epoch in range(training_epochs):\n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(validation_init_op1)\n",
    "        for step in range(val_batches_per_epoch1):\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)    \n",
    "            #print(len(label_batch))\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre = pre.astype('uint32')\n",
    "            for i in range(batch_size):\n",
    "                if pre[test_count][i] == False:\n",
    "                    order = step*batch_size + i\n",
    "                    set_num = num+1\n",
    "                    temp = tr_data_dir['train%d'%set_num][order]\n",
    "                    name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                    print(name)\n",
    "            test_count+=1\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
