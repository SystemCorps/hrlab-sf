{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import inception_v3\n",
    "import vgg\n",
    "from datagenerator import ImageDataGenerator\n",
    "from tensorflow.contrib.data import Iterator\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['valid3', 'train3', 'train2', 'train0', 'valid0', 'train1', 'valid4', 'valid1', 'valid2', 'train4'])\n"
     ]
    }
   ],
   "source": [
    "# Use same number for training and validation\n",
    "# Ex) 0th folding -> 'train0' for training, 'valid0' for validation\n",
    "data_txt = open('/hrlab-sf/icros/data_split/5fold_0802.txt', 'r')\n",
    "data_json = data_txt.read()\n",
    "tr_data_dir = json.loads(data_json)\n",
    "print(tr_data_dir.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the textfiles for the trainings and validation set\n",
    "num = 4\n",
    "date = \"20180810\"\n",
    "train_file = './train%d_HR_ICROS.txt'%num\n",
    "val_file1 = './valid%d_HR_ICROS.txt'%num\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 30\n",
    "display_step = 20\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"/hdd3/dhj_container/ICROS_vgg/vgg0811%d/\"% num\n",
    "checkpoint_path = \"/hdd3/dhj_container/ICROS_vgg/vggch0811%d\"% num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /hrlab-sf/icros/VGG/datagenerator.py:66: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:From /hrlab-sf/icros/VGG/datagenerator.py:71: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "114\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # data load\n",
    "    tr_data = ImageDataGenerator(train_file,\n",
    "                                 mode='training',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=False)\n",
    "    val_data1 = ImageDataGenerator(val_file1,\n",
    "                                  mode='inference',\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_classes=num_classes,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                           tr_data.data.output_shapes)\n",
    "\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "\n",
    "    # Ops for initializing the two different iterators\n",
    "    training_init_op = iterator.make_initializer(tr_data.data)\n",
    "    validation_init_op1 = iterator.make_initializer(val_data1.data)\n",
    "\n",
    "    # TF placeholder for graph input and output\n",
    "    x = tf.placeholder(tf.float32, [batch_size, 224, 224, 3])\n",
    "    y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "\n",
    "\n",
    "\n",
    "    net, net_points = vgg.vgg_16(x,\n",
    "                                num_classes=num_classes,\n",
    "                                dropout_keep_prob=0.5\n",
    "                               )\n",
    "        \n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "                \n",
    "    # Train op\n",
    "    with tf.name_scope(\"train\"):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            \n",
    "    # Add the loss to summary\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Add the accuracy to the summary\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all summaries together\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Initialize the FileWriter\n",
    "    writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "    # Initialize an saver for store model checkpoints\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    train_batches_per_epoch = int(np.floor(tr_data.data_size / batch_size))\n",
    "    val_batches_per_epoch1 = int(np.floor(val_data1.data_size / batch_size)) \n",
    "    print(train_batches_per_epoch)\n",
    "    print(val_batches_per_epoch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 08:19:38.601117 Start training...\n",
      "2018-08-11 08:19:38.601475 Open tensorboard --logdir=/hdd3/dhj_container/ICROS_vgg/vgg08114/\n",
      "2018-08-11 08:19:38.603529 Epoch number: 1\n",
      "2018-08-11 08:19:42.125940 0 step\n",
      "2018-08-11 08:19:48.687183 20 step\n",
      "2018-08-11 08:19:55.241256 40 step\n",
      "2018-08-11 08:20:01.775576 60 step\n",
      "2018-08-11 08:20:08.341222 80 step\n",
      "2018-08-11 08:20:14.911722 100 step\n",
      "2018-08-11 08:20:19.102041 Start validation\n",
      "2018-08-11 08:20:24.995463 Validation Accuracy = 0.7444\n",
      "2018-08-11 08:20:24.995643 Epoch number: 2\n",
      "2018-08-11 08:20:25.478468 0 step\n",
      "2018-08-11 08:20:32.092710 20 step\n",
      "2018-08-11 08:20:38.749850 40 step\n",
      "2018-08-11 08:20:45.400560 60 step\n",
      "2018-08-11 08:20:52.024828 80 step\n",
      "2018-08-11 08:20:58.626696 100 step\n",
      "2018-08-11 08:21:02.862870 Start validation\n",
      "2018-08-11 08:21:08.801790 Validation Accuracy = 0.8058\n",
      "2018-08-11 08:21:08.801983 Epoch number: 3\n",
      "2018-08-11 08:21:09.295468 0 step\n",
      "2018-08-11 08:21:16.053317 20 step\n",
      "2018-08-11 08:21:22.710690 40 step\n",
      "2018-08-11 08:21:29.385676 60 step\n",
      "2018-08-11 08:21:36.048525 80 step\n",
      "2018-08-11 08:21:42.809619 100 step\n",
      "2018-08-11 08:21:47.101634 Start validation\n",
      "2018-08-11 08:21:53.151150 Validation Accuracy = 0.8047\n",
      "2018-08-11 08:21:53.151338 Epoch number: 4\n",
      "2018-08-11 08:21:53.631186 0 step\n",
      "2018-08-11 08:22:00.342390 20 step\n",
      "2018-08-11 08:22:07.015172 40 step\n",
      "2018-08-11 08:22:13.792832 60 step\n",
      "2018-08-11 08:22:20.538001 80 step\n",
      "2018-08-11 08:22:27.284603 100 step\n",
      "2018-08-11 08:22:31.571739 Start validation\n",
      "2018-08-11 08:22:37.474971 Validation Accuracy = 0.8996\n",
      "2018-08-11 08:22:37.475453 Epoch number: 5\n",
      "2018-08-11 08:22:37.966864 0 step\n",
      "2018-08-11 08:22:44.726902 20 step\n",
      "2018-08-11 08:22:51.521787 40 step\n",
      "2018-08-11 08:22:58.275670 60 step\n",
      "2018-08-11 08:23:05.029130 80 step\n",
      "2018-08-11 08:23:11.705378 100 step\n",
      "2018-08-11 08:23:15.999799 Start validation\n",
      "2018-08-11 08:23:21.957863 Validation Accuracy = 0.9118\n",
      "2018-08-11 08:23:21.957988 Epoch number: 6\n",
      "2018-08-11 08:23:22.452852 0 step\n",
      "2018-08-11 08:23:29.275588 20 step\n",
      "2018-08-11 08:23:35.953879 40 step\n",
      "2018-08-11 08:23:42.644434 60 step\n",
      "2018-08-11 08:23:49.340894 80 step\n",
      "2018-08-11 08:23:56.027006 100 step\n",
      "2018-08-11 08:24:00.253026 Start validation\n",
      "2018-08-11 08:24:06.232509 Validation Accuracy = 0.9330\n",
      "2018-08-11 08:24:06.232660 Epoch number: 7\n",
      "2018-08-11 08:24:06.726818 0 step\n",
      "2018-08-11 08:24:13.496368 20 step\n",
      "2018-08-11 08:24:20.235147 40 step\n",
      "2018-08-11 08:24:26.830807 60 step\n",
      "2018-08-11 08:24:33.621607 80 step\n",
      "2018-08-11 08:24:40.338396 100 step\n",
      "2018-08-11 08:24:44.600864 Start validation\n",
      "2018-08-11 08:24:50.699961 Validation Accuracy = 0.9442\n",
      "2018-08-11 08:24:50.700185 Epoch number: 8\n",
      "2018-08-11 08:24:51.182911 0 step\n",
      "2018-08-11 08:24:57.854707 20 step\n",
      "2018-08-11 08:25:04.507286 40 step\n",
      "2018-08-11 08:25:11.140596 60 step\n",
      "2018-08-11 08:25:17.805826 80 step\n",
      "2018-08-11 08:25:24.406844 100 step\n",
      "2018-08-11 08:25:28.656810 Start validation\n",
      "2018-08-11 08:25:34.680754 Validation Accuracy = 0.9576\n",
      "2018-08-11 08:25:34.680930 Epoch number: 9\n",
      "2018-08-11 08:25:35.174752 0 step\n",
      "2018-08-11 08:25:41.898460 20 step\n",
      "2018-08-11 08:25:48.638852 40 step\n",
      "2018-08-11 08:25:55.381468 60 step\n",
      "2018-08-11 08:26:02.077920 80 step\n",
      "2018-08-11 08:26:08.696336 100 step\n",
      "2018-08-11 08:26:12.929222 Start validation\n",
      "2018-08-11 08:26:18.890514 Validation Accuracy = 0.9520\n",
      "2018-08-11 08:26:18.890617 Epoch number: 10\n",
      "2018-08-11 08:26:19.382617 0 step\n",
      "2018-08-11 08:26:26.159854 20 step\n",
      "2018-08-11 08:26:32.881504 40 step\n",
      "2018-08-11 08:26:39.600498 60 step\n",
      "2018-08-11 08:26:46.313040 80 step\n",
      "2018-08-11 08:26:53.124159 100 step\n",
      "2018-08-11 08:26:57.314211 Start validation\n",
      "2018-08-11 08:27:03.296837 Validation Accuracy = 0.9676\n",
      "2018-08-11 08:27:03.296992 Epoch number: 11\n",
      "2018-08-11 08:27:03.779675 0 step\n",
      "2018-08-11 08:27:10.499997 20 step\n",
      "2018-08-11 08:27:17.158904 40 step\n",
      "2018-08-11 08:27:23.902054 60 step\n",
      "2018-08-11 08:27:30.687096 80 step\n",
      "2018-08-11 08:27:37.409335 100 step\n",
      "2018-08-11 08:27:41.675781 Start validation\n",
      "2018-08-11 08:27:47.686017 Validation Accuracy = 0.9342\n",
      "2018-08-11 08:27:47.686194 Epoch number: 12\n",
      "2018-08-11 08:27:48.166563 0 step\n",
      "2018-08-11 08:27:54.859795 20 step\n",
      "2018-08-11 08:28:01.493762 40 step\n",
      "2018-08-11 08:28:08.160640 60 step\n",
      "2018-08-11 08:28:14.866045 80 step\n",
      "2018-08-11 08:28:21.491586 100 step\n",
      "2018-08-11 08:28:25.772286 Start validation\n",
      "2018-08-11 08:28:31.786556 Validation Accuracy = 0.9308\n",
      "2018-08-11 08:28:31.786741 Epoch number: 13\n",
      "2018-08-11 08:28:32.289753 0 step\n",
      "2018-08-11 08:28:38.980837 20 step\n",
      "2018-08-11 08:28:45.685698 40 step\n",
      "2018-08-11 08:28:52.361586 60 step\n",
      "2018-08-11 08:28:59.011746 80 step\n",
      "2018-08-11 08:29:05.644781 100 step\n",
      "2018-08-11 08:29:09.902766 Start validation\n",
      "2018-08-11 08:29:15.951329 Validation Accuracy = 0.9598\n",
      "2018-08-11 08:29:15.951436 Epoch number: 14\n",
      "2018-08-11 08:29:16.442404 0 step\n",
      "2018-08-11 08:29:23.192506 20 step\n",
      "2018-08-11 08:29:29.941549 40 step\n",
      "2018-08-11 08:29:36.670362 60 step\n",
      "2018-08-11 08:29:43.342229 80 step\n",
      "2018-08-11 08:29:50.037418 100 step\n",
      "2018-08-11 08:29:54.273543 Start validation\n",
      "2018-08-11 08:30:00.407427 Validation Accuracy = 0.9665\n",
      "2018-08-11 08:30:00.407575 Epoch number: 15\n",
      "2018-08-11 08:30:00.950912 0 step\n",
      "2018-08-11 08:30:07.726982 20 step\n",
      "2018-08-11 08:30:14.334201 40 step\n",
      "2018-08-11 08:30:21.006328 60 step\n",
      "2018-08-11 08:30:27.648769 80 step\n",
      "2018-08-11 08:30:34.273643 100 step\n",
      "2018-08-11 08:30:38.481489 Start validation\n",
      "2018-08-11 08:30:44.535907 Validation Accuracy = 0.9810\n",
      "2018-08-11 08:30:44.536063 Epoch number: 16\n",
      "2018-08-11 08:30:45.086719 0 step\n",
      "2018-08-11 08:30:51.868656 20 step\n",
      "2018-08-11 08:30:58.502987 40 step\n",
      "2018-08-11 08:31:05.261644 60 step\n",
      "2018-08-11 08:31:11.956274 80 step\n",
      "2018-08-11 08:31:18.607123 100 step\n",
      "2018-08-11 08:31:22.878346 Start validation\n",
      "2018-08-11 08:31:28.903415 Validation Accuracy = 0.9810\n",
      "2018-08-11 08:31:28.903530 Epoch number: 17\n",
      "2018-08-11 08:31:29.448599 0 step\n",
      "2018-08-11 08:31:36.262951 20 step\n",
      "2018-08-11 08:31:42.953305 40 step\n",
      "2018-08-11 08:31:49.612634 60 step\n",
      "2018-08-11 08:31:56.272839 80 step\n",
      "2018-08-11 08:32:02.996509 100 step\n",
      "2018-08-11 08:32:07.279094 Start validation\n",
      "2018-08-11 08:32:13.543388 Validation Accuracy = 0.9855\n",
      "2018-08-11 08:32:13.543526 Epoch number: 18\n",
      "2018-08-11 08:32:14.071347 0 step\n",
      "2018-08-11 08:32:20.842408 20 step\n",
      "2018-08-11 08:32:27.494283 40 step\n",
      "2018-08-11 08:32:34.099185 60 step\n",
      "2018-08-11 08:32:40.828260 80 step\n",
      "2018-08-11 08:32:47.452671 100 step\n",
      "2018-08-11 08:32:51.729884 Start validation\n",
      "2018-08-11 08:32:57.731564 Validation Accuracy = 0.9777\n",
      "2018-08-11 08:32:57.731681 Epoch number: 19\n",
      "2018-08-11 08:32:58.257836 0 step\n",
      "2018-08-11 08:33:04.878514 20 step\n",
      "2018-08-11 08:33:11.544198 40 step\n",
      "2018-08-11 08:33:18.359176 60 step\n",
      "2018-08-11 08:33:25.084520 80 step\n",
      "2018-08-11 08:33:31.740994 100 step\n",
      "2018-08-11 08:33:36.025238 Start validation\n",
      "2018-08-11 08:33:41.913855 Validation Accuracy = 0.9721\n",
      "2018-08-11 08:33:41.913984 Epoch number: 20\n",
      "2018-08-11 08:33:42.464403 0 step\n",
      "2018-08-11 08:33:49.374207 20 step\n",
      "2018-08-11 08:33:56.045941 40 step\n",
      "2018-08-11 08:34:02.775745 60 step\n",
      "2018-08-11 08:34:09.453525 80 step\n",
      "2018-08-11 08:34:16.148051 100 step\n",
      "2018-08-11 08:34:20.464180 Start validation\n",
      "2018-08-11 08:34:26.429299 Validation Accuracy = 0.9777\n",
      "2018-08-11 08:34:26.429440 Epoch number: 21\n",
      "2018-08-11 08:34:26.969467 0 step\n",
      "2018-08-11 08:34:33.805069 20 step\n",
      "2018-08-11 08:34:40.463554 40 step\n",
      "2018-08-11 08:34:47.165260 60 step\n",
      "2018-08-11 08:34:53.815724 80 step\n",
      "2018-08-11 08:35:00.551772 100 step\n",
      "2018-08-11 08:35:04.902002 Start validation\n",
      "2018-08-11 08:35:10.924559 Validation Accuracy = 0.9732\n",
      "2018-08-11 08:35:10.924705 Epoch number: 22\n",
      "2018-08-11 08:35:11.486184 0 step\n",
      "2018-08-11 08:35:18.325322 20 step\n",
      "2018-08-11 08:35:25.012553 40 step\n",
      "2018-08-11 08:35:31.725093 60 step\n",
      "2018-08-11 08:35:38.453213 80 step\n",
      "2018-08-11 08:35:45.070341 100 step\n",
      "2018-08-11 08:35:49.371101 Start validation\n",
      "2018-08-11 08:35:55.422500 Validation Accuracy = 0.9810\n",
      "2018-08-11 08:35:55.422637 Epoch number: 23\n",
      "2018-08-11 08:35:55.991673 0 step\n",
      "2018-08-11 08:36:02.691939 20 step\n",
      "2018-08-11 08:36:09.353080 40 step\n",
      "2018-08-11 08:36:16.078615 60 step\n",
      "2018-08-11 08:36:22.738055 80 step\n",
      "2018-08-11 08:36:29.443205 100 step\n",
      "2018-08-11 08:36:33.713996 Start validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 08:36:39.730961 Validation Accuracy = 0.9866\n",
      "2018-08-11 08:36:39.731114 Epoch number: 24\n",
      "2018-08-11 08:36:40.279655 0 step\n",
      "2018-08-11 08:36:46.939506 20 step\n",
      "2018-08-11 08:36:53.642400 40 step\n",
      "2018-08-11 08:37:00.323593 60 step\n",
      "2018-08-11 08:37:07.022796 80 step\n",
      "2018-08-11 08:37:13.701703 100 step\n",
      "2018-08-11 08:37:17.984554 Start validation\n",
      "2018-08-11 08:37:24.040876 Validation Accuracy = 0.9844\n",
      "2018-08-11 08:37:24.041023 Epoch number: 25\n",
      "2018-08-11 08:37:24.574240 0 step\n",
      "2018-08-11 08:37:31.222828 20 step\n",
      "2018-08-11 08:37:38.011340 40 step\n",
      "2018-08-11 08:37:44.733580 60 step\n",
      "2018-08-11 08:37:51.403796 80 step\n",
      "2018-08-11 08:37:58.163523 100 step\n",
      "2018-08-11 08:38:02.495288 Start validation\n",
      "2018-08-11 08:38:08.491791 Validation Accuracy = 0.9900\n",
      "2018-08-11 08:38:08.491945 Epoch number: 26\n",
      "2018-08-11 08:38:08.976458 0 step\n",
      "2018-08-11 08:38:15.628483 20 step\n",
      "2018-08-11 08:38:22.289076 40 step\n",
      "2018-08-11 08:38:29.070933 60 step\n",
      "2018-08-11 08:38:35.773557 80 step\n",
      "2018-08-11 08:38:42.494392 100 step\n",
      "2018-08-11 08:38:46.732145 Start validation\n",
      "2018-08-11 08:38:52.661222 Validation Accuracy = 0.9643\n",
      "2018-08-11 08:38:52.661327 Epoch number: 27\n",
      "2018-08-11 08:38:53.180004 0 step\n",
      "2018-08-11 08:38:59.874490 20 step\n",
      "2018-08-11 08:39:06.627793 40 step\n",
      "2018-08-11 08:39:13.236029 60 step\n",
      "2018-08-11 08:39:19.877687 80 step\n",
      "2018-08-11 08:39:26.509629 100 step\n",
      "2018-08-11 08:39:30.772216 Start validation\n",
      "2018-08-11 08:39:36.737722 Validation Accuracy = 0.9788\n",
      "2018-08-11 08:39:36.737925 Epoch number: 28\n",
      "2018-08-11 08:39:37.215652 0 step\n",
      "2018-08-11 08:39:43.777482 20 step\n",
      "2018-08-11 08:39:50.431579 40 step\n",
      "2018-08-11 08:39:57.088211 60 step\n",
      "2018-08-11 08:40:03.860311 80 step\n",
      "2018-08-11 08:40:10.515539 100 step\n",
      "2018-08-11 08:40:14.862212 Start validation\n",
      "2018-08-11 08:40:20.814624 Validation Accuracy = 0.9855\n",
      "2018-08-11 08:40:20.814860 Epoch number: 29\n",
      "2018-08-11 08:40:21.300391 0 step\n",
      "2018-08-11 08:40:27.870934 20 step\n",
      "2018-08-11 08:40:34.553752 40 step\n",
      "2018-08-11 08:40:41.340874 60 step\n",
      "2018-08-11 08:40:48.026270 80 step\n",
      "2018-08-11 08:40:54.742972 100 step\n",
      "2018-08-11 08:40:58.971970 Start validation\n",
      "2018-08-11 08:41:04.935194 Validation Accuracy = 0.9821\n",
      "2018-08-11 08:41:04.935401 Epoch number: 30\n",
      "2018-08-11 08:41:05.433240 0 step\n",
      "2018-08-11 08:41:12.150032 20 step\n",
      "2018-08-11 08:41:18.877864 40 step\n",
      "2018-08-11 08:41:25.599144 60 step\n",
      "2018-08-11 08:41:32.227076 80 step\n",
      "2018-08-11 08:41:38.979937 100 step\n",
      "2018-08-11 08:41:43.262293 Start validation\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'train5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b2b22699b61e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m                         \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                         \u001b[0mset_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr_data_dir\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train%d'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mset_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'('\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m')'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'train5'"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "vaild_result = np.array([])\n",
    "vaild_result.resize((training_epochs,1))\n",
    "\n",
    "\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "   \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open tensorboard --logdir={}\".format(datetime.now(),\n",
    "                                                      filewriter_path))\n",
    "    \n",
    "    img_batch = np.zeros((batch_size,224,224,3), dtype ='uint8')\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "       \n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "       \n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        for step in range(train_batches_per_epoch):\n",
    "\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)       \n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: img_batch, y: label_batch})\n",
    "\n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step % display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict={x: img_batch,\n",
    "                                                        y: label_batch})\n",
    "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "                print(\"{} {} step\".format(datetime.now(), step))\n",
    "\n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        sess.run(validation_init_op1)\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        wrong_cnt = 0\n",
    "        pre = np.array([])\n",
    "        pre.resize((val_batches_per_epoch1,batch_size))\n",
    "        \n",
    "        for a in range(val_batches_per_epoch1):\n",
    "\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "            acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            \n",
    "            if epoch == (training_epochs-1):\n",
    "                pre = pre.astype('uint32')\n",
    "                for i in range(batch_size):\n",
    "                    if pre[test_count][i] == False:\n",
    "                        order = step*batch_size + i\n",
    "                        set_num = num+1\n",
    "                        temp = tr_data_dir['train%d'%set_num][order]\n",
    "                        name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                        print(name)\n",
    "            \n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "            \n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
    "                                                       test_acc))\n",
    "        vaild_result[epoch] = test_acc\n",
    "        \n",
    "        if epoch == training_epochs-1 :\n",
    "            print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "            # save checkpoint of the model\n",
    "            checkpoint_name = os.path.join(checkpoint_path,\n",
    "                                           'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "            save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "            print(\"{} Model checkpoint saved at {}\".format(datetime.now(),\n",
    "                                                           checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaild_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맞추면 1, 틀리면 0\n",
    "test_count = 0\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "pre = np.array([])\n",
    "pre.resize((val_batches_per_epoch1,batch_size))\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    img_batch = np.zeros((batch_size,227,227,3), dtype ='uint8')\n",
    "    for epoch in range(training_epochs):\n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(validation_init_op1)\n",
    "        for step in range(val_batches_per_epoch1):\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)    \n",
    "            #print(len(label_batch))\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre = pre.astype('uint32')\n",
    "            for i in range(batch_size):\n",
    "                if pre[test_count][i] == False:\n",
    "                    order = step*batch_size + i\n",
    "                    set_num = num+1\n",
    "                    temp = tr_data_dir['train%d'%set_num][order]\n",
    "                    name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                    print(name)\n",
    "            test_count+=1\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
