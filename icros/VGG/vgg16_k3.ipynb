{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import inception_v3\n",
    "import vgg\n",
    "from datagenerator import ImageDataGenerator\n",
    "from tensorflow.contrib.data import Iterator\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['valid1', 'valid0', 'valid3', 'train2', 'valid4', 'train3', 'train4', 'valid2', 'train0', 'train1'])\n"
     ]
    }
   ],
   "source": [
    "# Use same number for training and validation\n",
    "# Ex) 0th folding -> 'train0' for training, 'valid0' for validation\n",
    "data_txt = open('/hrlab-sf/icros/data_split/5fold_0802.txt', 'r')\n",
    "data_json = data_txt.read()\n",
    "tr_data_dir = json.loads(data_json)\n",
    "print(tr_data_dir.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the textfiles for the trainings and validation set\n",
    "num = 3\n",
    "date = \"20180810\"\n",
    "train_file = './train%d_HR_ICROS.txt'%num\n",
    "val_file1 = './valid%d_HR_ICROS.txt'%num\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 30\n",
    "display_step = 20\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"/hdd3/dhj_container/ICROS_vgg/vgg0811%d/\"% num\n",
    "checkpoint_path = \"/hdd3/dhj_container/ICROS_vgg/vggch0811%d\"% num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /hrlab-sf/icros/VGG/datagenerator.py:66: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:From /hrlab-sf/icros/VGG/datagenerator.py:71: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "114\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # data load\n",
    "    tr_data = ImageDataGenerator(train_file,\n",
    "                                 mode='training',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=False)\n",
    "    val_data1 = ImageDataGenerator(val_file1,\n",
    "                                  mode='inference',\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_classes=num_classes,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                           tr_data.data.output_shapes)\n",
    "\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "\n",
    "    # Ops for initializing the two different iterators\n",
    "    training_init_op = iterator.make_initializer(tr_data.data)\n",
    "    validation_init_op1 = iterator.make_initializer(val_data1.data)\n",
    "\n",
    "    # TF placeholder for graph input and output\n",
    "    x = tf.placeholder(tf.float32, [batch_size, 224, 224, 3])\n",
    "    y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "\n",
    "\n",
    "\n",
    "    net, net_points = vgg.vgg_16(x,\n",
    "                                num_classes=num_classes,\n",
    "                                dropout_keep_prob=0.5\n",
    "                               )\n",
    "        \n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "                \n",
    "    # Train op\n",
    "    with tf.name_scope(\"train\"):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            \n",
    "    # Add the loss to summary\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Add the accuracy to the summary\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all summaries together\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Initialize the FileWriter\n",
    "    writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "    # Initialize an saver for store model checkpoints\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    train_batches_per_epoch = int(np.floor(tr_data.data_size / batch_size))\n",
    "    val_batches_per_epoch1 = int(np.floor(val_data1.data_size / batch_size)) \n",
    "    print(train_batches_per_epoch)\n",
    "    print(val_batches_per_epoch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 08:15:58.941337 Start training...\n",
      "2018-08-11 08:15:58.945069 Open tensorboard --logdir=/hdd3/dhj_container/ICROS_vgg/vgg08113/\n",
      "2018-08-11 08:15:58.946906 Epoch number: 1\n",
      "2018-08-11 08:16:02.890576 0 step\n",
      "2018-08-11 08:16:09.536647 20 step\n",
      "2018-08-11 08:16:16.093072 40 step\n",
      "2018-08-11 08:16:22.657412 60 step\n",
      "2018-08-11 08:16:29.355972 80 step\n",
      "2018-08-11 08:16:36.062605 100 step\n",
      "2018-08-11 08:16:40.311537 Start validation\n",
      "2018-08-11 08:16:46.370299 Validation Accuracy = 0.7143\n",
      "2018-08-11 08:16:46.370427 Epoch number: 2\n",
      "2018-08-11 08:16:46.938353 0 step\n",
      "2018-08-11 08:16:53.678541 20 step\n",
      "2018-08-11 08:17:00.382038 40 step\n",
      "2018-08-11 08:17:07.040719 60 step\n",
      "2018-08-11 08:17:13.705962 80 step\n",
      "2018-08-11 08:17:20.458606 100 step\n",
      "2018-08-11 08:17:24.647163 Start validation\n",
      "2018-08-11 08:17:30.673439 Validation Accuracy = 0.7623\n",
      "2018-08-11 08:17:30.673550 Epoch number: 3\n",
      "2018-08-11 08:17:31.163680 0 step\n",
      "2018-08-11 08:17:37.845929 20 step\n",
      "2018-08-11 08:17:44.558441 40 step\n",
      "2018-08-11 08:17:51.234305 60 step\n",
      "2018-08-11 08:17:57.949972 80 step\n",
      "2018-08-11 08:18:04.639945 100 step\n",
      "2018-08-11 08:18:08.874235 Start validation\n",
      "2018-08-11 08:18:14.810392 Validation Accuracy = 0.8292\n",
      "2018-08-11 08:18:14.810529 Epoch number: 4\n",
      "2018-08-11 08:18:15.309556 0 step\n",
      "2018-08-11 08:18:21.898896 20 step\n",
      "2018-08-11 08:18:28.484296 40 step\n",
      "2018-08-11 08:18:35.187062 60 step\n",
      "2018-08-11 08:18:41.900947 80 step\n",
      "2018-08-11 08:18:48.531779 100 step\n",
      "2018-08-11 08:18:52.809121 Start validation\n",
      "2018-08-11 08:18:58.713187 Validation Accuracy = 0.8717\n",
      "2018-08-11 08:18:58.713334 Epoch number: 5\n",
      "2018-08-11 08:18:59.208048 0 step\n",
      "2018-08-11 08:19:05.785017 20 step\n",
      "2018-08-11 08:19:12.425951 40 step\n",
      "2018-08-11 08:19:19.105656 60 step\n",
      "2018-08-11 08:19:25.737742 80 step\n",
      "2018-08-11 08:19:32.360775 100 step\n",
      "2018-08-11 08:19:36.625923 Start validation\n",
      "2018-08-11 08:19:42.642630 Validation Accuracy = 0.9018\n",
      "2018-08-11 08:19:42.642773 Epoch number: 6\n",
      "2018-08-11 08:19:43.194951 0 step\n",
      "2018-08-11 08:19:49.829210 20 step\n",
      "2018-08-11 08:19:56.512788 40 step\n",
      "2018-08-11 08:20:03.151102 60 step\n",
      "2018-08-11 08:20:09.790767 80 step\n",
      "2018-08-11 08:20:16.402558 100 step\n",
      "2018-08-11 08:20:20.751496 Start validation\n",
      "2018-08-11 08:20:26.705444 Validation Accuracy = 0.9174\n",
      "2018-08-11 08:20:26.705598 Epoch number: 7\n",
      "2018-08-11 08:20:27.271263 0 step\n",
      "2018-08-11 08:20:33.934455 20 step\n",
      "2018-08-11 08:20:40.567616 40 step\n",
      "2018-08-11 08:20:47.416391 60 step\n",
      "2018-08-11 08:20:54.056156 80 step\n",
      "2018-08-11 08:21:00.665792 100 step\n",
      "2018-08-11 08:21:04.960393 Start validation\n",
      "2018-08-11 08:21:10.903765 Validation Accuracy = 0.9074\n",
      "2018-08-11 08:21:10.903906 Epoch number: 8\n",
      "2018-08-11 08:21:11.445895 0 step\n",
      "2018-08-11 08:21:18.107814 20 step\n",
      "2018-08-11 08:21:24.699436 40 step\n",
      "2018-08-11 08:21:31.360944 60 step\n",
      "2018-08-11 08:21:38.028587 80 step\n",
      "2018-08-11 08:21:44.637419 100 step\n",
      "2018-08-11 08:21:48.900613 Start validation\n",
      "2018-08-11 08:21:54.909882 Validation Accuracy = 0.9431\n",
      "2018-08-11 08:21:54.910223 Epoch number: 9\n",
      "2018-08-11 08:21:55.471228 0 step\n",
      "2018-08-11 08:22:02.232972 20 step\n",
      "2018-08-11 08:22:08.898356 40 step\n",
      "2018-08-11 08:22:15.505780 60 step\n",
      "2018-08-11 08:22:22.180645 80 step\n",
      "2018-08-11 08:22:28.813013 100 step\n",
      "2018-08-11 08:22:33.098951 Start validation\n",
      "2018-08-11 08:22:39.097191 Validation Accuracy = 0.9498\n",
      "2018-08-11 08:22:39.097338 Epoch number: 10\n",
      "2018-08-11 08:22:39.651994 0 step\n",
      "2018-08-11 08:22:46.422115 20 step\n",
      "2018-08-11 08:22:53.104805 40 step\n",
      "2018-08-11 08:22:59.809724 60 step\n",
      "2018-08-11 08:23:06.464102 80 step\n",
      "2018-08-11 08:23:13.070656 100 step\n",
      "2018-08-11 08:23:17.357594 Start validation\n",
      "2018-08-11 08:23:23.266088 Validation Accuracy = 0.9621\n",
      "2018-08-11 08:23:23.266242 Epoch number: 11\n",
      "2018-08-11 08:23:23.813265 0 step\n",
      "2018-08-11 08:23:30.511579 20 step\n",
      "2018-08-11 08:23:37.138791 40 step\n",
      "2018-08-11 08:23:43.806135 60 step\n",
      "2018-08-11 08:23:50.385133 80 step\n",
      "2018-08-11 08:23:57.001295 100 step\n",
      "2018-08-11 08:24:01.234828 Start validation\n",
      "2018-08-11 08:24:07.267326 Validation Accuracy = 0.9643\n",
      "2018-08-11 08:24:07.267668 Epoch number: 12\n",
      "2018-08-11 08:24:07.794789 0 step\n",
      "2018-08-11 08:24:14.518691 20 step\n",
      "2018-08-11 08:24:21.232759 40 step\n",
      "2018-08-11 08:24:27.922520 60 step\n",
      "2018-08-11 08:24:34.595403 80 step\n",
      "2018-08-11 08:24:41.206605 100 step\n",
      "2018-08-11 08:24:45.420445 Start validation\n",
      "2018-08-11 08:24:51.380187 Validation Accuracy = 0.9286\n",
      "2018-08-11 08:24:51.380553 Epoch number: 13\n",
      "2018-08-11 08:24:51.930534 0 step\n",
      "2018-08-11 08:24:58.608633 20 step\n",
      "2018-08-11 08:25:05.298603 40 step\n",
      "2018-08-11 08:25:11.989984 60 step\n",
      "2018-08-11 08:25:18.639284 80 step\n",
      "2018-08-11 08:25:25.302570 100 step\n",
      "2018-08-11 08:25:29.536599 Start validation\n",
      "2018-08-11 08:25:35.535038 Validation Accuracy = 0.9408\n",
      "2018-08-11 08:25:35.535178 Epoch number: 14\n",
      "2018-08-11 08:25:36.104284 0 step\n",
      "2018-08-11 08:25:42.798233 20 step\n",
      "2018-08-11 08:25:49.469957 40 step\n",
      "2018-08-11 08:25:56.080393 60 step\n",
      "2018-08-11 08:26:02.701594 80 step\n",
      "2018-08-11 08:26:09.344395 100 step\n",
      "2018-08-11 08:26:13.581823 Start validation\n",
      "2018-08-11 08:26:19.577918 Validation Accuracy = 0.9598\n",
      "2018-08-11 08:26:19.578060 Epoch number: 15\n",
      "2018-08-11 08:26:20.146670 0 step\n",
      "2018-08-11 08:26:26.848060 20 step\n",
      "2018-08-11 08:26:33.590378 40 step\n",
      "2018-08-11 08:26:40.226649 60 step\n",
      "2018-08-11 08:26:46.836208 80 step\n",
      "2018-08-11 08:26:53.474815 100 step\n",
      "2018-08-11 08:26:57.713159 Start validation\n",
      "2018-08-11 08:27:03.690959 Validation Accuracy = 0.9498\n",
      "2018-08-11 08:27:03.691115 Epoch number: 16\n",
      "2018-08-11 08:27:04.245606 0 step\n",
      "2018-08-11 08:27:10.926113 20 step\n",
      "2018-08-11 08:27:17.601893 40 step\n",
      "2018-08-11 08:27:24.316541 60 step\n",
      "2018-08-11 08:27:30.944175 80 step\n",
      "2018-08-11 08:27:37.632650 100 step\n",
      "2018-08-11 08:27:41.862271 Start validation\n",
      "2018-08-11 08:27:47.998917 Validation Accuracy = 0.9520\n",
      "2018-08-11 08:27:47.999059 Epoch number: 17\n",
      "2018-08-11 08:27:48.530620 0 step\n",
      "2018-08-11 08:27:55.243688 20 step\n",
      "2018-08-11 08:28:01.806538 40 step\n",
      "2018-08-11 08:28:08.519722 60 step\n",
      "2018-08-11 08:28:15.148534 80 step\n",
      "2018-08-11 08:28:21.853936 100 step\n",
      "2018-08-11 08:28:26.141304 Start validation\n",
      "2018-08-11 08:28:32.130045 Validation Accuracy = 0.9688\n",
      "2018-08-11 08:28:32.130174 Epoch number: 18\n",
      "2018-08-11 08:28:32.663120 0 step\n",
      "2018-08-11 08:28:39.414332 20 step\n",
      "2018-08-11 08:28:46.065614 40 step\n",
      "2018-08-11 08:28:52.701278 60 step\n",
      "2018-08-11 08:28:59.347206 80 step\n",
      "2018-08-11 08:29:06.020140 100 step\n",
      "2018-08-11 08:29:10.234096 Start validation\n",
      "2018-08-11 08:29:16.258912 Validation Accuracy = 0.9676\n",
      "2018-08-11 08:29:16.259060 Epoch number: 19\n",
      "2018-08-11 08:29:16.785554 0 step\n",
      "2018-08-11 08:29:23.474068 20 step\n",
      "2018-08-11 08:29:30.130063 40 step\n",
      "2018-08-11 08:29:36.747298 60 step\n",
      "2018-08-11 08:29:43.406024 80 step\n",
      "2018-08-11 08:29:49.981808 100 step\n",
      "2018-08-11 08:29:54.237658 Start validation\n",
      "2018-08-11 08:30:00.286323 Validation Accuracy = 0.9710\n",
      "2018-08-11 08:30:00.286515 Epoch number: 20\n",
      "2018-08-11 08:30:00.762168 0 step\n",
      "2018-08-11 08:30:07.536510 20 step\n",
      "2018-08-11 08:30:14.123369 40 step\n",
      "2018-08-11 08:30:20.771578 60 step\n",
      "2018-08-11 08:30:27.400596 80 step\n",
      "2018-08-11 08:30:33.965162 100 step\n",
      "2018-08-11 08:30:38.164522 Start validation\n",
      "2018-08-11 08:30:44.111848 Validation Accuracy = 0.9688\n",
      "2018-08-11 08:30:44.112036 Epoch number: 21\n",
      "2018-08-11 08:30:44.627510 0 step\n",
      "2018-08-11 08:30:51.305644 20 step\n",
      "2018-08-11 08:30:58.074755 40 step\n",
      "2018-08-11 08:31:04.685809 60 step\n",
      "2018-08-11 08:31:11.321409 80 step\n",
      "2018-08-11 08:31:17.948779 100 step\n",
      "2018-08-11 08:31:22.180940 Start validation\n",
      "2018-08-11 08:31:28.159201 Validation Accuracy = 0.9509\n",
      "2018-08-11 08:31:28.159307 Epoch number: 22\n",
      "2018-08-11 08:31:28.654200 0 step\n",
      "2018-08-11 08:31:35.317148 20 step\n",
      "2018-08-11 08:31:42.113133 40 step\n",
      "2018-08-11 08:31:48.741429 60 step\n",
      "2018-08-11 08:31:55.353572 80 step\n",
      "2018-08-11 08:32:01.944773 100 step\n",
      "2018-08-11 08:32:06.214836 Start validation\n",
      "2018-08-11 08:32:12.226077 Validation Accuracy = 0.9576\n",
      "2018-08-11 08:32:12.226209 Epoch number: 23\n",
      "2018-08-11 08:32:12.714093 0 step\n",
      "2018-08-11 08:32:19.424256 20 step\n",
      "2018-08-11 08:32:26.043975 40 step\n",
      "2018-08-11 08:32:32.711203 60 step\n",
      "2018-08-11 08:32:39.357806 80 step\n",
      "2018-08-11 08:32:46.070555 100 step\n",
      "2018-08-11 08:32:50.267135 Start validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 08:32:56.111159 Validation Accuracy = 0.9665\n",
      "2018-08-11 08:32:56.111358 Epoch number: 24\n",
      "2018-08-11 08:32:56.611220 0 step\n",
      "2018-08-11 08:33:03.287508 20 step\n",
      "2018-08-11 08:33:09.969808 40 step\n",
      "2018-08-11 08:33:16.799664 60 step\n",
      "2018-08-11 08:33:23.482201 80 step\n",
      "2018-08-11 08:33:30.152074 100 step\n",
      "2018-08-11 08:33:34.362912 Start validation\n",
      "2018-08-11 08:33:40.329875 Validation Accuracy = 0.9598\n",
      "2018-08-11 08:33:40.330028 Epoch number: 25\n",
      "2018-08-11 08:33:40.820155 0 step\n",
      "2018-08-11 08:33:47.454582 20 step\n",
      "2018-08-11 08:33:54.046897 40 step\n",
      "2018-08-11 08:34:00.687723 60 step\n",
      "2018-08-11 08:34:07.348554 80 step\n",
      "2018-08-11 08:34:14.044630 100 step\n",
      "2018-08-11 08:34:18.293370 Start validation\n",
      "2018-08-11 08:34:24.187407 Validation Accuracy = 0.9676\n",
      "2018-08-11 08:34:24.187597 Epoch number: 26\n",
      "2018-08-11 08:34:24.680499 0 step\n",
      "2018-08-11 08:34:31.356216 20 step\n",
      "2018-08-11 08:34:37.963286 40 step\n",
      "2018-08-11 08:34:44.548206 60 step\n",
      "2018-08-11 08:34:51.228390 80 step\n",
      "2018-08-11 08:34:57.871776 100 step\n",
      "2018-08-11 08:35:02.126759 Start validation\n",
      "2018-08-11 08:35:08.081161 Validation Accuracy = 0.9654\n",
      "2018-08-11 08:35:08.081262 Epoch number: 27\n",
      "2018-08-11 08:35:08.559303 0 step\n",
      "2018-08-11 08:35:15.247737 20 step\n",
      "2018-08-11 08:35:21.891329 40 step\n",
      "2018-08-11 08:35:28.567050 60 step\n",
      "2018-08-11 08:35:35.179576 80 step\n",
      "2018-08-11 08:35:41.779897 100 step\n",
      "2018-08-11 08:35:46.032248 Start validation\n",
      "2018-08-11 08:35:51.964344 Validation Accuracy = 0.9710\n",
      "2018-08-11 08:35:51.964546 Epoch number: 28\n",
      "2018-08-11 08:35:52.451680 0 step\n",
      "2018-08-11 08:35:59.047534 20 step\n",
      "2018-08-11 08:36:05.817144 40 step\n",
      "2018-08-11 08:36:12.483912 60 step\n",
      "2018-08-11 08:36:19.135958 80 step\n",
      "2018-08-11 08:36:25.824189 100 step\n",
      "2018-08-11 08:36:30.020843 Start validation\n",
      "2018-08-11 08:36:35.951498 Validation Accuracy = 0.9609\n",
      "2018-08-11 08:36:35.951663 Epoch number: 29\n",
      "2018-08-11 08:36:36.430921 0 step\n",
      "2018-08-11 08:36:43.106405 20 step\n",
      "2018-08-11 08:36:49.726434 40 step\n",
      "2018-08-11 08:36:56.372593 60 step\n",
      "2018-08-11 08:37:03.132636 80 step\n",
      "2018-08-11 08:37:09.769676 100 step\n",
      "2018-08-11 08:37:14.049108 Start validation\n",
      "2018-08-11 08:37:19.979339 Validation Accuracy = 0.9676\n",
      "2018-08-11 08:37:19.979469 Epoch number: 30\n",
      "2018-08-11 08:37:20.463713 0 step\n",
      "2018-08-11 08:37:27.068998 20 step\n",
      "2018-08-11 08:37:33.688680 40 step\n",
      "2018-08-11 08:37:40.369987 60 step\n",
      "2018-08-11 08:37:46.995352 80 step\n",
      "2018-08-11 08:37:53.591607 100 step\n",
      "2018-08-11 08:37:57.835694 Start validation\n",
      "502\n",
      "1274\n",
      "385\n",
      "502\n",
      "502\n",
      "1283\n",
      "2011\n",
      "49\n",
      "1901\n",
      "2381\n",
      "1248\n",
      "1157\n",
      "385\n",
      "2088\n",
      "876\n",
      "1609\n",
      "849\n",
      "1355\n",
      "603\n",
      "537\n",
      "1604\n",
      "1283\n",
      "1867\n",
      "1867\n",
      "1912\n",
      "1355\n",
      "1511\n",
      "1355\n",
      "2018-08-11 08:38:03.783206 Validation Accuracy = 0.9699\n",
      "2018-08-11 08:38:03.783259 Saving checkpoint of model...\n",
      "2018-08-11 08:38:17.214574 Model checkpoint saved at /hdd3/dhj_container/ICROS_vgg/vggch08113/model_epoch30.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "vaild_result = np.array([])\n",
    "vaild_result.resize((training_epochs,1))\n",
    "\n",
    "\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "   \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open tensorboard --logdir={}\".format(datetime.now(),\n",
    "                                                      filewriter_path))\n",
    "    \n",
    "    img_batch = np.zeros((batch_size,224,224,3), dtype ='uint8')\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "       \n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "       \n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        for step in range(train_batches_per_epoch):\n",
    "\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)       \n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: img_batch, y: label_batch})\n",
    "\n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step % display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict={x: img_batch,\n",
    "                                                        y: label_batch})\n",
    "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "                print(\"{} {} step\".format(datetime.now(), step))\n",
    "\n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        sess.run(validation_init_op1)\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        wrong_cnt = 0\n",
    "        pre = np.array([])\n",
    "        pre.resize((val_batches_per_epoch1,batch_size))\n",
    "        \n",
    "        for a in range(val_batches_per_epoch1):\n",
    "\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "            acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            \n",
    "            if epoch == (training_epochs-1):\n",
    "                pre = pre.astype('uint32')\n",
    "                for i in range(batch_size):\n",
    "                    if pre[test_count][i] == False:\n",
    "                        order = step*batch_size + i\n",
    "                        set_num = num+1\n",
    "                        temp = tr_data_dir['train%d'%set_num][order]\n",
    "                        name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                        print(name)\n",
    "            \n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "            \n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
    "                                                       test_acc))\n",
    "        vaild_result[epoch] = test_acc\n",
    "        \n",
    "        if epoch == training_epochs-1 :\n",
    "            print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "            # save checkpoint of the model\n",
    "            checkpoint_name = os.path.join(checkpoint_path,\n",
    "                                           'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "            save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "            print(\"{} Model checkpoint saved at {}\".format(datetime.now(),\n",
    "                                                           checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71428571],\n",
       "       [0.76227679],\n",
       "       [0.82924107],\n",
       "       [0.87165179],\n",
       "       [0.90178571],\n",
       "       [0.91741071],\n",
       "       [0.90736607],\n",
       "       [0.94308036],\n",
       "       [0.94977679],\n",
       "       [0.96205357],\n",
       "       [0.96428571],\n",
       "       [0.92857143],\n",
       "       [0.94084821],\n",
       "       [0.95982143],\n",
       "       [0.94977679],\n",
       "       [0.95200893],\n",
       "       [0.96875   ],\n",
       "       [0.96763393],\n",
       "       [0.97098214],\n",
       "       [0.96875   ],\n",
       "       [0.95089286],\n",
       "       [0.95758929],\n",
       "       [0.96651786],\n",
       "       [0.95982143],\n",
       "       [0.96763393],\n",
       "       [0.96540179],\n",
       "       [0.97098214],\n",
       "       [0.9609375 ],\n",
       "       [0.96763393],\n",
       "       [0.96986607]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaild_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맞추면 1, 틀리면 0\n",
    "test_count = 0\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "pre = np.array([])\n",
    "pre.resize((val_batches_per_epoch1,batch_size))\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    img_batch = np.zeros((batch_size,227,227,3), dtype ='uint8')\n",
    "    for epoch in range(training_epochs):\n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(validation_init_op1)\n",
    "        for step in range(val_batches_per_epoch1):\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)    \n",
    "            #print(len(label_batch))\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre = pre.astype('uint32')\n",
    "            for i in range(batch_size):\n",
    "                if pre[test_count][i] == False:\n",
    "                    order = step*batch_size + i\n",
    "                    set_num = num+1\n",
    "                    temp = tr_data_dir['train%d'%set_num][order]\n",
    "                    name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                    print(name)\n",
    "            test_count+=1\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
