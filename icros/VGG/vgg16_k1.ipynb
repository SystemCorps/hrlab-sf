{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import inception_v3\n",
    "import vgg\n",
    "from datagenerator import ImageDataGenerator\n",
    "from tensorflow.contrib.data import Iterator\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train2', 'valid2', 'train3', 'train1', 'train0', 'valid3', 'valid4', 'valid1', 'valid0', 'train4'])\n"
     ]
    }
   ],
   "source": [
    "# Use same number for training and validation\n",
    "# Ex) 0th folding -> 'train0' for training, 'valid0' for validation\n",
    "data_txt = open('/hrlab-sf/icros/data_split/5fold_0802.txt', 'r')\n",
    "data_json = data_txt.read()\n",
    "tr_data_dir = json.loads(data_json)\n",
    "print(tr_data_dir.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the textfiles for the trainings and validation set\n",
    "num = 1\n",
    "date = \"20180810\"\n",
    "train_file = './train%d_HR_ICROS.txt'%num\n",
    "val_file1 = './valid%d_HR_ICROS.txt'%num\n",
    "batch_size = 32\n",
    "num_classes = 2\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 30\n",
    "display_step = 20\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = \"/hdd3/dhj_container/ICROS_vgg/vgg0811%d/\"% num\n",
    "checkpoint_path = \"/hdd3/dhj_container/ICROS_vgg/vggch0811%d\"% num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /hrlab-sf/icros/VGG/datagenerator.py:66: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:From /hrlab-sf/icros/VGG/datagenerator.py:71: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "114\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # data load\n",
    "    tr_data = ImageDataGenerator(train_file,\n",
    "                                 mode='training',\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_classes=num_classes,\n",
    "                                 shuffle=False)\n",
    "    val_data1 = ImageDataGenerator(val_file1,\n",
    "                                  mode='inference',\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_classes=num_classes,\n",
    "                                  shuffle=False)\n",
    "\n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(tr_data.data.output_types,\n",
    "                                           tr_data.data.output_shapes)\n",
    "\n",
    "    next_batch = iterator.get_next()\n",
    "    \n",
    "\n",
    "    # Ops for initializing the two different iterators\n",
    "    training_init_op = iterator.make_initializer(tr_data.data)\n",
    "    validation_init_op1 = iterator.make_initializer(val_data1.data)\n",
    "\n",
    "    # TF placeholder for graph input and output\n",
    "    x = tf.placeholder(tf.float32, [batch_size, 224, 224, 3])\n",
    "    y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "\n",
    "\n",
    "\n",
    "    net, net_points = vgg.vgg_16(x,\n",
    "                                num_classes=num_classes,\n",
    "                                dropout_keep_prob=0.5\n",
    "                               )\n",
    "        \n",
    "    # Op for calculating the loss\n",
    "    with tf.name_scope(\"cross_ent\"):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "                \n",
    "    # Train op\n",
    "    with tf.name_scope(\"train\"):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net, labels=y))\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "            \n",
    "    # Add the loss to summary\n",
    "    tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(net, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # Add the accuracy to the summary\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all summaries together\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Initialize the FileWriter\n",
    "    writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "    # Initialize an saver for store model checkpoints\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    train_batches_per_epoch = int(np.floor(tr_data.data_size / batch_size))\n",
    "    val_batches_per_epoch1 = int(np.floor(val_data1.data_size / batch_size)) \n",
    "    print(train_batches_per_epoch)\n",
    "    print(val_batches_per_epoch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 07:56:12.401332 Start training...\n",
      "2018-08-11 07:56:12.401616 Open tensorboard --logdir=/hdd3/dhj_container/ICROS_vgg/vgg08111/\n",
      "2018-08-11 07:56:12.403301 Epoch number: 1\n",
      "2018-08-11 07:56:16.063930 0 step\n",
      "2018-08-11 07:56:22.525180 20 step\n",
      "2018-08-11 07:56:28.992175 40 step\n",
      "2018-08-11 07:56:35.512083 60 step\n",
      "2018-08-11 07:56:42.025440 80 step\n",
      "2018-08-11 07:56:48.581252 100 step\n",
      "2018-08-11 07:56:52.788854 Start validation\n",
      "2018-08-11 07:56:58.835262 Validation Accuracy = 0.7388\n",
      "2018-08-11 07:56:58.835381 Epoch number: 2\n",
      "2018-08-11 07:56:59.320288 0 step\n",
      "2018-08-11 07:57:05.851841 20 step\n",
      "2018-08-11 07:57:12.417124 40 step\n",
      "2018-08-11 07:57:18.995051 60 step\n",
      "2018-08-11 07:57:25.556336 80 step\n",
      "2018-08-11 07:57:32.112381 100 step\n",
      "2018-08-11 07:57:36.417842 Start validation\n",
      "2018-08-11 07:57:42.577486 Validation Accuracy = 0.7846\n",
      "2018-08-11 07:57:42.577634 Epoch number: 3\n",
      "2018-08-11 07:57:43.066544 0 step\n",
      "2018-08-11 07:57:49.798825 20 step\n",
      "2018-08-11 07:57:56.424785 40 step\n",
      "2018-08-11 07:58:03.031609 60 step\n",
      "2018-08-11 07:58:09.653196 80 step\n",
      "2018-08-11 07:58:16.337915 100 step\n",
      "2018-08-11 07:58:20.651821 Start validation\n",
      "2018-08-11 07:58:26.873025 Validation Accuracy = 0.8092\n",
      "2018-08-11 07:58:26.873130 Epoch number: 4\n",
      "2018-08-11 07:58:27.379836 0 step\n",
      "2018-08-11 07:58:34.027140 20 step\n",
      "2018-08-11 07:58:40.669237 40 step\n",
      "2018-08-11 07:58:47.311403 60 step\n",
      "2018-08-11 07:58:53.947247 80 step\n",
      "2018-08-11 07:59:00.741878 100 step\n",
      "2018-08-11 07:59:05.108116 Start validation\n",
      "2018-08-11 07:59:11.181467 Validation Accuracy = 0.8728\n",
      "2018-08-11 07:59:11.181594 Epoch number: 5\n",
      "2018-08-11 07:59:11.672179 0 step\n",
      "2018-08-11 07:59:18.309655 20 step\n",
      "2018-08-11 07:59:24.934458 40 step\n",
      "2018-08-11 07:59:31.612887 60 step\n",
      "2018-08-11 07:59:38.251162 80 step\n",
      "2018-08-11 07:59:44.986523 100 step\n",
      "2018-08-11 07:59:49.318867 Start validation\n",
      "2018-08-11 07:59:55.430372 Validation Accuracy = 0.9219\n",
      "2018-08-11 07:59:55.430499 Epoch number: 6\n",
      "2018-08-11 07:59:55.922608 0 step\n",
      "2018-08-11 08:00:02.628077 20 step\n",
      "2018-08-11 08:00:09.289437 40 step\n",
      "2018-08-11 08:00:15.913059 60 step\n",
      "2018-08-11 08:00:22.627750 80 step\n",
      "2018-08-11 08:00:29.342371 100 step\n",
      "2018-08-11 08:00:33.685155 Start validation\n",
      "2018-08-11 08:00:39.760750 Validation Accuracy = 0.9330\n",
      "2018-08-11 08:00:39.761168 Epoch number: 7\n",
      "2018-08-11 08:00:40.266465 0 step\n",
      "2018-08-11 08:00:46.891176 20 step\n",
      "2018-08-11 08:00:53.532850 40 step\n",
      "2018-08-11 08:01:00.171950 60 step\n",
      "2018-08-11 08:01:06.875317 80 step\n",
      "2018-08-11 08:01:13.669947 100 step\n",
      "2018-08-11 08:01:17.986657 Start validation\n",
      "2018-08-11 08:01:24.037532 Validation Accuracy = 0.9554\n",
      "2018-08-11 08:01:24.037733 Epoch number: 8\n",
      "2018-08-11 08:01:24.531946 0 step\n",
      "2018-08-11 08:01:31.259223 20 step\n",
      "2018-08-11 08:01:38.017093 40 step\n",
      "2018-08-11 08:01:44.712907 60 step\n",
      "2018-08-11 08:01:51.330409 80 step\n",
      "2018-08-11 08:01:58.028300 100 step\n",
      "2018-08-11 08:02:02.429661 Start validation\n",
      "2018-08-11 08:02:08.557557 Validation Accuracy = 0.9766\n",
      "2018-08-11 08:02:08.557689 Epoch number: 9\n",
      "2018-08-11 08:02:09.050415 0 step\n",
      "2018-08-11 08:02:15.659785 20 step\n",
      "2018-08-11 08:02:22.303351 40 step\n",
      "2018-08-11 08:02:29.007372 60 step\n",
      "2018-08-11 08:02:35.654034 80 step\n",
      "2018-08-11 08:02:42.432088 100 step\n",
      "2018-08-11 08:02:46.707185 Start validation\n",
      "2018-08-11 08:02:52.801530 Validation Accuracy = 0.9743\n",
      "2018-08-11 08:02:52.801719 Epoch number: 10\n",
      "2018-08-11 08:02:53.287572 0 step\n",
      "2018-08-11 08:02:59.937835 20 step\n",
      "2018-08-11 08:03:06.595163 40 step\n",
      "2018-08-11 08:03:13.321298 60 step\n",
      "2018-08-11 08:03:20.043866 80 step\n",
      "2018-08-11 08:03:26.861783 100 step\n",
      "2018-08-11 08:03:31.180284 Start validation\n",
      "2018-08-11 08:03:37.278954 Validation Accuracy = 0.9442\n",
      "2018-08-11 08:03:37.279065 Epoch number: 11\n",
      "2018-08-11 08:03:37.759557 0 step\n",
      "2018-08-11 08:03:44.392755 20 step\n",
      "2018-08-11 08:03:51.056110 40 step\n",
      "2018-08-11 08:03:57.687578 60 step\n",
      "2018-08-11 08:04:04.368118 80 step\n",
      "2018-08-11 08:04:11.095161 100 step\n",
      "2018-08-11 08:04:15.383014 Start validation\n",
      "2018-08-11 08:04:21.471758 Validation Accuracy = 0.9721\n",
      "2018-08-11 08:04:21.471871 Epoch number: 12\n",
      "2018-08-11 08:04:21.953555 0 step\n",
      "2018-08-11 08:04:28.629716 20 step\n",
      "2018-08-11 08:04:35.308552 40 step\n",
      "2018-08-11 08:04:41.951209 60 step\n",
      "2018-08-11 08:04:48.640193 80 step\n",
      "2018-08-11 08:04:55.362718 100 step\n",
      "2018-08-11 08:04:59.694269 Start validation\n",
      "2018-08-11 08:05:05.753383 Validation Accuracy = 0.9788\n",
      "2018-08-11 08:05:05.753564 Epoch number: 13\n",
      "2018-08-11 08:05:06.238399 0 step\n",
      "2018-08-11 08:05:12.829068 20 step\n",
      "2018-08-11 08:05:19.523447 40 step\n",
      "2018-08-11 08:05:26.174981 60 step\n",
      "2018-08-11 08:05:32.896960 80 step\n",
      "2018-08-11 08:05:39.767736 100 step\n",
      "2018-08-11 08:05:44.022023 Start validation\n",
      "2018-08-11 08:05:50.032912 Validation Accuracy = 0.9732\n",
      "2018-08-11 08:05:50.033080 Epoch number: 14\n",
      "2018-08-11 08:05:50.539569 0 step\n",
      "2018-08-11 08:05:57.235808 20 step\n",
      "2018-08-11 08:06:03.834838 40 step\n",
      "2018-08-11 08:06:10.477013 60 step\n",
      "2018-08-11 08:06:17.137315 80 step\n",
      "2018-08-11 08:06:23.975530 100 step\n",
      "2018-08-11 08:06:28.266287 Start validation\n",
      "2018-08-11 08:06:34.416664 Validation Accuracy = 0.9754\n",
      "2018-08-11 08:06:34.416792 Epoch number: 15\n",
      "2018-08-11 08:06:34.895749 0 step\n",
      "2018-08-11 08:06:41.545138 20 step\n",
      "2018-08-11 08:06:48.140212 40 step\n",
      "2018-08-11 08:06:54.821037 60 step\n",
      "2018-08-11 08:07:01.547627 80 step\n",
      "2018-08-11 08:07:08.292847 100 step\n",
      "2018-08-11 08:07:12.527520 Start validation\n",
      "2018-08-11 08:07:18.580942 Validation Accuracy = 0.9777\n",
      "2018-08-11 08:07:18.581077 Epoch number: 16\n",
      "2018-08-11 08:07:19.093727 0 step\n",
      "2018-08-11 08:07:25.741662 20 step\n",
      "2018-08-11 08:07:32.383123 40 step\n",
      "2018-08-11 08:07:39.054519 60 step\n",
      "2018-08-11 08:07:45.713004 80 step\n",
      "2018-08-11 08:07:52.529574 100 step\n",
      "2018-08-11 08:07:56.798966 Start validation\n",
      "2018-08-11 08:08:02.909715 Validation Accuracy = 0.9710\n",
      "2018-08-11 08:08:02.910115 Epoch number: 17\n",
      "2018-08-11 08:08:03.394605 0 step\n",
      "2018-08-11 08:08:10.019895 20 step\n",
      "2018-08-11 08:08:16.626721 40 step\n",
      "2018-08-11 08:08:23.260339 60 step\n",
      "2018-08-11 08:08:29.966990 80 step\n",
      "2018-08-11 08:08:36.680571 100 step\n",
      "2018-08-11 08:08:41.012104 Start validation\n",
      "2018-08-11 08:08:46.981972 Validation Accuracy = 0.9866\n",
      "2018-08-11 08:08:46.982269 Epoch number: 18\n",
      "2018-08-11 08:08:47.462747 0 step\n",
      "2018-08-11 08:08:54.108835 20 step\n",
      "2018-08-11 08:09:00.743805 40 step\n",
      "2018-08-11 08:09:07.427983 60 step\n",
      "2018-08-11 08:09:14.073351 80 step\n",
      "2018-08-11 08:09:20.789018 100 step\n",
      "2018-08-11 08:09:25.040064 Start validation\n",
      "2018-08-11 08:09:30.960374 Validation Accuracy = 0.9888\n",
      "2018-08-11 08:09:30.960485 Epoch number: 19\n",
      "2018-08-11 08:09:31.442995 0 step\n",
      "2018-08-11 08:09:38.083103 20 step\n",
      "2018-08-11 08:09:44.771031 40 step\n",
      "2018-08-11 08:09:51.385035 60 step\n",
      "2018-08-11 08:09:58.025124 80 step\n",
      "2018-08-11 08:10:04.687691 100 step\n",
      "2018-08-11 08:10:09.026893 Start validation\n",
      "2018-08-11 08:10:14.983999 Validation Accuracy = 0.9833\n",
      "2018-08-11 08:10:14.984187 Epoch number: 20\n",
      "2018-08-11 08:10:15.475904 0 step\n",
      "2018-08-11 08:10:22.173536 20 step\n",
      "2018-08-11 08:10:28.840595 40 step\n",
      "2018-08-11 08:10:35.430628 60 step\n",
      "2018-08-11 08:10:42.142400 80 step\n",
      "2018-08-11 08:10:48.859952 100 step\n",
      "2018-08-11 08:10:53.135778 Start validation\n",
      "2018-08-11 08:10:59.047282 Validation Accuracy = 0.9754\n",
      "2018-08-11 08:10:59.047436 Epoch number: 21\n",
      "2018-08-11 08:10:59.535691 0 step\n",
      "2018-08-11 08:11:06.179908 20 step\n",
      "2018-08-11 08:11:12.788365 40 step\n",
      "2018-08-11 08:11:19.407733 60 step\n",
      "2018-08-11 08:11:26.099151 80 step\n",
      "2018-08-11 08:11:32.792030 100 step\n",
      "2018-08-11 08:11:37.080826 Start validation\n",
      "2018-08-11 08:11:42.970710 Validation Accuracy = 0.9900\n",
      "2018-08-11 08:11:42.970825 Epoch number: 22\n",
      "2018-08-11 08:11:43.502646 0 step\n",
      "2018-08-11 08:11:50.138431 20 step\n",
      "2018-08-11 08:11:56.752180 40 step\n",
      "2018-08-11 08:12:03.391678 60 step\n",
      "2018-08-11 08:12:10.065097 80 step\n",
      "2018-08-11 08:12:16.766905 100 step\n",
      "2018-08-11 08:12:21.050918 Start validation\n",
      "2018-08-11 08:12:26.972974 Validation Accuracy = 0.9743\n",
      "2018-08-11 08:12:26.973115 Epoch number: 23\n",
      "2018-08-11 08:12:27.467703 0 step\n",
      "2018-08-11 08:12:34.093893 20 step\n",
      "2018-08-11 08:12:40.742187 40 step\n",
      "2018-08-11 08:12:47.334767 60 step\n",
      "2018-08-11 08:12:54.031976 80 step\n",
      "2018-08-11 08:13:00.719731 100 step\n",
      "2018-08-11 08:13:05.003051 Start validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-11 08:13:10.931947 Validation Accuracy = 0.9799\n",
      "2018-08-11 08:13:10.932136 Epoch number: 24\n",
      "2018-08-11 08:13:11.436280 0 step\n",
      "2018-08-11 08:13:18.052472 20 step\n",
      "2018-08-11 08:13:24.733234 40 step\n",
      "2018-08-11 08:13:31.381524 60 step\n",
      "2018-08-11 08:13:38.091396 80 step\n",
      "2018-08-11 08:13:44.732766 100 step\n",
      "2018-08-11 08:13:48.959236 Start validation\n",
      "2018-08-11 08:13:54.939298 Validation Accuracy = 0.9621\n",
      "2018-08-11 08:13:54.939410 Epoch number: 25\n",
      "2018-08-11 08:13:55.420098 0 step\n",
      "2018-08-11 08:14:02.069594 20 step\n",
      "2018-08-11 08:14:08.725017 40 step\n",
      "2018-08-11 08:14:15.360792 60 step\n",
      "2018-08-11 08:14:22.021398 80 step\n",
      "2018-08-11 08:14:28.722349 100 step\n",
      "2018-08-11 08:14:32.979215 Start validation\n",
      "2018-08-11 08:14:38.893734 Validation Accuracy = 0.9777\n",
      "2018-08-11 08:14:38.893920 Epoch number: 26\n",
      "2018-08-11 08:14:39.412859 0 step\n",
      "2018-08-11 08:14:46.091881 20 step\n",
      "2018-08-11 08:14:52.715310 40 step\n",
      "2018-08-11 08:14:59.313873 60 step\n",
      "2018-08-11 08:15:06.003365 80 step\n",
      "2018-08-11 08:15:12.687360 100 step\n",
      "2018-08-11 08:15:16.897107 Start validation\n",
      "2018-08-11 08:15:22.819525 Validation Accuracy = 0.9833\n",
      "2018-08-11 08:15:22.819681 Epoch number: 27\n",
      "2018-08-11 08:15:23.310362 0 step\n",
      "2018-08-11 08:15:29.980880 20 step\n",
      "2018-08-11 08:15:36.631668 40 step\n",
      "2018-08-11 08:15:43.295064 60 step\n",
      "2018-08-11 08:15:50.106116 80 step\n",
      "2018-08-11 08:15:56.718525 100 step\n",
      "2018-08-11 08:16:00.943828 Start validation\n",
      "2018-08-11 08:16:07.009228 Validation Accuracy = 0.9855\n",
      "2018-08-11 08:16:07.009397 Epoch number: 28\n",
      "2018-08-11 08:16:07.497497 0 step\n",
      "2018-08-11 08:16:14.099380 20 step\n",
      "2018-08-11 08:16:20.752291 40 step\n",
      "2018-08-11 08:16:27.413305 60 step\n",
      "2018-08-11 08:16:34.054935 80 step\n",
      "2018-08-11 08:16:40.633073 100 step\n",
      "2018-08-11 08:16:44.842147 Start validation\n",
      "2018-08-11 08:16:50.858757 Validation Accuracy = 0.9799\n",
      "2018-08-11 08:16:50.858908 Epoch number: 29\n",
      "2018-08-11 08:16:51.437341 0 step\n",
      "2018-08-11 08:16:58.166469 20 step\n",
      "2018-08-11 08:17:04.834740 40 step\n",
      "2018-08-11 08:17:11.486220 60 step\n",
      "2018-08-11 08:17:18.129923 80 step\n",
      "2018-08-11 08:17:24.732522 100 step\n",
      "2018-08-11 08:17:28.913468 Start validation\n",
      "2018-08-11 08:17:34.864606 Validation Accuracy = 0.9676\n",
      "2018-08-11 08:17:34.864760 Epoch number: 30\n",
      "2018-08-11 08:17:35.433864 0 step\n",
      "2018-08-11 08:17:42.178454 20 step\n",
      "2018-08-11 08:17:48.837719 40 step\n",
      "2018-08-11 08:17:55.419192 60 step\n",
      "2018-08-11 08:18:02.094799 80 step\n",
      "2018-08-11 08:18:08.709779 100 step\n",
      "2018-08-11 08:18:13.006772 Start validation\n",
      "1108\n",
      "1322\n",
      "361\n",
      "2249\n",
      "30\n",
      "404\n",
      "1708\n",
      "1993\n",
      "1999\n",
      "1054\n",
      "2080\n",
      "2288\n",
      "2067\n",
      "2080\n",
      "1708\n",
      "56\n",
      "1784\n",
      "1993\n",
      "1784\n",
      "1993\n",
      "1345\n",
      "121\n",
      "1478\n",
      "2018-08-11 08:18:19.036894 Validation Accuracy = 0.9799\n",
      "2018-08-11 08:18:19.036972 Saving checkpoint of model...\n",
      "2018-08-11 08:18:30.064382 Model checkpoint saved at /hdd3/dhj_container/ICROS_vgg/vggch08111/model_epoch30.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "vaild_result = np.array([])\n",
    "vaild_result.resize((training_epochs,1))\n",
    "\n",
    "\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Add the model graph to TensorBoard\n",
    "    writer.add_graph(sess.graph)\n",
    "   \n",
    "    print(\"{} Start training...\".format(datetime.now()))\n",
    "    print(\"{} Open tensorboard --logdir={}\".format(datetime.now(),\n",
    "                                                      filewriter_path))\n",
    "    \n",
    "    img_batch = np.zeros((batch_size,224,224,3), dtype ='uint8')\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "       \n",
    "        print(\"{} Epoch number: {}\".format(datetime.now(), epoch+1))\n",
    "       \n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(training_init_op)\n",
    "\n",
    "        for step in range(train_batches_per_epoch):\n",
    "\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)       \n",
    "\n",
    "            # And run the training op\n",
    "            sess.run(train_op, feed_dict={x: img_batch, y: label_batch})\n",
    "\n",
    "            # Generate summary with the current batch of data and write to file\n",
    "            if step % display_step == 0:\n",
    "                s = sess.run(merged_summary, feed_dict={x: img_batch,\n",
    "                                                        y: label_batch})\n",
    "                writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "                print(\"{} {} step\".format(datetime.now(), step))\n",
    "\n",
    "        # Validate the model on the entire validation set\n",
    "        print(\"{} Start validation\".format(datetime.now()))\n",
    "        sess.run(validation_init_op1)\n",
    "        test_acc = 0.\n",
    "        test_count = 0\n",
    "        wrong_cnt = 0\n",
    "        pre = np.array([])\n",
    "        pre.resize((val_batches_per_epoch1,batch_size))\n",
    "        \n",
    "        for a in range(val_batches_per_epoch1):\n",
    "\n",
    "            img_batch, label_batch = sess.run(next_batch)\n",
    "            acc = sess.run(accuracy, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            \n",
    "            if epoch == (training_epochs-1):\n",
    "                pre = pre.astype('uint32')\n",
    "                for i in range(batch_size):\n",
    "                    if pre[test_count][i] == False:\n",
    "                        order = step*batch_size + i\n",
    "                        set_num = num+1\n",
    "                        temp = tr_data_dir['train%d'%set_num][order]\n",
    "                        name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                        print(name)\n",
    "            \n",
    "            test_acc += acc\n",
    "            test_count += 1\n",
    "            \n",
    "        test_acc /= test_count\n",
    "        print(\"{} Validation Accuracy = {:.4f}\".format(datetime.now(),\n",
    "                                                       test_acc))\n",
    "        vaild_result[epoch] = test_acc\n",
    "        \n",
    "        if epoch == training_epochs-1 :\n",
    "            print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "            # save checkpoint of the model\n",
    "            checkpoint_name = os.path.join(checkpoint_path,\n",
    "                                           'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "            save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "            print(\"{} Model checkpoint saved at {}\".format(datetime.now(),\n",
    "                                                           checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73883929],\n",
       "       [0.78459821],\n",
       "       [0.80915179],\n",
       "       [0.87276786],\n",
       "       [0.921875  ],\n",
       "       [0.93303571],\n",
       "       [0.95535714],\n",
       "       [0.9765625 ],\n",
       "       [0.97433036],\n",
       "       [0.94419643],\n",
       "       [0.97209821],\n",
       "       [0.97879464],\n",
       "       [0.97321429],\n",
       "       [0.97544643],\n",
       "       [0.97767857],\n",
       "       [0.97098214],\n",
       "       [0.98660714],\n",
       "       [0.98883929],\n",
       "       [0.98325893],\n",
       "       [0.97544643],\n",
       "       [0.98995536],\n",
       "       [0.97433036],\n",
       "       [0.97991071],\n",
       "       [0.96205357],\n",
       "       [0.97767857],\n",
       "       [0.98325893],\n",
       "       [0.98549107],\n",
       "       [0.97991071],\n",
       "       [0.96763393],\n",
       "       [0.97991071]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaild_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맞추면 1, 틀리면 0\n",
    "test_count = 0\n",
    "config=tf.ConfigProto(allow_soft_placement = True, log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "pre = np.array([])\n",
    "pre.resize((val_batches_per_epoch1,batch_size))\n",
    "with tf.Session(config=config, graph=tf.get_default_graph()) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    img_batch = np.zeros((batch_size,227,227,3), dtype ='uint8')\n",
    "    for epoch in range(training_epochs):\n",
    "        # Initialize iterator with the training dataset\n",
    "        sess.run(validation_init_op1)\n",
    "        for step in range(val_batches_per_epoch1):\n",
    "            # get next batch of data\n",
    "            img_batch, label_batch = sess.run(next_batch)    \n",
    "            #print(len(label_batch))\n",
    "            pre[test_count] = sess.run(correct_prediction, feed_dict={x: img_batch,\n",
    "                                                y: label_batch})\n",
    "            pre = pre.astype('uint32')\n",
    "            for i in range(batch_size):\n",
    "                if pre[test_count][i] == False:\n",
    "                    order = step*batch_size + i\n",
    "                    set_num = num+1\n",
    "                    temp = tr_data_dir['train%d'%set_num][order]\n",
    "                    name = temp.split('/')[-1].split('.')[0].split('(')[1].split(')')[0]\n",
    "                    print(name)\n",
    "            test_count+=1\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
