{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Trainig Templete\n",
    "## Based on Tensorlfow - CNN MNIST example\n",
    "https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/examples/tutorials/layers/cnn_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 library 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.contrib.layers.python.layers import layers as layers_lib\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# GPU selection (memory)\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_txt = open('./data_split/data_dir_0801.txt', 'r')\n",
    "data_json = data_txt.read()\n",
    "tr_data_dir = json.loads(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info.\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "channel = 3\n",
    "\n",
    "\n",
    "# resize\n",
    "r_w = 224\n",
    "r_h = 224\n",
    "\n",
    "total_pix = r_w * r_h * channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dataset_full/Torn/Torn (780).png'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data_dir['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = np.zeros((len(tr_data_dir['train']), total_pix), dtype=np.float32)\n",
    "tr_label = np.zeros((len(tr_data_dir['train']), 1), dtype=np.int32)\n",
    "\n",
    "for i in range(len(tr_data_dir['train'])):\n",
    "    img = cv2.imread(tr_data_dir['train'][i])\n",
    "    img2 = cv2.resize(img, (r_w, r_h), interpolation=cv2.INTER_CUBIC)\n",
    "    tr_data[i,:] = img2.flatten()\n",
    "    \n",
    "    if 'Untorn' in tr_data_dir['train'][i]:\n",
    "        tr_label[i] = 0\n",
    "    elif 'normal' in tr_data_dir['train'][i]:\n",
    "        tr_label[i] = 0\n",
    "        \n",
    "    else:\n",
    "        tr_label[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = np.zeros((len(tr_data_dir['valid']), total_pix), dtype=np.float32)\n",
    "val_label = np.zeros((len(tr_data_dir['valid']), 1), dtype=np.int32)\n",
    "\n",
    "for j in range(len(val_data)):\n",
    "    img = cv2.imread(tr_data_dir['valid'][j])\n",
    "    img2 = cv2.resize(img, (r_w, r_h), interpolation=cv2.INTER_CUBIC)\n",
    "    val_data[j,:] = img2.flatten()\n",
    "\n",
    "    if 'Untorn' in tr_data_dir['valid'][j]:\n",
    "        val_label[j] = 0\n",
    "    elif 'normal' in tr_data_dir['valid'][j]:\n",
    "        val_label[j] = 0\n",
    "\n",
    "    else:\n",
    "        val_label[j] = 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg16(features, labels, mode):\n",
    "    \n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, height, width, channels]\n",
    "    # Our Fishing net image size is 640x480 and 3-channel (RGB)\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 224, 224, 3])\n",
    "    dropout_keep_prob = 0.5\n",
    "    num_classes = 2\n",
    "    is_training = True\n",
    "    \n",
    "    \n",
    "    net = layers_lib.repeat(\n",
    "        input_layer, 2, layers.conv2d, 64, [3, 3], scope='conv1')\n",
    "    net = layers_lib.max_pool2d(net, [2, 2], scope='pool1')\n",
    "    net = layers_lib.repeat(net, 2, layers.conv2d, 128, [3, 3], scope='conv2')\n",
    "    net = layers_lib.max_pool2d(net, [2, 2], scope='pool2')\n",
    "    net = layers_lib.repeat(net, 3, layers.conv2d, 256, [3, 3], scope='conv3')\n",
    "    net = layers_lib.max_pool2d(net, [2, 2], scope='pool3')\n",
    "    net = layers_lib.repeat(net, 3, layers.conv2d, 512, [3, 3], scope='conv4')\n",
    "    net = layers_lib.max_pool2d(net, [2, 2], scope='pool4')\n",
    "    net = layers_lib.repeat(net, 3, layers.conv2d, 512, [3, 3], scope='conv5')\n",
    "    net = layers_lib.max_pool2d(net, [2, 2], scope='pool5')\n",
    "    # Use conv2d instead of fully_connected layers.\n",
    "    net = layers.conv2d(net, 4096, [7, 7], padding='VALID', scope='fc6')\n",
    "    net = layers_lib.dropout(\n",
    "        net, dropout_keep_prob, is_training=is_training, scope='dropout6')\n",
    "    net = layers.conv2d(net, 4096, [1, 1], scope='fc7')\n",
    "    net = layers_lib.dropout(\n",
    "        net, dropout_keep_prob, is_training=is_training, scope='dropout7')\n",
    "    net = layers.conv2d(\n",
    "        net,\n",
    "        num_classes, [1, 1],\n",
    "        activation_fn=None,\n",
    "        normalizer_fn=None,\n",
    "        scope='fc8')\n",
    "    net = array_ops.squeeze(net, [1, 2], name='fc8/squeezed')\n",
    "    \n",
    "    logits = net\n",
    "    \n",
    "\n",
    "    with tf.device('/GPU:3'):\n",
    "\n",
    "        predictions = {\n",
    "            # Generate predictions (for PREDICT and EVAL mode)\n",
    "            \"classes\": tf.argmax(input=logits, axis=1),\n",
    "            # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "            # `logging_hook`.\n",
    "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "        }\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "        # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "        # sparse_softmax_cross_entropy cannot use one-hot encoding\n",
    "\n",
    "        #loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "        # Configure the Training Op (for TRAIN mode)\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.00001)\n",
    "            train_op = optimizer.minimize(\n",
    "                loss=loss,\n",
    "                global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "        # Add evaluation metrics (for EVAL mode)\n",
    "        eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(\n",
    "                labels=labels, predictions=predictions[\"classes\"])}\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "#session = tf.Session(config=config)\n",
    "for i in range(3,5):\n",
    "    \n",
    "# Create the Estimator\n",
    "# RL 1=0.001, 2=0.0001, 3=0.00001, 4=0.000001\n",
    "    f_dir = \"/models/ICROS_vgg/TrainRL_%d/\" % 3\n",
    "\n",
    "    if not tf.gfile.Exists(f_dir):\n",
    "        tf.gfile.MakeDirs(f_dir)\n",
    "\n",
    "    net_classifier = tf.estimator.Estimator(\n",
    "        model_fn=vgg16, model_dir=f_dir)\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "        tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "    batch_s = 32\n",
    "    epochs = 30\n",
    "    tr_steps = int(tr_data.shape[0]/batch_s*epochs)\n",
    "    step_in_epoch = int(tr_data.shape[0]/batch_s)+1\n",
    "    in_steps = 1\n",
    "\n",
    "\n",
    "    for ee in range(epochs):\n",
    "        \n",
    "        train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": tr_data},\n",
    "            y=tr_label,\n",
    "            batch_size=batch_s,\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "\n",
    "        eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": val_data},\n",
    "            y=val_label,\n",
    "            batch_size=64,\n",
    "            num_epochs=1,\n",
    "            shuffle=False)\n",
    "\n",
    "        net_classifier.train(\n",
    "            input_fn=train_input_fn,\n",
    "            steps=step_in_epoch,\n",
    "            hooks=[logging_hook])\n",
    "\n",
    "        eval_results = net_classifier.evaluate(input_fn=eval_input_fn, hooks=[logging_hook])\n",
    "        temp_res = eval_results[\"accuracy\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
