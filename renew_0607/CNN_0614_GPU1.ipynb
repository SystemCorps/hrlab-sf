{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU selection (memory)\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_txt = open('./data_split/data_dir_dict_0614.txt', 'r')\n",
    "data_json = data_txt.read()\n",
    "tr_data_dir = json.loads(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info.\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "channel = 3\n",
    "\n",
    "\n",
    "# resize\n",
    "r_w = 512\n",
    "r_h = 512\n",
    "\n",
    "total_pix = r_w * r_h * channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = np.zeros((len(tr_data_dir['train']), total_pix), dtype=np.float32)\n",
    "tr_label = np.zeros((len(tr_data_dir['train']), 1), dtype=np.int32)\n",
    "\n",
    "for i in range(len(tr_data_dir['train'])):\n",
    "    img = cv2.imread(tr_data_dir['train'][i][0])\n",
    "    img2 = cv2.resize(img, (r_w, r_h), interpolation=cv2.INTER_CUBIC)\n",
    "    tr_data[i,:] = img2.flatten()\n",
    "    tr_label[i] = tr_data_dir['train'][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_data = np.zeros((len(tr_data_dir['test']), total_pix), dtype=np.float32)\n",
    "ev_label = np.zeros((len(tr_data_dir['test']), 1), dtype=np.int32)\n",
    "\n",
    "for i in range(len(tr_data_dir['test'])):\n",
    "    img = cv2.imread(tr_data_dir['test'][i][0])\n",
    "    img2 = cv2.resize(img, (r_w, r_h), interpolation=cv2.INTER_CUBIC)\n",
    "    ev_data[i,:] = img2.flatten()\n",
    "    ev_label[i] = tr_data_dir['test'][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(x, n_class, dropout, reuse, is_training, width=512, height=512):\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # RGB data, 3-channel\n",
    "        input_layer = tf.reshape(x, shape=[-1, height, width, 3], name='input_layer')\n",
    "        \n",
    "        k_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(\n",
    "            inputs=input_layer,\n",
    "            filters=48,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=k_init,\n",
    "            name='conv1')\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2, name='pool1')\n",
    "        # height/2, width/2\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=96,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=k_init,\n",
    "            name='conv2')\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2, name='pool2')\n",
    "        # height/4, width/4\n",
    "        \n",
    "        conv3 = tf.layers.conv2d(\n",
    "            inputs=pool2,\n",
    "            filters=96,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=k_init,\n",
    "            name='conv3')\n",
    "        pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2, name='pool3')\n",
    "        # height/8, width/8\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(\n",
    "            inputs=pool3,\n",
    "            filters=96,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=k_init,\n",
    "            name='conv4')\n",
    "        pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2, name='pool4')\n",
    "        # height/16, width/16\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(\n",
    "            inputs=pool4,\n",
    "            filters=96,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=k_init,\n",
    "            name='conv5')\n",
    "        pool5 = tf.layers.max_pooling2d(inputs=conv5, pool_size=[2, 2], strides=2, name='pool5')\n",
    "        # height/32, width/32\n",
    "        \n",
    "        #pool5_flat = tf.reshape(pool5, [-1, int(height/32) * int(width/32) * 96])\n",
    "        pool5_flat = tf.contrib.layers.flatten(pool5)\n",
    "        \n",
    "        dense1 = tf.layers.dense(pool5_flat, 1024, activation=tf.nn.relu, name='dense1')\n",
    "        \n",
    "        dropout = tf.layers.dropout(dense1, rate=dropout, training=is_training, name='drop')\n",
    "        \n",
    "        # Output\n",
    "        out = tf.layers.dense(dropout, n_class, name='output')\n",
    "        out = tf.nn.softmax(out) if not is_training else out\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.5\n",
    "learning_rate = 0.0001\n",
    "batch_size = 16\n",
    "disp_step = 20\n",
    "save_step = 500\n",
    "n_classes = 2\n",
    "epochs = 30\n",
    "\n",
    "tr_steps = int(tr_data.shape[0]/batch_size*epochs)\n",
    "inner_steps = int((tr_data.shape[0])/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    reuse_vars = False\n",
    "    \n",
    "    # tf graph input\n",
    "    X = tf.placeholder(tf.float32, [None, total_pix], name='inputs')\n",
    "    Y = tf.placeholder(tf.int32, [None, 1], name='labels')\n",
    "    \n",
    "    with tf.device('/gpu:1'):\n",
    "        # cnn_model(x, n_class, dropout, re_use, is_training, width, height)\n",
    "        logits_train = cnn_model(X, n_classes, dropout, reuse=reuse_vars,\n",
    "                                 is_training=True)\n",
    "        \n",
    "        logits_test = cnn_model(X, n_classes, dropout, reuse=True,\n",
    "                                is_training=False)\n",
    "        \n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(logits=logits_train, labels=Y)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        grads = optimizer.compute_gradients(loss)\n",
    "        \n",
    "        pred = tf.argmax(input=logits_test, axis=1)\n",
    "        correct_pred = tf.equal(pred, tf.cast(Y, tf.int64))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        #accuracy = tf.metrics.accuracy(labels=Y, predictions=pred)\n",
    "        \n",
    "        \n",
    "        prob = tf.nn.softmax(logits_test, name='softmax_tensor')\n",
    "        \n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        \n",
    "        \n",
    "        reuse_vars = True\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)\n",
    "\n",
    "        file_n = '/models/ICROS/GPU1(1)'\n",
    "        t_count = 1\n",
    "        saver.save(sess, file_n)\n",
    "        for j in range(epochs):\n",
    "\n",
    "            for i in range(inner_steps):\n",
    "\n",
    "                tr_batch = {}\n",
    "\n",
    "                if i < inner_steps-1:\n",
    "                    tr_batch['data'] = tr_data[i*batch_size:(i+1)*batch_size][:]\n",
    "                    tr_batch['label'] = tr_label[i*batch_size:(i+1)*batch_size][:]\n",
    "\n",
    "                else:\n",
    "                    tr_batch['data'] = tr_data[i*batch_size:][:]\n",
    "                    tr_batch['label'] = tr_label[i*batch_size:][:]\n",
    "\n",
    "                sess.run(train_op, feed_dict={X: tr_batch['data'],\n",
    "                                              Y: tr_batch['label']})\n",
    "\n",
    "                if t_count % disp_step == 1:\n",
    "                    \n",
    "                    loss_tr = sess.run(loss, feed_dict = {X: tr_batch['data'],\n",
    "                                                          Y: tr_batch['label']})\n",
    "\n",
    "                    acc_tr = sess.run(accuracy, feed_dict = {X: tr_batch['data'],\n",
    "                                                             Y: tr_batch['label']})\n",
    "                    \n",
    "                    prob_tr = sess.run(prob, feed_dict = {X: tr_batch['data'],\n",
    "                                                         Y: tr_batch['label']})\n",
    "\n",
    "                    #print('step %d, training accuracy %f, loss %f' % (t_count, accuracy, loss))\n",
    "                    print('Training step: ', t_count)\n",
    "                    print('Accuracy', acc_tr)\n",
    "                    print('Probability')\n",
    "                    print(prob_tr)\n",
    "                    print('Loss: ', loss_tr)\n",
    "                    #print(acc_tr, loss_tr)\n",
    "                    \n",
    "                if t_count % save_step == 0:\n",
    "                    saver.save(sess, file_n, t_count)\n",
    "\n",
    "                t_count = t_count+1\n",
    "        \n",
    "        saver.save(sess, file_n, t_count)\n",
    "        print(\"Training is finished!!!\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        ev_accuracy = np.zeros((len(ev_label), 1))\n",
    "        for k in range(len(ev_label)):\n",
    "            ev_batch = {}\n",
    "            ev_batch['data'] = ev_data[k:(k+1)][:]\n",
    "            ev_batch['label'] = ev_label[k:(k+1)]\n",
    "            ev_accuracy[k] = sess.run(accuracy, feed_dict={X: ev_batch['data'],\n",
    "                                                           Y: ev_batch['label']})\n",
    "\n",
    "        print(np.mean(ev_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
