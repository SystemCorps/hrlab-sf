{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Trainig Templete\n",
    "## Based on Tensorlfow - CNN MNIST example\n",
    "https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/examples/tutorials/layers/cnn_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 library 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_txt = open('./data_split/dir_0720_only_train(0614 for ev).txt', 'r')\n",
    "data_json = data_txt.read()\n",
    "tr_data_dir = json.loads(data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntest = open('./data_split/data_dir_dict_oh_0717.txt', 'r')\\ndata = test.read()\\ntr_data_dir2 = json.loads(data)\\nprint(tr_data_dir2[0])\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test = open('./data_split/data_dir_dict_oh_0717.txt', 'r')\n",
    "data = test.read()\n",
    "tr_data_dir2 = json.loads(data)\n",
    "print(tr_data_dir2[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntemp1 = tr_data_dir1['train']\\nprint(temp1[0][0])\\ntemp2 = [0]*(len(temp1) + len(tr_data_dir2))\\nfor i in range(len(temp1)):\\n    temp2[i] = temp1[i][0]\\n    \\n    \\nj = 0\\nfor k in range(len(temp1), len(temp2)):\\n    temp2[k] = tr_data_dir2[j]\\n    j = j+1\\nrandom.shuffle(temp2)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "temp1 = tr_data_dir1['train']\n",
    "print(temp1[0][0])\n",
    "temp2 = [0]*(len(temp1) + len(tr_data_dir2))\n",
    "for i in range(len(temp1)):\n",
    "    temp2[i] = temp1[i][0]\n",
    "    \n",
    "    \n",
    "j = 0\n",
    "for k in range(len(temp1), len(temp2)):\n",
    "    temp2[k] = tr_data_dir2[j]\n",
    "    j = j+1\n",
    "random.shuffle(temp2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntr_data_dir = {}\\ntr_data_dir['train'] = temp2\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tr_data_dir = {}\n",
    "tr_data_dir['train'] = temp2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef default(o):\\n    if isinstance(o, (np.int_, np.intc, np.intp, np.int8,\\n                      np.int16, np.int32, np.int64, np.uint8,\\n                      np.uint16,np.uint32, np.uint64)):\\n        return int(o)\\n\\n    raise TypeError\\n\\nwith open('./data_split/dir_0720_only_train(0614 for ev).txt', 'w') as file:\\n    file.write(json.dumps(tr_data_dir, default=default))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def default(o):\n",
    "    if isinstance(o, (np.int_, np.intc, np.intp, np.int8,\n",
    "                      np.int16, np.int32, np.int64, np.uint8,\n",
    "                      np.uint16,np.uint32, np.uint64)):\n",
    "        return int(o)\n",
    "\n",
    "    raise TypeError\n",
    "\n",
    "with open('./data_split/dir_0720_only_train(0614 for ev).txt', 'w') as file:\n",
    "    file.write(json.dumps(tr_data_dir, default=default))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info.\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "channel = 3\n",
    "\n",
    "\n",
    "# resize\n",
    "r_w = 512\n",
    "r_h = 512\n",
    "\n",
    "total_pix = r_w * r_h * channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = np.zeros((len(tr_data_dir['train']), total_pix), dtype=np.float32)\n",
    "tr_label = np.zeros((len(tr_data_dir['train']), 1), dtype=np.int32)\n",
    "\n",
    "for i in range(len(tr_data_dir['train'])):\n",
    "    img = cv2.imread(tr_data_dir['train'][i])\n",
    "    img2 = cv2.resize(img, (r_w, r_h), interpolation=cv2.INTER_CUBIC)\n",
    "    tr_data[i,:] = img2.flatten()\n",
    "    \n",
    "    if 'Untorn' in tr_data_dir['train'][i]:\n",
    "        tr_label[i] = 0\n",
    "    elif 'normal' in tr_data_dir['train'][i]:\n",
    "        tr_label[i] = 0\n",
    "        \n",
    "    else:\n",
    "        tr_label[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dataset_full/Torn/Torn (1066).png [1]\n",
      "4154\n"
     ]
    }
   ],
   "source": [
    "ii = 766\n",
    "print(tr_data_dir['train'][ii], tr_label[ii])\n",
    "print(len(tr_data_dir['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, height, width, channels]\n",
    "    # Our Fishing net image size is 640x480 and 3-channel (RGB)\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 512, 512, 3])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 48 features using a 5x5 filter with ReLU activation.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 480, 640, 3]\n",
    "    # Output Tensor Shape: [batch_size, 480, 640, 48]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        \n",
    "        inputs=input_layer,\n",
    "        filters=48,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu,\n",
    "        name='conv1')\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 480, 640, 48]\n",
    "    # Output Tensor Shape: [batch_size, 240, 320, 48]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    # Computes 96 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 240, 320, 48]\n",
    "    # Output Tensor Shape: [batch_size, 240, 320, 96]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=96,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 240, 320, 96]\n",
    "    # Output Tensor Shape: [batch_size, 120, 160, 96]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Input Tensor Shape: [batch_size, 120, 160, 96]\n",
    "    # Output Tensor Shape: [batch_size, 120, 160, 96]    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "        inputs=pool2,\n",
    "        filters=96,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Input Tensor Shape: [batch_size, 120, 160, 96]\n",
    "    # Output Tensor Shape: [batch_size, 60, 80, 96]\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Input Tensor Shape: [batch_size, 60, 80, 96]\n",
    "    # Output Tensor Shape: [batch_size, 60, 80, 96]    \n",
    "    conv4 = tf.layers.conv2d(\n",
    "        inputs=pool3,\n",
    "        filters=96,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Input Tensor Shape: [batch_size, 60, 80, 96]\n",
    "    # Output Tensor Shape: [batch_size, 30, 40, 96]\n",
    "    pool4 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Input Tensor Shape: [batch_size, 30, 40, 96]\n",
    "    # Output Tensor Shape: [batch_size, 30, 40, 96]    \n",
    "    conv5 = tf.layers.conv2d(\n",
    "        inputs=pool4,\n",
    "        filters=96,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Input Tensor Shape: [batch_size, 30, 40, 96]\n",
    "    # Output Tensor Shape: [batch_size, 15, 20, 96]\n",
    "    pool5 = tf.layers.max_pooling2d(inputs=conv5, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape: [batch_size, 15, 20, 96]\n",
    "    # Output Tensor Shape: [batch_size, 15 * 20 * 96]\n",
    "    pool5_flat = tf.contrib.layers.flatten(pool5)\n",
    "\n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 15 * 20 * 96]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool5_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=dense, rate=0.5, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 2]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=2)\n",
    "    \n",
    "\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    # sparse_softmax_cross_entropy cannot use one-hot encoding\n",
    "    \n",
    "    #loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_service': None, '_save_summary_steps': 100, '_task_type': 'worker', '_model_dir': '/models/tr_together/Train_2/', '_is_chief': True, '_save_checkpoints_secs': 600, '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5d729e5278>, '_keep_checkpoint_max': 5, '_master': '', '_save_checkpoints_steps': None, '_session_config': None, '_task_id': 0, '_num_worker_replicas': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_global_id_in_cluster': 0, '_num_ps_replicas': 0, '_evaluation_master': ''}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /models/tr_together/Train_2/model.ckpt-3894\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3895 into /models/tr_together/Train_2/model.ckpt.\n",
      "INFO:tensorflow:probabilities = [[0.00000148 0.99999857]\n",
      " [0.00121723 0.9987828 ]\n",
      " [0.00000006 1.        ]\n",
      " [0.9931685  0.00683148]\n",
      " [0.00000485 0.9999951 ]\n",
      " [0.         1.        ]\n",
      " [0.9999113  0.00008873]\n",
      " [0.         1.        ]\n",
      " [0.99774337 0.0022566 ]\n",
      " [0.05257252 0.94742745]\n",
      " [0.00000007 0.9999999 ]\n",
      " [0.00000554 0.9999944 ]\n",
      " [0.00462301 0.99537694]\n",
      " [0.00000016 0.9999999 ]\n",
      " [0.         1.        ]\n",
      " [0.00014587 0.9998541 ]]\n",
      "INFO:tensorflow:loss = 1.1801125, step = 3895\n",
      "INFO:tensorflow:probabilities = [[0.7213185  0.27868146]\n",
      " [0.60884285 0.39115715]\n",
      " [0.18960585 0.81039417]\n",
      " [0.47432595 0.52567405]\n",
      " [0.5182009  0.48179913]\n",
      " [0.33227795 0.66772205]\n",
      " [0.70704454 0.29295543]\n",
      " [0.147976   0.8520241 ]\n",
      " [0.78380114 0.21619892]\n",
      " [0.47855514 0.5214448 ]\n",
      " [0.63613135 0.36386862]\n",
      " [0.6831266  0.31687337]\n",
      " [0.5422011  0.45779887]\n",
      " [0.4471881  0.5528119 ]\n",
      " [0.70131344 0.2986866 ]\n",
      " [0.11329482 0.8867052 ]] (12.645 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.98222\n",
      "INFO:tensorflow:probabilities = [[0.99990237 0.00009768]\n",
      " [0.9977289  0.00227116]\n",
      " [1.         0.        ]\n",
      " [0.9692849  0.03071517]\n",
      " [0.9999051  0.0000949 ]\n",
      " [1.         0.        ]\n",
      " [0.38393763 0.61606234]\n",
      " [1.         0.        ]\n",
      " [0.0000008  0.99999917]\n",
      " [1.         0.        ]\n",
      " [0.29856372 0.7014363 ]\n",
      " [0.3278146  0.6721854 ]\n",
      " [0.4685086  0.53149146]\n",
      " [0.8288688  0.17113127]\n",
      " [0.05401612 0.94598395]\n",
      " [1.         0.00000001]] (12.468 sec)\n",
      "INFO:tensorflow:loss = 0.14195882, step = 3995 (25.113 sec)\n",
      "INFO:tensorflow:probabilities = [[0.02717154 0.9728284 ]\n",
      " [0.99999976 0.00000026]\n",
      " [0.90706265 0.09293738]\n",
      " [0.03091384 0.9690861 ]\n",
      " [0.999655   0.00034503]\n",
      " [0.00007872 0.9999213 ]\n",
      " [0.50057095 0.49942902]\n",
      " [0.22530356 0.7746964 ]\n",
      " [0.06511157 0.9348884 ]\n",
      " [0.0019344  0.99806565]\n",
      " [0.9966804  0.00331965]\n",
      " [0.996342   0.00365799]\n",
      " [0.54887605 0.45112395]\n",
      " [0.9986325  0.0013675 ]\n",
      " [0.04433016 0.9556698 ]\n",
      " [1.         0.        ]] (12.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.01444\n",
      "INFO:tensorflow:probabilities = [[1.         0.        ]\n",
      " [0.99999976 0.00000022]\n",
      " [0.98971444 0.01028553]\n",
      " [0.99996066 0.00003938]\n",
      " [0.00000438 0.9999956 ]\n",
      " [0.99910945 0.00089056]\n",
      " [0.00015827 0.9998417 ]\n",
      " [1.         0.        ]\n",
      " [0.8191611  0.18083888]\n",
      " [0.99998856 0.00001141]\n",
      " [0.00015957 0.9998404 ]\n",
      " [1.         0.        ]\n",
      " [0.99999285 0.00000713]\n",
      " [0.         1.        ]\n",
      " [0.9999926  0.00000741]\n",
      " [0.13537648 0.8646235 ]] (12.486 sec)\n",
      "INFO:tensorflow:loss = 0.11670172, step = 4095 (24.910 sec)\n",
      "INFO:tensorflow:probabilities = [[0.01851931 0.98148066]\n",
      " [0.04609064 0.95390934]\n",
      " [0.00000109 0.9999989 ]\n",
      " [0.00400293 0.9959971 ]\n",
      " [1.         0.        ]\n",
      " [0.00004669 0.99995327]\n",
      " [0.00001191 0.9999881 ]\n",
      " [0.999175   0.00082491]\n",
      " [0.02022642 0.9797736 ]\n",
      " [0.03088627 0.96911377]\n",
      " [0.05867874 0.94132125]\n",
      " [0.99994767 0.00005235]\n",
      " [0.         1.        ]\n",
      " [0.00006342 0.9999366 ]\n",
      " [1.         0.        ]\n",
      " [0.5090544  0.49094555]] (12.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 4.00977\n",
      "INFO:tensorflow:probabilities = [[0.6021662  0.39783385]\n",
      " [0.00922172 0.9907783 ]\n",
      " [0.9999975  0.00000254]\n",
      " [0.9987913  0.00120881]\n",
      " [0.00000057 0.9999994 ]\n",
      " [0.0000458  0.9999542 ]\n",
      " [0.99986124 0.00013876]\n",
      " [0.99998856 0.00001143]\n",
      " [0.24719076 0.7528093 ]\n",
      " [0.00000001 1.        ]\n",
      " [0.00000016 0.9999999 ]\n",
      " [0.0000011  0.9999989 ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.00000005]\n",
      " [1.         0.        ]] (12.457 sec)\n",
      "INFO:tensorflow:loss = 0.07602115, step = 4195 (24.940 sec)\n",
      "INFO:tensorflow:probabilities = [[1.         0.00000001]\n",
      " [0.75559676 0.24440321]\n",
      " [0.00000002 1.        ]\n",
      " [0.00391283 0.9960872 ]\n",
      " [0.04458878 0.9554112 ]\n",
      " [0.00031003 0.99968994]\n",
      " [0.9831866  0.01681336]\n",
      " [0.98651564 0.01348435]\n",
      " [0.00010606 0.9998939 ]\n",
      " [1.         0.00000001]\n",
      " [1.         0.        ]\n",
      " [0.00008591 0.99991405]\n",
      " [0.9987515  0.00124845]\n",
      " [0.1151313  0.8848687 ]\n",
      " [1.         0.        ]\n",
      " [0.14335589 0.85664415]] (12.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.98278\n",
      "INFO:tensorflow:probabilities = [[0.00959983 0.99040014]\n",
      " [0.00707706 0.9929229 ]\n",
      " [0.99840134 0.00159865]\n",
      " [0.00000003 1.        ]\n",
      " [0.13076991 0.86923015]\n",
      " [0.00147953 0.99852043]\n",
      " [0.99773514 0.00226478]\n",
      " [0.999995   0.00000497]\n",
      " [0.07184165 0.92815834]\n",
      " [0.         1.        ]\n",
      " [0.9999397  0.00006031]\n",
      " [0.00007201 0.999928  ]\n",
      " [0.00132882 0.9986712 ]\n",
      " [0.12021824 0.8797817 ]\n",
      " [0.9831344  0.01686557]\n",
      " [0.9761627  0.02383734]] (12.570 sec)\n",
      "INFO:tensorflow:loss = 0.025467545, step = 4295 (25.108 sec)\n",
      "INFO:tensorflow:probabilities = [[0.99999917 0.00000085]\n",
      " [0.52759916 0.4724008 ]\n",
      " [1.         0.00000004]\n",
      " [0.00018988 0.99981016]\n",
      " [0.9996222  0.00037774]\n",
      " [0.9999397  0.00006028]\n",
      " [0.05187341 0.9481266 ]\n",
      " [0.03937661 0.9606234 ]\n",
      " [1.         0.        ]\n",
      " [0.07723933 0.92276067]\n",
      " [0.99697137 0.00302863]\n",
      " [0.028395   0.97160494]\n",
      " [0.0000002  0.99999976]\n",
      " [0.         1.        ]\n",
      " [0.9998596  0.00014037]\n",
      " [0.         1.        ]] (12.550 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.97652\n",
      "INFO:tensorflow:probabilities = [[0.64720833 0.35279173]\n",
      " [0.         1.        ]\n",
      " [0.00001213 0.99998784]\n",
      " [0.0002055  0.99979454]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.00003742 0.99996257]\n",
      " [0.9987809  0.00121905]\n",
      " [0.0000061  0.9999939 ]\n",
      " [0.992967   0.00703304]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.71491766 0.28508234]\n",
      " [0.9999943  0.0000057 ]] (12.597 sec)\n",
      "INFO:tensorflow:loss = 0.04870122, step = 4395 (25.147 sec)\n",
      "INFO:tensorflow:probabilities = [[0.00010915 0.9998908 ]\n",
      " [0.00000002 1.        ]\n",
      " [0.00001256 0.9999875 ]\n",
      " [0.00002221 0.9999778 ]\n",
      " [0.9999516  0.00004843]\n",
      " [0.99996686 0.00003319]\n",
      " [0.03455047 0.96544945]\n",
      " [0.00012267 0.99987733]\n",
      " [1.         0.        ]\n",
      " [0.06480009 0.93519986]\n",
      " [0.00189415 0.9981059 ]\n",
      " [0.00007378 0.9999262 ]\n",
      " [0.00257446 0.9974255 ]\n",
      " [0.99989545 0.00010457]\n",
      " [0.14939748 0.8506025 ]\n",
      " [0.00002185 0.9999782 ]] (12.537 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.97502\n",
      "INFO:tensorflow:probabilities = [[0.99998856 0.00001139]\n",
      " [0.00006485 0.99993515]\n",
      " [0.04968043 0.9503196 ]\n",
      " [0.00000003 1.        ]\n",
      " [0.9955527  0.0044473 ]\n",
      " [0.9999043  0.00009569]\n",
      " [0.00027355 0.9997265 ]\n",
      " [0.00000017 0.9999999 ]\n",
      " [0.00003621 0.99996376]\n",
      " [0.00000908 0.99999094]\n",
      " [0.9999664  0.0000336 ]\n",
      " [0.99999964 0.0000003 ]\n",
      " [0.         1.        ]\n",
      " [0.00000003 1.        ]\n",
      " [0.00009053 0.9999095 ]\n",
      " [0.00010686 0.9998932 ]] (12.620 sec)\n",
      "INFO:tensorflow:loss = 0.003508521, step = 4495 (25.158 sec)\n",
      "INFO:tensorflow:probabilities = [[1.         0.        ]\n",
      " [0.98583895 0.01416106]\n",
      " [0.00057629 0.99942374]\n",
      " [0.59415096 0.4058491 ]\n",
      " [1.         0.        ]\n",
      " [0.00000112 0.9999989 ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.9999577  0.0000423 ]\n",
      " [1.         0.        ]\n",
      " [0.99892384 0.00107619]\n",
      " [0.03505399 0.96494603]\n",
      " [0.00072393 0.9992761 ]\n",
      " [0.9999994  0.00000064]\n",
      " [1.         0.        ]\n",
      " [0.00000002 1.        ]] (12.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.9876\n",
      "INFO:tensorflow:probabilities = [[0.99903345 0.00096658]\n",
      " [0.06690204 0.9330979 ]\n",
      " [0.9999938  0.00000624]\n",
      " [0.00217248 0.9978275 ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.00000085 0.99999917]\n",
      " [0.00011581 0.9998841 ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.04814388 0.95185614]\n",
      " [0.         1.        ]\n",
      " [0.9964521  0.0035479 ]\n",
      " [0.00003423 0.9999658 ]\n",
      " [0.00128366 0.99871635]] (12.604 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.007920267, step = 4595 (25.076 sec)\n",
      "INFO:tensorflow:probabilities = [[0.00116091 0.9988391 ]\n",
      " [0.         1.        ]\n",
      " [0.9995919  0.00040812]\n",
      " [0.00000017 0.9999999 ]\n",
      " [0.00013429 0.99986565]\n",
      " [0.         1.        ]\n",
      " [0.9999989  0.00000104]\n",
      " [0.00000008 0.9999999 ]\n",
      " [0.00001021 0.99998975]\n",
      " [0.9536321  0.04636783]\n",
      " [0.00082949 0.99917054]\n",
      " [0.9999949  0.00000509]\n",
      " [0.00094631 0.99905366]\n",
      " [0.00000009 0.9999999 ]\n",
      " [0.0000016  0.99999845]\n",
      " [0.00003861 0.9999614 ]] (12.554 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.991\n",
      "INFO:tensorflow:probabilities = [[0.00000121 0.9999988 ]\n",
      " [0.00021199 0.999788  ]\n",
      " [0.00000073 0.9999993 ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.00005069 0.99994934]\n",
      " [1.         0.00000003]\n",
      " [0.00000014 0.9999999 ]\n",
      " [0.00074548 0.9992545 ]\n",
      " [0.9999999  0.00000014]\n",
      " [0.99989045 0.00010958]\n",
      " [0.         1.        ]\n",
      " [0.00087765 0.9991223 ]\n",
      " [0.00000001 1.        ]\n",
      " [0.         1.        ]\n",
      " [0.00000004 1.        ]] (12.503 sec)\n",
      "INFO:tensorflow:loss = 0.00012488799, step = 4695 (25.059 sec)\n",
      "INFO:tensorflow:probabilities = [[0.00608861 0.9939114 ]\n",
      " [0.9827145  0.01728551]\n",
      " [0.99861896 0.00138101]\n",
      " [0.9999348  0.00006515]\n",
      " [0.00154518 0.9984548 ]\n",
      " [0.9999852  0.00001473]\n",
      " [0.9985934  0.00140665]\n",
      " [1.         0.        ]\n",
      " [0.9999989  0.00000111]\n",
      " [0.99438757 0.00561244]\n",
      " [0.9999174  0.00008266]\n",
      " [0.9999939  0.00000602]\n",
      " [0.00007697 0.999923  ]\n",
      " [0.9999528  0.00004722]\n",
      " [0.00008752 0.9999125 ]\n",
      " [0.00380448 0.99619555]] (12.622 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.97626\n",
      "INFO:tensorflow:probabilities = [[0.00736853 0.99263144]\n",
      " [0.00009395 0.99990606]\n",
      " [0.9948843  0.00511565]\n",
      " [0.00049044 0.9995096 ]\n",
      " [1.         0.00000001]\n",
      " [1.         0.        ]\n",
      " [0.00000008 0.9999999 ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.07295795 0.92704207]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.44867784 0.55132216]\n",
      " [0.00669076 0.99330926]\n",
      " [0.02597982 0.97402024]\n",
      " [1.         0.        ]] (12.527 sec)\n",
      "INFO:tensorflow:loss = 0.044833627, step = 4795 (25.147 sec)\n",
      "INFO:tensorflow:probabilities = [[0.         1.        ]\n",
      " [0.00000007 0.9999999 ]\n",
      " [0.9999999  0.00000016]\n",
      " [0.00013449 0.99986553]\n",
      " [0.00002779 0.9999722 ]\n",
      " [1.         0.        ]\n",
      " [0.99794334 0.00205661]\n",
      " [1.         0.        ]\n",
      " [0.00000026 0.99999976]\n",
      " [0.         1.        ]\n",
      " [0.99999976 0.0000002 ]\n",
      " [0.9999597  0.00004028]\n",
      " [0.00040888 0.9995912 ]\n",
      " [0.03548215 0.9645179 ]\n",
      " [0.9999603  0.00003969]\n",
      " [0.         1.        ]] (12.624 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.98214\n",
      "INFO:tensorflow:probabilities = [[1.         0.        ]\n",
      " [0.99923253 0.00076748]\n",
      " [0.         1.        ]\n",
      " [0.00000046 0.9999995 ]\n",
      " [0.9999995  0.0000005 ]\n",
      " [0.00000001 1.        ]\n",
      " [0.00000012 0.9999999 ]\n",
      " [0.3138159  0.68618417]\n",
      " [0.9837991  0.01620086]\n",
      " [0.         1.        ]\n",
      " [1.         0.00000005]\n",
      " [0.         1.        ]\n",
      " [0.9999999  0.00000008]\n",
      " [0.00220859 0.9977914 ]\n",
      " [1.         0.00000001]\n",
      " [0.         1.        ]] (12.489 sec)\n",
      "INFO:tensorflow:loss = 0.0736414, step = 4895 (25.114 sec)\n",
      "INFO:tensorflow:probabilities = [[0.99999917 0.00000084]\n",
      " [0.00006895 0.9999311 ]\n",
      " [0.999966   0.00003401]\n",
      " [0.00556003 0.99443996]\n",
      " [0.00000002 1.        ]\n",
      " [0.99995697 0.000043  ]\n",
      " [0.9999974  0.00000263]\n",
      " [0.00000205 0.999998  ]\n",
      " [0.         1.        ]\n",
      " [0.9999919  0.00000816]\n",
      " [0.01736444 0.9826355 ]\n",
      " [0.         1.        ]\n",
      " [0.9995937  0.0004064 ]\n",
      " [0.00000001 1.        ]\n",
      " [1.         0.00000001]\n",
      " [0.00000001 1.        ]] (12.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.97664\n",
      "INFO:tensorflow:probabilities = [[1.         0.        ]\n",
      " [0.05046207 0.9495379 ]\n",
      " [0.00309493 0.996905  ]\n",
      " [1.         0.00000001]\n",
      " [0.00000118 0.9999988 ]\n",
      " [0.99999654 0.00000352]\n",
      " [0.39071456 0.6092855 ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.000001   0.99999905]\n",
      " [0.00053302 0.99946696]\n",
      " [0.00056501 0.999435  ]\n",
      " [0.99797314 0.00202688]\n",
      " [0.999992   0.00000801]\n",
      " [0.00751087 0.9924891 ]\n",
      " [1.         0.        ]] (12.675 sec)\n",
      "INFO:tensorflow:loss = 0.035064254, step = 4995 (25.146 sec)\n",
      "INFO:tensorflow:probabilities = [[1.         0.        ]\n",
      " [1.         0.00000001]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.9999999  0.00000008]\n",
      " [0.9999993  0.00000071]\n",
      " [0.9999968  0.00000323]\n",
      " [0.         1.        ]\n",
      " [0.00016206 0.9998379 ]\n",
      " [0.00208404 0.9979159 ]\n",
      " [0.00792508 0.99207497]\n",
      " [1.         0.        ]\n",
      " [0.00775976 0.9922402 ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]] (12.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.98906\n",
      "INFO:tensorflow:probabilities = [[0.00004951 0.9999505 ]\n",
      " [0.9988482  0.00115179]\n",
      " [0.9999989  0.00000102]\n",
      " [0.00756305 0.99243695]\n",
      " [0.00047295 0.9995271 ]\n",
      " [0.         1.        ]\n",
      " [0.00025197 0.99974805]\n",
      " [0.00002737 0.9999726 ]\n",
      " [0.00075289 0.9992472 ]\n",
      " [0.02747736 0.9725227 ]\n",
      " [0.9995123  0.00048774]\n",
      " [0.         1.        ]\n",
      " [0.95570946 0.04429058]\n",
      " [1.         0.        ]\n",
      " [0.00000001 1.        ]\n",
      " [0.         1.        ]] (12.530 sec)\n",
      "INFO:tensorflow:loss = 0.0052469596, step = 5095 (25.069 sec)\n",
      "INFO:tensorflow:probabilities = [[0.99997485 0.00002519]\n",
      " [0.00380661 0.9961934 ]\n",
      " [0.00005823 0.9999418 ]\n",
      " [0.00109522 0.9989048 ]\n",
      " [0.9999988  0.00000117]\n",
      " [1.         0.        ]\n",
      " [0.00004662 0.9999534 ]\n",
      " [1.         0.        ]\n",
      " [0.0000613  0.9999387 ]\n",
      " [0.9999999  0.00000008]\n",
      " [0.00519502 0.9948049 ]\n",
      " [0.00000016 0.9999999 ]\n",
      " [1.         0.        ]\n",
      " [0.9999999  0.00000011]\n",
      " [0.0002171  0.99978286]\n",
      " [0.         1.        ]] (12.380 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5192 into /models/tr_together/Train_2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00062325096.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7f5d729e50f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "#session = tf.Session(config=config)\n",
    "\n",
    "# Create the Estimator\n",
    "f_dir = \"/models/tr_together/Train_2/\"\n",
    "net_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=f_dir)\n",
    "\n",
    "# Set up logging for predictions\n",
    "# Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "    tensors=tensors_to_log, every_n_iter=50)\n",
    "\n",
    "batch_s = 16\n",
    "epochs = 5\n",
    "tr_steps = int(tr_data.shape[0]/batch_s*epochs)\n",
    "in_steps = 1\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": tr_data},\n",
    "    y=tr_label,\n",
    "    batch_size=batch_s,\n",
    "    num_epochs=epochs,\n",
    "    shuffle=False)\n",
    "net_classifier.train(\n",
    "    input_fn=train_input_fn,\n",
    "    steps=tr_steps,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nev_results = np.zeros((len(ev_label), 1))\\n\\n\\n# Evaluate the model and print results\\nfor i in range(len(ev_label)):\\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\\n        x={\"x\": ev_data[i:i+1]},\\n        y=ev_label[i:i+1],\\n        num_epochs=1,\\n        shuffle=False)\\n    eval_results = net_classifier.evaluate(input_fn=eval_input_fn)\\n\\n    ev_results[i] = eval_results[\"accuracy\"]\\n    \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ev_results = np.zeros((len(ev_label), 1))\n",
    "\n",
    "\n",
    "# Evaluate the model and print results\n",
    "for i in range(len(ev_label)):\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": ev_data[i:i+1]},\n",
    "        y=ev_label[i:i+1],\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    eval_results = net_classifier.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "    ev_results[i] = eval_results[\"accuracy\"]\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.mean(ev_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
